import os
import re
import cv2
import time
import json
import argparse
import traceback
import requests
import pytesseract
from PIL import Image, ImageFilter, ImageEnhance
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from notion_client import Client
from datetime import datetime
import shutil

# âœ… åºƒå‘Šé™¤å¤–ã€RT/å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ã€æŠ•ç¨¿IDè£œå®Œä»˜ã
AD_KEYWORDS = [
    "r10.to",
    "ãµã‚‹ã•ã¨ç´ç¨",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³",
    "ãŠé‡‘å€Ÿã‚Šã‚‰ã‚Œã‚‹",
    "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¬ãƒ¬ãƒ¼ã‚¸",
    "UNEXT",
    "ã‚¨ã‚³ã‚ªã‚¯",
    "#PR",
    "æ¥½å¤©",
    "Amazon",
    "A8",
    "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    "å‰¯æ¥­",
    "bit.ly",
    "shp.ee",
    "t.co/",
]


def normalize_text(text):
    return text.strip()


def login(driver, target=None):
    if os.path.exists("twitter_cookies.json"):
        print("âœ… Cookieã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º â†’ ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—")
        print("ğŸŒ https://twitter.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­â€¦")
        driver.get("https://twitter.com/")
        driver.delete_all_cookies()
        with open("twitter_cookies.json", "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.get(f"https://twitter.com/{target or TWITTER_USERNAME}")
        return

    print("ğŸ” åˆå›ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹")
    driver.get("https://twitter.com/i/flow/login")
    email_input = WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.NAME, "text"))
    )
    email_input.send_keys(TWITTER_EMAIL)
    email_input.send_keys(Keys.ENTER)
    time.sleep(2)

    try:
        username_input = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "text"))
        )
        username_input.send_keys(TWITTER_USERNAME)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)
    except Exception:
        print("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åå…¥åŠ›ã‚¹ã‚­ãƒƒãƒ—")

    password_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.NAME, "password"))
    )
    password_input.send_keys(TWITTER_PASSWORD)
    password_input.send_keys(Keys.ENTER)
    time.sleep(6)

    cookies = driver.get_cookies()
    with open("twitter_cookies.json", "w") as f:
        json.dump(cookies, f)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ â†’ æŠ•ç¨¿è€…ãƒšãƒ¼ã‚¸ã«é·ç§»")
    driver.get(f"https://twitter.com/{EXTRACT_TARGET}")


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  â† ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=options)


def extract_tweet_id(article):
    href_els = article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
    for el in href_els:
        h = el.get_attribute("href")
        m = re.search(r"/status/(\d+)", h or "")
        if m:
            return m.group(1)
    return None


def ocr_image(image_path):
    try:
        img = Image.open(image_path)
        img = img.convert("L")
        img = img.resize((img.width * 2, img.height * 2))
        img = ImageEnhance.Contrast(img).enhance(2.0)
        img = img.filter(ImageFilter.SHARPEN)
        import numpy as np

        img_np = np.array(img)
        img_np = cv2.medianBlur(img_np, 3)
        _, img_np = cv2.threshold(img_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        img = Image.fromarray(img_np)
        text = pytesseract.image_to_string(img, lang="jpn", config="--oem 1 --psm 6")
        print(f"ğŸ“ OCRç”»åƒ({image_path})çµæœ:\n{text.strip()}")
        if not text.strip() or sum(c.isalnum() for c in text) < 3:
            print(f"âš ï¸ OCRç”»åƒ({image_path})ã§æ–‡å­—åŒ–ã‘ã¾ãŸã¯èªè­˜å¤±æ•—ã®å¯èƒ½æ€§")
        return text.strip()
    except Exception as e:
        print(f"OCRå¤±æ•—({image_path}): {e}")
        return "[OCRã‚¨ãƒ©ãƒ¼]"


def extract_self_replies(driver, username):
    replies = []
    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” extract_self_replies: ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–")
            break

        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")

        def is_quote_reply(article):
            quote_els = article.find_elements(
                By.XPATH,
                ".//*[contains(text(), 'å¼•ç”¨')] | .//*[contains(text(), 'Quote')]",
            )
            quote_struct = article.find_elements(
                By.XPATH, ".//div[contains(@aria-label, 'å¼•ç”¨')]"
            )
            return bool(quote_els or quote_struct)

        for article in articles:
            try:
                handle_el = article.find_element(
                    By.XPATH,
                    ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                )
                handle = handle_el.text.strip()
                if handle.replace("@", "") != username:
                    continue

                if is_quote_reply(article):
                    print("âš ï¸ extract_self_replies: å¼•ç”¨RTå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                reply_text = text_el.text.strip() if text_el and text_el.text else ""

                tweet_id = extract_tweet_id(article)
                if not tweet_id:
                    print("âš ï¸ extract_self_replies: tweet_idãŒå–å¾—ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ç”»åƒãƒ»å‹•ç”»æƒ…å ±ã‚‚å–å¾—
                images = [
                    img.get_attribute("src")
                    for img in article.find_elements(
                        By.XPATH,
                        ".//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                    )
                    if img.get_attribute("src")
                ]
                video_posters = [
                    v.get_attribute("poster")
                    for v in article.find_elements(By.XPATH, ".//video")
                    if v.get_attribute("poster")
                ]

                if reply_text:
                    replies.append(
                        {
                            "id": tweet_id,
                            "text": reply_text,
                            "images": images,
                            "video_posters": video_posters,
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue
    return replies


def is_ad_post(text):
    lowered = text.lower()
    return any(k.lower() in lowered for k in AD_KEYWORDS)


def extract_thread_from_detail_page(driver, tweet_url):
    print(f"\nğŸ•µï¸ æŠ•ç¨¿ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {tweet_url}")
    driver.get(tweet_url)
    time.sleep(3)  # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å¾…ã¡

    if (
        "Something went wrong" in driver.page_source
        or "ã“ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ã¾ã›ã‚“" in driver.page_source
    ):
        print(f"âŒ æŠ•ç¨¿ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {tweet_url}")
        return []

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿è¨˜äº‹ã®å–å¾—ã«å¤±æ•—: {e}")
        return []

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    tweet_blocks = []
    current_id_from_url = re.sub(r"\D", "", tweet_url.split("/")[-1])

    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")
    print(f"cellInnerDivæ•°: {len(cell_divs)}")
    cell_divs = sorted(cell_divs, key=get_transform_y)

    found_other_user_reply_in_thread = False
    for cell_idx, cell in enumerate(cell_divs):
        if found_other_user_reply_in_thread:
            print(
                f"ğŸ›‘ ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ä»–äººãƒªãƒ—ãƒ©ã‚¤æ¤œå‡ºæ¸ˆã¿ã®ãŸã‚ã€cell {cell_idx + 1} ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—"
            )
            break

        articles_in_cell = cell.find_elements(
            By.XPATH, ".//article[@data-testid='tweet']"
        )
        if not articles_in_cell:
            continue

        for article_idx, article in enumerate(articles_in_cell):
            if found_other_user_reply_in_thread:
                break

            tweet_id = None  # ãƒ«ãƒ¼ãƒ—ã®å…ˆé ­ã§åˆæœŸåŒ–
            try:
                href_el = article.find_element(
                    By.XPATH, ".//a[contains(@href, '/status/')]"
                )
                href = href_el.get_attribute("href")
                match = re.search(r"/status/(\d{10,})", href)
                tweet_id = match.group(1) if match else None

                if not tweet_id:
                    # print("âš ï¸ articleã‹ã‚‰tweet_idæŠ½å‡ºå¤±æ•—ã€ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                username = ""
                try:
                    username_el = article.find_element(
                        By.XPATH,
                        ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                    )
                    username = username_el.text.replace("@", "").strip()
                except:
                    pass  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒå–ã‚Œãªã„å ´åˆã‚‚ã‚ã‚‹

                if not username:
                    print(
                        f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒå–å¾—ã§ããªã‹ã£ãŸæŠ•ç¨¿ï¼ˆID: {tweet_id}ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚"
                    )
                    continue

                # å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¥å¤–ã®æŠ•ç¨¿ã¯ã€ãã‚ŒãŒã‚¹ãƒ¬ãƒƒãƒ‰ã®èµ·ç‚¹URLã®æŠ•ç¨¿ã§ãªã‘ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
                if username != EXTRACT_TARGET:
                    if tweet_id != current_id_from_url:  # èµ·ç‚¹URLã®IDã¨æ¯”è¼ƒ
                        print(
                            f"ğŸ›‘ ä»–äººã®æŠ•ç¨¿ï¼ˆ@{username}ã€ID: {tweet_id}ï¼‰ã‚’æ¤œå‡ºã€‚ä»¥é™ã®å–å¾—ã‚’åœæ­¢ã€‚"
                        )
                        found_other_user_reply_in_thread = True
                        break
                    else:
                        # èµ·ç‚¹æŠ•ç¨¿ãŒå¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ãªã„å ´åˆã¯ã€ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰å…¨ä½“ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã¹ã
                        # ã“ã®ãƒã‚§ãƒƒã‚¯ã¯å¾Œæ®µã® initial_post_data ã§è¡Œã†
                        print(
                            f"ğŸ”¶ èµ·ç‚¹æŠ•ç¨¿({current_id_from_url})ãŒä»–äºº(@{username})ã§ã™ãŒã€ä¸€åº¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚"
                        )

                text = ""
                try:
                    tweet_div = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    raw_text_content = driver.execute_script(
                        "return arguments[0].textContent;", tweet_div
                    )
                    text = raw_text_content.strip() if raw_text_content else ""
                except Exception as e_text:
                    # print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ (ID: {tweet_id}): {e_text}")
                    text = ""  # ãƒ†ã‚­ã‚¹ãƒˆãŒãªãã¦ã‚‚ä»–ã®æƒ…å ±ã¯å–å¾—è©¦è¡Œ

                # --- ç”»åƒåé›†ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ ---
                images = []
                # 1. é€šå¸¸ã®ãƒ¡ãƒ‡ã‚£ã‚¢ç”»åƒ (tweetPhoto å†…)
                tweet_photo_elements = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media')]",
                )
                for img_el in tweet_photo_elements:
                    try:
                        closest_article_anc = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_article_anc == article:
                            src = img_el.get_attribute("src")
                            if src and src not in images:
                                images.append(src)
                    except StaleElementReferenceException:
                        print(
                            f"âš ï¸ ç”»åƒ(media)è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})"
                        )
                        continue
                    except Exception:
                        pass  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

                # 2. ã‚«ãƒ¼ãƒ‰ç”»åƒ (article å†…ã®ã©ã“ã‹ã«ã‚ã‚‹ card_img)
                card_image_elements = article.find_elements(
                    By.XPATH, ".//img[contains(@src, 'twimg.com/card_img')]"
                )
                for img_el in card_image_elements:
                    try:
                        closest_article_anc = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_article_anc == article:
                            # ã“ã® card_img ãŒãƒã‚¹ãƒˆã•ã‚ŒãŸå¼•ç”¨RTã®ä¸€éƒ¨ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                            is_in_quote_rt = False
                            try:
                                # card_img ã®ç¥–å…ˆã« role="link" ãŒã‚ã‚Šã€ãã®ä¸­ã«ã•ã‚‰ã« article ãŒã‚ã‚Œã°å¼•ç”¨RTå†…ã®ã‚«ãƒ¼ãƒ‰
                                quote_container = img_el.find_element(
                                    By.XPATH,
                                    "ancestor::div[@role='link'][.//article[@data-testid='tweet']]",
                                )
                                if quote_container:
                                    is_in_quote_rt = True
                            except:  # role="link" ãŒãªã‘ã‚Œã°å¼•ç”¨RTå†…ã§ã¯ãªã„
                                pass

                            if not is_in_quote_rt:
                                src = img_el.get_attribute("src")
                                if src and src not in images:
                                    images.append(src)
                    except StaleElementReferenceException:
                        print(
                            f"âš ï¸ ç”»åƒ(card)è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})"
                        )
                        continue
                    except Exception:
                        pass  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                # --- ç”»åƒåé›†ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ã“ã“ã¾ã§ ---

                video_posters = []
                video_elements_in_article = article.find_elements(
                    By.XPATH, ".//div[@data-testid='videoPlayer']//video[@poster]"
                )
                for v_el in video_elements_in_article:
                    try:
                        closest_article_anc = v_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_article_anc == article:
                            poster_url = v_el.get_attribute("poster")
                            if poster_url:
                                poster_filename = (
                                    f"video_poster_{tweet_id}_{len(video_posters)}.jpg"
                                )
                                temp_poster_dir = "temp_posters"
                                if not os.path.exists(temp_poster_dir):
                                    os.makedirs(temp_poster_dir)
                                poster_path = os.path.join(
                                    temp_poster_dir, poster_filename
                                )
                                try:
                                    resp = requests.get(
                                        poster_url, stream=True, timeout=10
                                    )
                                    with open(poster_path, "wb") as f:
                                        for chunk in resp.iter_content(1024):
                                            f.write(chunk)
                                    video_posters.append(poster_path)
                                except Exception as e_poster:
                                    print(
                                        f"âŒ posterç”»åƒä¿å­˜å¤±æ•— (ID: {tweet_id}): {e_poster}"
                                    )
                    except StaleElementReferenceException:
                        print(
                            f"âš ï¸ å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})"
                        )
                        continue
                    except Exception:
                        pass

                time_els = article.find_elements(By.XPATH, ".//time")
                date_str = time_els[0].get_attribute("datetime") if time_els else None

                tweet_blocks.append(
                    {
                        "article_element": article,  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºç”¨ã«ä¿æŒ
                        "text": text,
                        "date": date_str,
                        "id": tweet_id,
                        "username": username,
                        "images": images,  # ä¿®æ­£ã•ã‚ŒãŸç”»åƒãƒªã‚¹ãƒˆ
                        "video_posters": video_posters,
                    }
                )

            except StaleElementReferenceException:
                print(
                    f"âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿã€‚articleè¦ç´ ãŒç„¡åŠ¹ã«ãªã‚Šã¾ã—ãŸã€‚ID: {tweet_id if tweet_id else 'ä¸æ˜'}"
                )
                break
            except Exception as e:
                print(
                    f"âš ï¸ articleè§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {str(e)} (ID: {tweet_id if tweet_id else 'ä¸æ˜'})"
                )
                continue

        if found_other_user_reply_in_thread:
            break

    def remove_temp_posters_from_list(blocks_to_clean):
        for block in blocks_to_clean:
            for poster_p in block.get("video_posters", []):
                if os.path.exists(poster_p):
                    try:
                        os.remove(poster_p)
                    except Exception as e_del:
                        print(
                            f"âš ï¸ ä¸€æ™‚ãƒã‚¹ã‚¿ãƒ¼å‰Šé™¤å¤±æ•— (ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—): {poster_p}, {e_del}"
                        )

    if not tweet_blocks:
        print("âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return []

    initial_post_data = next(
        (block for block in tweet_blocks if block["id"] == current_id_from_url), None
    )

    if not initial_post_data:
        print(
            f"âš ï¸ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ãŒæŠ½å‡ºã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    if initial_post_data["username"] != EXTRACT_TARGET:
        print(
            f"ğŸ›‘ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼(@{initial_post_data['username']})ãŒå¯¾è±¡({EXTRACT_TARGET})ã¨ç•°ãªã‚Šã¾ã™ã€‚ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¯ç„¡åŠ¹ã§ã™ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    final_results = []
    for block_item in tweet_blocks:
        # article_element ã¯ extract_metrics ã«æ¸¡ã™ãŸã‚ã«å¿…è¦
        if "article_element" not in block_item:
            remove_temp_posters_from_list([block_item])  # ãƒã‚¹ã‚¿ãƒ¼ãŒã‚ã‚Œã°å‰Šé™¤
            continue

        if block_item["username"] != EXTRACT_TARGET:
            remove_temp_posters_from_list([block_item])
            continue

        if is_ad_post(block_item["text"]):
            print(f"ğŸš« åºƒå‘ŠæŠ•ç¨¿ï¼ˆID: {block_item['id']}ï¼‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            remove_temp_posters_from_list([block_item])
            continue

        impressions, retweets, likes, bookmarks, replies_count = extract_metrics(
            block_item["article_element"]
        )

        # article_element ã¯ final_results ã«ã¯ä¸è¦ãªã®ã§ã“ã“ã§é™¤ãã‹ã€
        # upload_to_notion ã«æ¸¡ã™ç›´å‰ã§é™¤ã
        final_results.append(
            {
                "url": f"https://x.com/{block_item['username']}/status/{block_item['id']}",
                "id": block_item["id"],
                "text": block_item["text"],
                "date": block_item["date"],
                "images": block_item["images"],  # ä¿®æ­£ã•ã‚ŒãŸç”»åƒãƒªã‚¹ãƒˆ
                "username": block_item["username"],
                "impressions": impressions,
                "retweets": retweets,
                "likes": likes,
                "bookmarks": bookmarks,
                "replies": replies_count,
                "video_posters": block_item["video_posters"],
            }
        )

    if not final_results:
        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµæœã€æœ‰åŠ¹ãªæŠ•ç¨¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # ã“ã®æ™‚ç‚¹ã§ tweet_blocks ã«æ®‹ã£ã¦ã„ã‚‹ãŒ final_results ã«ãªã„ã‚‚ã®ã®ãƒã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤
        final_ids = {item["id"] for item in final_results}
        for block in tweet_blocks:
            if block["id"] not in final_ids:
                remove_temp_posters_from_list([block])
        return []

    final_results.sort(key=lambda x: int(x["id"]))  # æŠ•ç¨¿IDæ˜‡é †ã§è¿”ã™
    return final_results


def extract_and_merge_tweets(driver, tweet_urls_data, max_tweets_to_register):
    final_tweets_for_notion = []
    processed_ids = set()
    actually_registered_count = 0

    tweet_urls_data.sort(
        key=lambda x: (
            int(x["id"])
            if isinstance(x, dict) and x.get("id") and x["id"].isdigit()
            else 0
        ),
        reverse=True,
    )

    def is_media_present_in_post(post_data):
        # images ã« card_img ã‚‚å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã®ã§ã€ã“ã‚Œã§OK
        has_images = bool(post_data.get("images"))
        has_video_posters = bool(post_data.get("video_posters"))
        return has_images or has_video_posters

    for i, meta in enumerate(tweet_urls_data):
        if actually_registered_count >= max_tweets_to_register:
            print(f"ğŸ¯ Notionã¸ã®ç™»éŒ²ä»¶æ•°ãŒ {max_tweets_to_register} ã«é”ã—ãŸãŸã‚çµ‚äº†")
            break

        tweet_url = meta["url"] if isinstance(meta, dict) else meta

        try:
            # extract_thread_from_detail_page ã¯ã€å…ƒã®æŠ•ç¨¿ã¨å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’
            # ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿(ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±å«ã‚€)ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™
            thread_posts = extract_thread_from_detail_page(driver, tweet_url)
            if not thread_posts:
                continue

            parent_post_candidate = None

            # thread_posts ã¯æ—¢ã«IDæ˜‡é †ã«ãªã£ã¦ã„ã‚‹ã¯ãš
            for post_in_thread in thread_posts:
                current_post_id = post_in_thread.get("id")

                if not current_post_id:
                    print("âš ï¸ IDãŒãªã„æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                if current_post_id in processed_ids:
                    continue

                if already_registered(current_post_id):
                    processed_ids.add(current_post_id)
                    continue

                # is_reply_structure ã®ã‚ˆã†ãªåˆ¤å®šã¯ extract_tweets ã§æ¸ˆã‚“ã§ã„ã‚‹æƒ³å®š
                # ã“ã“ã§ã¯ extract_target ã®æŠ•ç¨¿ã®ã¿ã‚’æ‰±ã†

                if parent_post_candidate is None:
                    # æœ€åˆã®æŠ•ç¨¿ã‚’è¦ªå€™è£œã¨ã™ã‚‹
                    parent_post_candidate = post_in_thread
                else:
                    # 2ã¤ç›®ä»¥é™ã®æŠ•ç¨¿ã¯ãƒªãƒ—ãƒ©ã‚¤ã¨ã¿ãªã™
                    # ã“ã®ãƒªãƒ—ãƒ©ã‚¤ãŒãƒ¡ãƒ‡ã‚£ã‚¢(card_imgå«ã‚€)ã‚’æŒã¤ã‹åˆ¤å®š
                    reply_has_media = is_media_present_in_post(post_in_thread)

                    if reply_has_media:
                        # ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ããƒªãƒ—ãƒ©ã‚¤ã®å ´åˆ:
                        # 1. ãã‚Œã¾ã§ã®è¦ªå€™è£œã‚’ç™»éŒ²
                        if parent_post_candidate["id"] not in processed_ids:
                            if actually_registered_count < max_tweets_to_register:
                                final_tweets_for_notion.append(parent_post_candidate)
                                actually_registered_count += 1
                                print(
                                    f"âœ… è¦ªæŠ•ç¨¿ã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ (ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ—ãƒ©ã‚¤åˆ†é›¢): {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                                )
                                processed_ids.add(parent_post_candidate["id"])
                            else:
                                print(
                                    f"ğŸ¯ ç™»éŒ²ä¸Šé™({max_tweets_to_register})ã®ãŸã‚ã€è¦ªå€™è£œ {parent_post_candidate['id']} ã¯ç™»éŒ²ã‚¹ã‚­ãƒƒãƒ—(ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ—ãƒ©ã‚¤åˆ†é›¢æ™‚)"
                                )
                                # ä¸Šé™ã«é”ã—ãŸã‚‰ä»¥é™ã®å‡¦ç†ã¯ä¸è¦
                                break

                        # 2. ã“ã®ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ããƒªãƒ—ãƒ©ã‚¤ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã™ã‚‹
                        parent_post_candidate = post_in_thread
                    else:
                        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒªãƒ—ãƒ©ã‚¤ã®å ´åˆ â†’ è¦ªå€™è£œã«ãƒãƒ¼ã‚¸
                        parent_post_candidate["text"] = (
                            parent_post_candidate.get("text", "")
                            + "\n\n"
                            + post_in_thread.get("text", "")
                        ).strip()
                        processed_ids.add(
                            current_post_id
                        )  # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã‚‚å‡¦ç†æ¸ˆã¿

            # ãƒ«ãƒ¼ãƒ—å¾Œã€æœ€å¾Œã«æ®‹ã£ãŸè¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸
            if (
                parent_post_candidate
                and parent_post_candidate["id"] not in processed_ids
            ):
                if actually_registered_count < max_tweets_to_register:
                    final_tweets_for_notion.append(parent_post_candidate)
                    actually_registered_count += 1
                    print(
                        f"âœ… æœ€çµ‚è¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ : {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                    )
                    processed_ids.add(parent_post_candidate["id"])
                else:
                    print(
                        f"ğŸ¯ ç™»éŒ²ä¸Šé™({max_tweets_to_register})ã®ãŸã‚ã€æœ€çµ‚è¦ªå€™è£œ {parent_post_candidate['id']} ã¯ç™»éŒ²ã‚¹ã‚­ãƒƒãƒ—"
                    )

            # ç™»éŒ²ä¸Šé™ã«é”ã—ã¦ã„ãŸã‚‰å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã‚‚æŠœã‘ã‚‹
            if actually_registered_count >= max_tweets_to_register:
                print(
                    f"ğŸ¯ Notionã¸ã®ç™»éŒ²ä»¶æ•°ãŒ {max_tweets_to_register} ã«é”ã—ãŸãŸã‚ã€URLå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†"
                )
                break

        except Exception as e:
            print(
                f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ ({tweet_url}): {type(e).__name__} - {e}\n{traceback.format_exc()}"
            )
            continue

    print(f"\nğŸ“ˆ æœ€çµ‚çš„ãªNotionç™»éŒ²å¯¾è±¡æŠ•ç¨¿æ•°: {len(final_tweets_for_notion)} ä»¶")
    return final_tweets_for_notion


def extract_metrics(article):
    """
    ã„ã„ã­æ•°ãƒ»ãƒªãƒã‚¹ãƒˆæ•°ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ»ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒ»ãƒªãƒ—ãƒ©ã‚¤æ•°ã‚’æŠ½å‡º
    å–å¾—ã§ããªã„ã‚‚ã®ã¯0ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿Noneï¼‰ã§è¿”ã™
    """
    impressions_str = retweets_str = likes_str = bookmarks_str = replies_str = None
    try:
        # å„ªå…ˆçš„ã« div[role="group"] ã® aria-label ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        # ã“ã‚ŒãŒæœ€ã‚‚æƒ…å ±ãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
        group_divs = article.find_elements(
            By.XPATH, ".//div[@role='group' and @aria-label]"
        )

        primary_label_processed = False
        if group_divs:
            for group_div in group_divs:
                label = group_div.get_attribute("aria-label")
                if not label:
                    continue

                print(f"ğŸŸ¦ metrics group aria-labelå†…å®¹: {label}")
                primary_label_processed = True  # ã“ã®ãƒ©ãƒ™ãƒ«ã‚’å‡¦ç†ã—ãŸã“ã¨ã‚’ãƒãƒ¼ã‚¯

                # å„æŒ‡æ¨™ã‚’å€‹åˆ¥ã«æŠ½å‡ºã™ã‚‹ (é †ç•ªã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«)
                m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                if m_replies:
                    replies_str = m_replies.group(1)

                m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                if m_retweets:
                    retweets_str = m_retweets.group(1)

                m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                if m_likes:
                    likes_str = m_likes.group(1)

                m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                if m_bookmarks:
                    bookmarks_str = m_bookmarks.group(1)

                m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                if m_impressions:
                    impressions_str = m_impressions.group(1)

                # ä¸€ã¤ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰å…¨ã¦å–ã‚ŒãŸã‚‰æŠœã‘ã‚‹ã“ã¨ãŒå¤šã„ãŒã€ç¨€ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ã€
                # åŸºæœ¬çš„ã«ã¯æœ€åˆã® group_div ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸»ã¨ã™ã‚‹ã€‚
                # ã‚‚ã—ã€è¤‡æ•°ã® group_div ãŒç•°ãªã‚‹æƒ…å ±ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒç¢ºèªã•ã‚Œã‚Œã°ã€ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯å†è€ƒã€‚
                break

        if not primary_label_processed:
            # group_div ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€aria-label ãŒãªã„å ´åˆã€ä»¥å‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚è©¦ã™
            # ãŸã ã—ã€ã“ã®ãƒ‘ã‚¹ã¯Xã®UIãŒå¤§ããå¤‰ã‚ã£ãŸå ´åˆã¯æ©Ÿèƒ½ã—ãªã„å¯èƒ½æ€§ãŒé«˜ã„
            other_divs = article.find_elements(
                By.XPATH,
                ".//div[contains(@aria-label, 'ä»¶ã®è¡¨ç¤º') and not(@role='group')]",
            )
            for div in other_divs:
                label = div.get_attribute("aria-label")
                if not label:
                    continue
                print(f"ğŸŸ¦ other metrics div aria-labelå†…å®¹: {label}")
                # ã“ã“ã§ã‚‚åŒæ§˜ã«å€‹åˆ¥æŠ½å‡ºã‚’è©¦ã¿ã‚‹ (ä¸Šè¨˜ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
                if replies_str is None:
                    m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                    if m_replies:
                        replies_str = m_replies.group(1)
                if retweets_str is None:
                    m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                    if m_retweets:
                        retweets_str = m_retweets.group(1)
                if likes_str is None:
                    m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                    if m_likes:
                        likes_str = m_likes.group(1)
                if bookmarks_str is None:
                    m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                    if m_bookmarks:
                        bookmarks_str = m_bookmarks.group(1)
                if impressions_str is None:
                    m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                    if m_impressions:
                        impressions_str = m_impressions.group(1)
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã§å‡¦ç†

        # å€‹åˆ¥ãƒœã‚¿ãƒ³ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—
        if replies_str is None:
            try:
                reply_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='reply']"
                )
                for btn in reply_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label or "")
                    if m:
                        replies_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤æ•°å–å¾—: {replies_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if retweets_str is None:
            try:
                rt_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='retweet']"
                )
                for btn in rt_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label or "")
                    if m:
                        retweets_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒã‚¹ãƒˆæ•°å–å¾—: {retweets_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒã‚¹ãƒˆæ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if likes_str is None:
            try:
                like_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='like']"
                )
                for btn in like_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label or "")
                    if m:
                        likes_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ã„ã„ã­æ•°å–å¾—: {likes_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ã„ã„ã­æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if bookmarks_str is None:
            try:
                bm_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='bookmark']"
                )
                for btn in bm_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label or "")
                    if m:
                        bookmarks_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—: {bookmarks_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã¯ãƒœã‚¿ãƒ³ã‹ã‚‰ã¯é€šå¸¸å–ã‚Œãªã„ã®ã§ã€aria-labelé ¼ã¿
        # ã‚‚ã— impressions_str ãŒ None ã§ã€ä»–ã®æŒ‡æ¨™ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆã€
        # ã‹ã¤ã¦ã®ã€Œã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€
        # likes/retweets/bookmarks/replies ãŒå…¨ã¦0ãªã‚‰ã€impressions_str ã‚’æ¡ç”¨ã—ä»–ã‚’0ã«ã™ã‚‹ã€‚
        # ãŸã ã—ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯è¤‡é›‘ãªã®ã§ã€ä¸€æ—¦ã¯ä¸Šè¨˜ã§å–å¾—ã§ããŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
        # ã‚‚ã—ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã ã‘ãŒå–ã‚Œã¦ä»–ãŒ0ã«ãªã‚‹ã¹ãã‚±ãƒ¼ã‚¹ãŒå¤šç™ºã™ã‚‹ãªã‚‰å†æ¤œè¨ã€‚

        def parse_num(s):
            if not s:
                return 0  # None ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯0ã¨ã—ã¦æ‰±ã† (ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ä»¥å¤–)
            s_cleaned = str(s).replace(",", "")
            if "ä¸‡" in s_cleaned:
                try:
                    return int(float(s_cleaned.replace("ä¸‡", "")) * 10000)
                except ValueError:
                    return 0  # "ä¸‡" ãŒã‚ã£ã¦ã‚‚æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆ
            try:
                return int(s_cleaned)
            except ValueError:  # "K" ã‚„ "M" ãªã©ã®è‹±èªåœã®ç•¥ç§°ã¯ç¾çŠ¶éå¯¾å¿œ
                return 0  # æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆã¯0

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ None ã‚’è¨±å®¹ã—ã€ä»–ã¯0ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
        impressions = (
            parse_num(impressions_str) if impressions_str is not None else None
        )
        retweets = parse_num(retweets_str)
        likes = parse_num(likes_str)
        bookmarks = parse_num(bookmarks_str)
        replies = parse_num(replies_str)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœ€çµ‚çš„ãªå€¤ã‚’è¡¨ç¤º
        print(
            f"ğŸ”¢ æŠ½å‡ºçµæœ: è¡¨ç¤º={impressions}, RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, ãƒªãƒ—ãƒ©ã‚¤={replies}"
        )

    except Exception as e:
        print(f"âš ï¸ extract_metricså…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (impressions=None, ä»–=0)
        impressions = None
        retweets = 0
        likes = 0
        bookmarks = 0
        replies = 0

    return impressions, retweets, likes, bookmarks, replies


def is_reply_structure(
    article,
    tweet_id=None,
    text="",
    image_urls=None,  # ã“ã®image_urlsã¯å‘¼ã³å‡ºã—å…ƒ(extract_tweets)ã§åé›†ã•ã‚ŒãŸã€ç¾åœ¨ã®articleã«ç›´æ¥å±ã™ã‚‹ç”»åƒURL
    video_poster_urls=None,  # åŒæ§˜ã«ã€ç¾åœ¨ã®articleã«ç›´æ¥å±ã™ã‚‹å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼URL
):
    try:
        id_display = f"ï¼ˆID={tweet_id}ï¼‰" if tweet_id else ""

        # 1. åºƒå‘ŠæŠ•ç¨¿ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ (is_ad_post ã¯åˆ¥é€”å®šç¾©ã•ã‚Œã¦ã„ã‚‹æƒ³å®š)
        # if is_ad_post(text): # is_reply_structure ã®è²¬å‹™ã§ã¯ãªã„ãŸã‚ã€å‘¼ã³å‡ºã—å…ƒã§è¡Œã†
        #     print(f"ğŸš« is_reply_structure: åºƒå‘Šåˆ¤å®š â†’ é™¤å¤– {id_display}")
        #     return True

        # 2. å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆã®åˆ¤å®š
        # å¼•ç”¨RTã¯ã€è‡ªèº«ã® <article> å†…ã«ã€å¼•ç”¨å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®
        # <div role="link"> (ã¾ãŸã¯é¡ä¼¼ã®ã‚³ãƒ³ãƒ†ãƒŠ) ãŒã‚ã‚Šã€ãã®ä¸­ã«ã•ã‚‰ã« <article data-testid="tweet"> ãŒãƒã‚¹ãƒˆã•ã‚Œã‚‹æ§‹é€ ãŒå¤šã„ã€‚
        quoted_tweet_articles_in_link_role = article.find_elements(
            By.XPATH, ".//div[@role='link' and .//article[@data-testid='tweet']]"
        )
        is_quote_tweet_structure = len(quoted_tweet_articles_in_link_role) > 0

        if is_quote_tweet_structure:
            text_length = len(text.strip()) if text else 0

            # å¼•ç”¨RTæœ¬ä½“ãŒæŒã¤ãƒ¡ãƒ‡ã‚£ã‚¢ã®åˆ¤å®š
            has_own_images = bool(
                image_urls and any("twimg.com/media" in url for url in image_urls)
            )
            has_own_video_posters = bool(video_poster_urls)
            has_own_card_img = bool(
                image_urls and any("twimg.com/card_img" in url for url in image_urls)
            )

            quote_rt_has_own_media = (
                has_own_images or has_own_video_posters or has_own_card_img
            )

            # ãƒ«ãƒ¼ãƒ«: ã€Œ50æ–‡å­—ä»¥ä¸Šã€ã‹ã¤ã€Œãƒ¡ãƒ‡ã‚£ã‚¢ãŒãªã„ã€å¼•ç”¨RTã¯å–å¾—ã—ãªã„ (ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹)
            if text_length >= 50 and not quote_rt_has_own_media:
                print(
                    f"ğŸ›‘ is_reply_structure: å¼•ç”¨RTï¼ˆ50æ–‡å­—ä»¥ä¸Š ã‹ã¤ æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—ï¼‰â†’ é™¤å¤– {id_display} | é•·ã•={text_length}"
                )
                return True  # ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ (å–å¾—ã—ãªã„)
            else:
                # ä¸Šè¨˜ã®ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã«è©²å½“ã—ãªã„å¼•ç”¨RTã¯ã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ã¯å–å¾—å¯¾è±¡ã¨ã™ã‚‹
                print(
                    f"âœ… is_reply_structure: å¼•ç”¨RTï¼ˆä¸Šè¨˜é™¤å¤–æ¡ä»¶ã«è©²å½“ã›ãšï¼‰â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ {id_display} | é•·ã•={text_length} | æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Š={quote_rt_has_own_media}"
                )
                return False  # ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ (å–å¾—ã™ã‚‹)

        # 3. é€šå¸¸ã®ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ã®åˆ¤å®š
        #   - ã€Œè¿”ä¿¡å…ˆ: @usernameã€ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹
        #   - æŠ•ç¨¿ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã®æ•°ãŒå°‘ãªã„ã‹ï¼ˆé€šå¸¸æŠ•ç¨¿ã¯4ã¤ä»¥ä¸Šã€ãƒªãƒ—ãƒ©ã‚¤ã¯å°‘ãªã„ã“ã¨ãŒã‚ã‚‹ï¼‰

        # è¿”ä¿¡å…ˆè¡¨ç¤ºã®ç¢ºèª (ã‚ˆã‚Šç¢ºå®Ÿãªãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š)
        # XPathã‚’èª¿æ•´ã—ã¦ã€articleç›´ä¸‹ã®è¦ç´ ã«é™å®šã™ã‚‹ã‹ã€ã‚ˆã‚Šå…·ä½“çš„ãªæ§‹é€ ã‚’æŒ‡å®šã™ã‚‹
        reply_to_indicator = article.find_elements(
            By.XPATH,
            ".//div[contains(text(), 'Replying to') or contains(text(), 'è¿”ä¿¡å…ˆ:') or starts-with(.//span/text(), 'Replying to') or starts-with(.//span/text(), 'è¿”ä¿¡å…ˆ:')]",
        )
        if reply_to_indicator:
            # ã•ã‚‰ã«ã€ãã®è¿”ä¿¡å…ˆè¡¨ç¤ºãŒå®Ÿéš›ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹ï¼ˆéè¡¨ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ã§ãªã„ã‹ï¼‰ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚‚æ¤œè¨
            is_indicator_visible = False
            for indicator_el in reply_to_indicator:
                try:
                    if indicator_el.is_displayed():
                        is_indicator_visible = True
                        break
                except StaleElementReferenceException:
                    # è¦ç´ ãŒæ¶ˆãˆãŸå ´åˆã¯ç„¡è¦–
                    pass
            if is_indicator_visible:
                print(
                    f"ğŸ’¬ is_reply_structure: è¿”ä¿¡å…ˆè¡¨ç¤ºã‚ã‚Š â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š {id_display}"
                )
                return True

        # ãƒœã‚¿ãƒ³ã®æ•°ã«ã‚ˆã‚‹åˆ¤å®š (è£œåŠ©çš„ã€ã¾ãŸã¯ä¸Šè¨˜ã§åˆ¤å®šã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã®ãƒ„ã‚¤ãƒ¼ãƒˆã¨è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ãƒœã‚¿ãƒ³æ§‹é€ ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ã®ã§æ³¨æ„
        # data-testid ã‚’æŒã¤ button è¦ç´ ã‚’æ•°ãˆã‚‹
        buttons = article.find_elements(
            By.XPATH, ".//div[@role='group']//button[@data-testid]"
        )
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã§ã¯é€šå¸¸4ã¤ (reply, retweet, like, view/bookmark)
        # ãƒªãƒ—ãƒ©ã‚¤ã®å ´åˆã€viewãŒãªã„ã“ã¨ãŒã‚ã‚‹ (3ã¤ã«ãªã‚‹)
        # éå¸¸ã«å¤ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚„ç‰¹æ®Šãªã‚±ãƒ¼ã‚¹ã§ã¯ã•ã‚‰ã«å°‘ãªã„ã“ã¨ã‚‚
        if len(buttons) < 4:  # é–¾å€¤ã¯çŠ¶æ³ã«å¿œã˜ã¦èª¿æ•´
            print(
                f"ğŸ’¬ is_reply_structure: ãƒœã‚¿ãƒ³æ•° {len(buttons)} å€‹ï¼ˆ4æœªæº€ï¼‰â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®šã®å¯èƒ½æ€§ {id_display}"
            )
            # ã“ã‚Œã ã‘ã§ã¯æ–­å®šã§ããªã„å ´åˆã‚‚ã‚ã‚‹ã®ã§ã€ä»–ã®è¦ç´ ã¨çµ„ã¿åˆã‚ã›ã‚‹ã‹ã€
            # ã“ã‚Œã‚’ãƒªãƒ—ãƒ©ã‚¤ã¨ã¿ãªã™ã‹ã©ã†ã‹ã®åˆ¤æ–­ã¯è¦ä»¶ã«ã‚ˆã‚‹
            return True  # ã“ã“ã§ã¯ãƒœã‚¿ãƒ³æ•°ãŒå°‘ãªã‘ã‚Œã°ãƒªãƒ—ãƒ©ã‚¤ã¨ã¿ãªã™

        # ä¸Šè¨˜ã®ã„ãšã‚Œã®æ¡ä»¶ï¼ˆå¼•ç”¨RTã®é™¤å¤–ã€é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ï¼‰ã«ã‚‚è©²å½“ã—ãªã„å ´åˆã¯è¦ªæŠ•ç¨¿ã¨ã¿ãªã™
        print(
            f"âœ… is_reply_structure: æ§‹é€ ä¸Šå•é¡Œãªã—ï¼ˆéå¼•ç”¨RTã€éãƒªãƒ—ãƒ©ã‚¤ï¼‰â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤å®š {id_display}"
        )
        return False

    except StaleElementReferenceException:
        print(
            f"âš ï¸ is_reply_structure: StaleElementReferenceExceptionç™ºç”Ÿ {id_display} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False  # è¦ç´ ãŒç„¡åŠ¹ã«ãªã£ãŸå ´åˆã¯ã€èª¤ã£ã¦é™¤å¤–ã—ãªã„ã‚ˆã†ã«Falseã‚’è¿”ã™ï¼ˆè¦ä»¶ã«ã‚ˆã‚‹ï¼‰
    except Exception as e:
        print(
            f"âš ï¸ is_reply_structure: åˆ¤å®šã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e).__name__}: {e} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨å´ã«å€’ã™


def has_media_in_html(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    # ç”»åƒåˆ¤å®š
    if soup.find("img", {"src": lambda x: x and "twimg.com/media" in x}):
        return True
    # å‹•ç”»åˆ¤å®š
    if soup.find("div", {"data-testid": "video-player-mini-ui-"}):
        return True
    if soup.find("button", {"aria-label": "å‹•ç”»ã‚’å†ç”Ÿ"}):
        return True
    if soup.find("video"):
        return True
    return False


def extract_tweets(driver, extract_target, max_tweets):
    print(f"\nâœ¨ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: https://twitter.com/{extract_target}")
    driver.get(f"https://twitter.com/{extract_target}")
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, "//article"))
    )

    tweet_urls = []
    seen_urls = set()
    scroll_count = 0
    max_scrolls = 20  # ä¾‹ãˆã°20å›ã‚’æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã¨ã—ã¦è¨­å®š
    # ... (rest of the variables) ...
    pause_counter = 0
    pause_threshold = 3
    last_seen_count = 0

    while scroll_count < max_scrolls and len(tweet_urls) < max_tweets:
        print(f"\nğŸ” ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {scroll_count + 1}/{max_scrolls} å›ç›®")
        current_articles_in_dom = driver.find_elements(
            By.XPATH, "//article[@data-testid='tweet']"
        )
        print(f"ğŸ“„ ç¾åœ¨ã®articleæ•°: {len(current_articles_in_dom)}")

        new_tweets_found_in_scroll = 0
        for i, article in enumerate(current_articles_in_dom):
            try:
                # ... (URL and tweet_id extraction logic remains the same) ...
                href_els = article.find_elements(
                    By.XPATH,
                    ".//a[contains(@href, '/status/') and not(ancestor::div[contains(@style, 'display: none')])]",
                )
                if not href_els:
                    continue

                tweet_url = ""
                for href_el in href_els:
                    href_attr = href_el.get_attribute("href")
                    if href_attr and "/status/" in href_attr:
                        if f"/{extract_target}/status/" in href_attr:
                            tweet_url = (
                                href_attr
                                if href_attr.startswith("http")
                                else f"https://x.com{href_attr}"
                            )
                            break
                if not tweet_url:
                    first_href = href_els[0].get_attribute("href")
                    tweet_url = (
                        first_href
                        if first_href.startswith("http")
                        else f"https://x.com{first_href}"
                    )

                tweet_id_match = re.search(r"/status/(\d+)", tweet_url)
                if not tweet_id_match:
                    print(f"âš ï¸ URLã‹ã‚‰tweet_idæŠ½å‡ºå¤±æ•—: {tweet_url}")
                    continue
                tweet_id = tweet_id_match.group(1)

                if tweet_url in seen_urls:
                    continue

                username_in_url_match = re.search(r"x\.com/([^/]+)/status", tweet_url)
                if (
                    not username_in_url_match
                    or username_in_url_match.group(1).lower() != extract_target.lower()
                ):
                    continue

                text_el = None
                try:
                    text_el = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                except:
                    pass
                text = normalize_text(text_el.text) if text_el and text_el.text else ""

                # ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã®æŠ½å‡ºï¼ˆis_reply_structureã«æ¸¡ã™ãŸã‚ï¼‰
                # ç¾åœ¨ã® 'article' ã«ç›´æ¥å±ã™ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
                images_for_check = []
                image_elements = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                )
                for img_el in image_elements:
                    # ã“ã®img_elãŒç¾åœ¨ã®articleç›´ä¸‹ï¼ˆã¾ãŸã¯ãã®tweetPhotoå†…ï¼‰ã«ã‚ã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸå¼•ç”¨RTå†…ã®ã‚‚ã®ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    # æœ€ã‚‚è¿‘ã„ç¥–å…ˆã®articleãŒç¾åœ¨ã®articleè‡ªèº«ã§ã‚ã‚‹ã‹ã§åˆ¤å®š
                    try:
                        closest_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            src = img_el.get_attribute("src")
                            if src:
                                images_for_check.append(src)
                    except StaleElementReferenceException:  # è¦ç´ ãŒæ¶ˆãˆãŸå ´åˆ
                        print(f"âš ï¸ ç”»åƒè¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                        pass

                videos_for_check = []
                video_elements = article.find_elements(
                    By.XPATH, ".//div[@data-testid='videoPlayer']//video[@poster]"
                )
                for video_el in video_elements:
                    try:
                        closest_ancestor_article = video_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            poster = video_el.get_attribute("poster")
                            if poster:
                                videos_for_check.append(poster)
                    except StaleElementReferenceException:
                        print(f"âš ï¸ å‹•ç”»è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:
                        pass

                # has_media ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ is_reply_structure ã‹ã‚‰å‰Šé™¤ã—ãŸã®ã§ã€ã“ã“ã§ã‚‚æ¸¡ã•ãªã„
                if is_reply_structure(
                    article,
                    tweet_id=tweet_id,
                    text=text,
                    # has_media=has_media_for_check, # å‰Šé™¤
                    image_urls=images_for_check,
                    video_poster_urls=videos_for_check,
                ):
                    continue

                if is_ad_post(text):
                    continue

                tweet_urls.append({"url": tweet_url, "id": tweet_id})
                seen_urls.add(tweet_url)
                new_tweets_found_in_scroll += 1

                print(f"âœ… åé›†å€™è£œã«è¿½åŠ : {tweet_url} ({len(tweet_urls)}ä»¶ç›®)")
                if len(tweet_urls) >= max_tweets:
                    break

            except StaleElementReferenceException:
                print(
                    "âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿã€‚DOMãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚"
                )
                break
            except Exception as e:
                print(
                    f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e} (URL: {tweet_url if 'tweet_url' in locals() else 'ä¸æ˜'})"
                )
                continue

        # ... (rest of the scroll and loop logic) ...
        if len(tweet_urls) >= max_tweets:
            print(
                f"ğŸ¯ åé›†å€™è£œæ•°ãŒMAX_TWEETS ({max_tweets}) ã«é”ã—ãŸãŸã‚ã€URLåé›†ã‚’çµ‚äº†ã€‚"
            )
            break

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2.5)
        scroll_count += 1

        if new_tweets_found_in_scroll == 0:
            pause_counter += 1
            print(
                f"ğŸ§Š ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§æ–°è¦æŠ•ç¨¿ãªã— â†’ pause_counter={pause_counter}/{pause_threshold}"
            )
            if pause_counter >= pause_threshold:
                print("ğŸ›‘ æ–°ã—ã„æŠ•ç¨¿ãŒé€£ç¶šã—ã¦æ¤œå‡ºã•ã‚Œãªã„ãŸã‚URLåé›†ã‚’ä¸­æ–­")
                break
        else:
            pause_counter = 0

    print(f"\nğŸ“ˆ åé›†å€™è£œã®URLå–å¾—å®Œäº† â†’ åˆè¨ˆ: {len(tweet_urls)} ä»¶")
    return tweet_urls


def already_registered(tweet_id):
    if not tweet_id or not tweet_id.isdigit():
        return False
    query = {"filter": {"property": "æŠ•ç¨¿ID", "rich_text": {"equals": tweet_id}}}
    try:
        result = notion.databases.query(database_id=DATABASE_ID, **query)
        return len(result.get("results", [])) > 0
    except Exception as e:
        print(f"âš ï¸ Notionã‚¯ã‚¨ãƒªå¤±æ•—: {e}")
        return False


def ocr_and_remove_image(image_path, label=None):
    """
    ç”»åƒãƒ‘ã‚¹ã‚’å—ã‘å–ã‚ŠOCRã—ã€ä½¿ç”¨å¾Œã«å‰Šé™¤ã™ã‚‹ã€‚
    labelãŒã‚ã‚Œã°çµæœã®å…ˆé ­ã«ä»˜ä¸ã€‚
    """
    result = ""
    try:
        ocr_result = ocr_image(image_path)
        if ocr_result:
            cleaned = clean_ocr_text(ocr_result)
            result = f"[{label}]\n{cleaned}" if label else cleaned
    except Exception as e:
        print(f"âš ï¸ OCRå¤±æ•—: {e}")
    finally:
        try:
            os.remove(image_path)
            print(f"ğŸ—‘ï¸ ç”»åƒå‰Šé™¤: {image_path}")
        except Exception as e:
            print(f"âš ï¸ ç”»åƒå‰Šé™¤å¤±æ•—: {e}")
    return result


def clean_ocr_text(text):
    # é™¤å¤–ã—ãŸã„æ–‡è¨€ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã“ã“ã«è¿½åŠ 
    EXCLUDE_PATTERNS = [
        "æœè³ªå•ã‚’ã€Œã„ã„ã­!ã€ ã™ã‚‹",
        "ã“ã®æŠ•ç¨¿ã‚’ã„ã„ã­ï¼",
        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    ]
    lines = text.splitlines()
    cleaned = [
        line for line in lines if not any(pat in line for pat in EXCLUDE_PATTERNS)
    ]
    return "\n".join(cleaned)


def upload_to_notion(tweet):
    print(f"ğŸ“¤ Notionç™»éŒ²å‡¦ç†é–‹å§‹: {tweet['id']}")
    print(f"ğŸ–¼ï¸ images: {tweet.get('images')}")

    if already_registered(tweet["id"]):
        print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ—æ¸ˆ: {tweet['id']}")
        return

    props = {
        "æŠ•ç¨¿ID": {
            "rich_text": [{"type": "text", "text": {"content": str(tweet["id"])}}]
        },
        "æœ¬æ–‡": {
            "rich_text": [
                {
                    "type": "text",
                    "text": {
                        "content": tweet["text"],
                        "link": None,
                    },
                    "annotations": {
                        "bold": False,
                        "italic": False,
                        "strikethrough": False,
                        "underline": False,
                        "code": False,
                        "color": "default",
                    },
                }
            ]
        },
        "URL": {"url": tweet["url"]},
        "æŠ•ç¨¿æ—¥æ™‚": {"date": {"start": tweet["date"]} if tweet["date"] else None},
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": {"select": {"name": "æœªå›ç­”"}},
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°": {
            "number": (
                int(tweet["impressions"])
                if tweet.get("impressions") is not None
                else None
            )
        },
        "ãƒªãƒã‚¹ãƒˆæ•°": {
            "number": int(tweet["retweets"]) if tweet.get("retweets") is not None else 0
        },
        "ã„ã„ã­æ•°": {
            "number": int(tweet["likes"]) if tweet.get("likes") is not None else 0
        },
        "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°": {
            "number": (
                int(tweet["bookmarks"]) if tweet.get("bookmarks") is not None else 0
            )
        },
        "ãƒªãƒ—ãƒ©ã‚¤æ•°": {
            "number": int(tweet["replies"]) if tweet.get("replies") is not None else 0
        },
        "æ–‡å­—èµ·ã“ã—": {"rich_text": []},
    }

    ocr_texts = []

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®OCRï¼ˆtweet["images"]ï¼‰
    for idx, img_url in enumerate(tweet.get("images", [])):
        img_path = f"ocr_image_{tweet['id']}_{idx}.jpg"
        try:
            resp = requests.get(img_url, stream=True)
            with open(img_path, "wb") as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            ocr_text = ocr_and_remove_image(img_path, label=f"ç”»åƒ{idx+1}")
            if ocr_text:
                ocr_texts.append(ocr_text)
        except Exception as e:
            print(f"âš ï¸ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

    # posterç”»åƒã®OCR
    poster_paths = tweet.get("video_posters") or []
    if isinstance(poster_paths, str):
        poster_paths = [poster_paths]
    for idx, poster_path in enumerate(poster_paths):
        ocr_text = ocr_and_remove_image(poster_path, label=f"å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«{idx+1}")
        if ocr_text:
            ocr_texts.append(ocr_text)

    if ocr_texts:
        props["æ–‡å­—èµ·ã“ã—"]["rich_text"] = [
            {"type": "text", "text": {"content": "\n\n".join(ocr_texts)}}
        ]

    children_blocks = []

    try:
        new_page = notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties=props,
            children=children_blocks,
        )
        print(f"ğŸ“ Notionç™»éŒ²å®Œäº†: {tweet['url']}")
    except Exception as e:
        print(f"âŒ Notionç™»éŒ²å¤±æ•—: {tweet['id']} ã‚¨ãƒ©ãƒ¼: {e}")


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def is_recruit_account(name, bio, config):
    return any(
        k in name or k in bio for k in config.get("filter_keywords_name_bio", [])
    )


def is_recruit_post(text, config):
    return any(k in text for k in config.get("filter_keywords_tweet", []))


def search_accounts(driver, keyword_list):
    results = []
    for keyword in keyword_list:
        search_url = f"https://twitter.com/search?q={keyword}&f=user"
        driver.get(search_url)
        time.sleep(3)

        # âš  æ–°UIæ§‹é€ ã«å¯¾å¿œ
        users = driver.find_elements(
            By.XPATH, "//a[contains(@href, '/')]//div[@dir='auto']/../../.."
        )
        print(f"ğŸ” å€™è£œãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¶æ•°: {len(users)}")

        for user in users:
            try:
                spans = user.find_elements(By.XPATH, ".//span")
                name = ""
                username = ""

                for span in spans:
                    text = span.text.strip()
                    if text.startswith("@"):
                        username = text.replace("@", "")
                    elif not name:
                        name = text

                if username and name:
                    results.append(
                        {
                            "name": name,
                            "username": username,
                            "bio": "",  # ã“ã®æ®µéšã§ã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»é¢ã«é£›ã‚“ã§ã„ãªã„
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºå¤±æ•—: {e}")
                continue

    return results


def merge_replies_with_driver(driver, tweet):
    try:
        driver.get(tweet["url"])
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )

        replies = extract_self_replies(driver, tweet.get("username", ""))
        if not isinstance(replies, list):
            print(
                f"âš ï¸ merge_replies_with_driver() ã§å–å¾—ã—ãŸrepliesãŒä¸æ­£ãªå‹: {type(replies)} â†’ ç©ºãƒªã‚¹ãƒˆã«ç½®æ›"
            )
            replies = []

        tweet_text = tweet.get("text") or ""
        replies = sorted(
            [r for r in replies if r.get("id") and r.get("text")],
            key=lambda x: int(x["id"]),
        )

        existing_chunks = set(tweet_text.strip().split("\n\n"))
        reply_texts = []

        for r in replies:
            # ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ãªã„
            if r.get("images") or r.get("video_posters"):
                print(
                    f"ğŸ›‘ ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ã¾ã›ã‚“: {r['id']}"
                )
                continue

            reply_id = r["id"]
            reply_body = r["text"].strip()
            clean_body = reply_body[:20].replace("\n", " ")
            print(f"ğŸ§µ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå€™è£œ: ID={reply_id} | textå…ˆé ­: {clean_body}")

            if not reply_body:
                continue
            if reply_body in existing_chunks:
                continue
            if r["id"] == tweet["id"]:
                continue

            reply_texts.append(reply_body)
            existing_chunks.add(reply_body)

        if reply_texts:
            tweet["text"] = tweet_text + "\n\n" + "\n\n".join(reply_texts)

    except Exception as e:
        print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå¤±æ•—ï¼ˆ{tweet.get('url', 'ä¸æ˜URL')}ï¼‰: {e}")
    return tweet


def extract_from_search(driver, keywords, max_tweets, name_bio_keywords=None):
    tweets = []
    seen_urls = set()
    seen_users = set()

    for keyword in keywords:
        print(f"ğŸ” è©±é¡Œã®ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ä¸­: {keyword}")
        search_url = f"https://twitter.com/search?q={keyword}&src=typed_query&f=top"
        driver.get(search_url)
        time.sleep(3)

        scroll_count = 0
        max_scrolls = 10
        pause_counter = 0
        pause_threshold = 3
        last_article_count = 0

        while len(tweets) < max_tweets and scroll_count < max_scrolls:
            articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            article_count = len(articles)
            print(f"ğŸ“„ è¡¨ç¤ºä¸­ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {article_count}")
            for article in articles:
                try:
                    # ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åå–å¾—
                    time_el = article.find_element(By.XPATH, ".//time")
                    tweet_href = time_el.find_element(By.XPATH, "..").get_attribute(
                        "href"
                    )
                    tweet_url = (
                        tweet_href
                        if tweet_href.startswith("http")
                        else f"https://x.com{tweet_href}"
                    )
                    if tweet_url in seen_urls:
                        continue
                    seen_urls.add(tweet_url)

                    name_block = article.find_element(
                        By.XPATH, ".//div[@data-testid='User-Name']"
                    )
                    spans = name_block.find_elements(By.XPATH, ".//span")
                    display_name = ""
                    username = ""
                    for s in spans:
                        text = s.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not display_name:
                            display_name = text

                    if not username or username in seen_users:
                        continue
                    seen_users.add(username)

                    # bioãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸å…ˆã«ã‚¢ã‚¯ã‚»ã‚¹
                    if name_bio_keywords:
                        driver.execute_script("window.open('');")
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.get(f"https://twitter.com/{username}")
                        time.sleep(2)
                        try:
                            bio_text = (
                                WebDriverWait(driver, 5)
                                .until(
                                    EC.presence_of_element_located(
                                        (
                                            By.XPATH,
                                            "//div[@data-testid='UserDescription']",
                                        )
                                    )
                                )
                                .text
                            )
                        except:
                            bio_text = ""
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])

                        if not any(
                            k in display_name for k in name_bio_keywords
                        ) and not any(k in bio_text for k in name_bio_keywords):
                            print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿éä¸€è‡´ â†’ ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                            continue

                    # âœ… æ¡ä»¶ã‚’é€šéã—ãŸå ´åˆã®ã¿æŠ•ç¨¿è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦æŠ½å‡º
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(tweet_url)
                    WebDriverWait(driver, 10).until(EC.url_contains("/status/"))

                    try:
                        full_text_el = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located(
                                (By.XPATH, "//div[@data-testid='tweetText']")
                            )
                        )
                        text = full_text_el.text.strip()
                    except Exception as e:
                        print(f"âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")
                        text = ""

                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆå®‰å®šåŒ– + ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« + ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–ï¼‰
                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸å†…ã€ã‚¨ãƒ©ãƒ¼å›é¿ãƒ»å¤šæ®µæ§‹é€ ã«å¯¾å¿œï¼‰
                    date = ""
                    for attempt in range(5):
                        try:
                            driver.execute_script("window.scrollTo(0, 0);")
                            WebDriverWait(driver, 3).until(
                                EC.presence_of_element_located((By.XPATH, "//article"))
                            )
                            time_el = driver.find_element(By.XPATH, "//article//a/time")
                            if time_el:
                                date = time_el.get_attribute("datetime")
                                break
                        except Exception as e:
                            print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—è©¦è¡Œ {attempt+1}/5 å¤±æ•—: {e}")
                            time.sleep(1)

                    if not date:
                        print("âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—ã«å¤±æ•— â†’ ç©ºæ–‡å­—ã§ç¶™ç¶š")

                    # è‡ªãƒªãƒ—ãƒ©ã‚¤å–å¾—ï¼ˆçœç•¥å¯ï¼‰
                    replies = extract_self_replies(driver, username)
                    if replies:
                        reply_texts = [
                            r["text"]
                            for r in replies
                            if "text" in r and r["text"] not in text
                        ]
                        if reply_texts:
                            text += "\n\n" + "\n\n".join(reply_texts)

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])
                    if already_registered(tweet_id):
                        print(f"ğŸš« ç™»éŒ²æ¸ˆ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                        continue

                    tweets.append(
                        {
                            "url": tweet_url,
                            "text": text,
                            "date": date,
                            "id": tweet_id,
                            "username": username,
                            "display_name": display_name,
                        }
                    )

                    print(f"âœ… åé›†: {tweet_url} @{username}")
                    if len(tweets) >= max_tweets:
                        break

                except Exception as e:
                    print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # èª­ã¿è¾¼ã¿åˆ¤å®š
            if article_count == last_article_count:
                pause_counter += 1
                print("ğŸ§Š ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«æ–°ã—ã„æŠ•ç¨¿ãªã—")
                if pause_counter >= pause_threshold:
                    print("ğŸ›‘ æŠ•ç¨¿ãŒå¢—ãˆãªã„ãŸã‚ä¸­æ–­")
                    break
            else:
                pause_counter = 0

            last_article_count = article_count
            scroll_count += 1

    return tweets


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument(
        "--account", default="accounts.json", help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰"
    )
    args = parser.parse_args()

    config = load_config(args.config)
    account = load_config(args.account)

    global NOTION_TOKEN, DATABASE_ID, notion
    global TWITTER_EMAIL, TWITTER_USERNAME, TWITTER_PASSWORD
    global EXTRACT_TARGET, MAX_TWEETS

    NOTION_TOKEN = config["notion_token"]
    DATABASE_ID = config["database_id"]
    EXTRACT_TARGET = config["extract_target"]
    MAX_TWEETS = config["max_tweets"]

    TWITTER_EMAIL = account["email"]
    TWITTER_USERNAME = account["username"]
    TWITTER_PASSWORD = account["password"]

    notion = Client(auth=NOTION_TOKEN)
    driver = setup_driver()

    login(driver, EXTRACT_TARGET if config["mode"] == "target_only" else None)

    tweets_for_notion_upload = []

    if config["mode"] == "target_only":
        print(
            f"ğŸ¯ mode: target_only â†’ extract_target = {EXTRACT_TARGET} ã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™"
        )
        URL_BUFFER_FACTOR = 3
        tweet_url_dicts = extract_tweets(
            driver, EXTRACT_TARGET, MAX_TWEETS * URL_BUFFER_FACTOR
        )
        # extract_tweets ã¯ {"url": url, "id": id} ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_and_merge_tweets(
            driver, tweet_url_dicts, MAX_TWEETS
        )

    elif config["mode"] == "search_filtered":
        print(
            "ğŸ” mode: search_filtered â†’ æ¤œç´¢ + name/bio + tweetãƒ•ã‚£ãƒ«ã‚¿ã‚’ã‹ã‘ã¦æŠ•ç¨¿ã‚’åé›†ã—ã¾ã™"
        )
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        users = search_accounts(driver, config["filter_keywords_name_bio"])
        collected_tweets_count = 0
        for user_info in users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            if is_recruit_account(
                user_info["name"], user_info["bio"], config
            ):  # bioã¯search_accountså†…ã§å–å¾—ãƒ»è¨­å®šãŒå¿…è¦
                # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                user_tweet_urls = extract_tweets(
                    driver, user_info["username"], MAX_TWEETS - collected_tweets_count
                )
                # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                processed_user_tweets = extract_and_merge_tweets(
                    driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
                )

                # ã•ã‚‰ã«æŠ•ç¨¿å†…å®¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                for tweet_data in processed_user_tweets:
                    if is_recruit_post(tweet_data["text"], config):
                        tweets_for_notion_upload.append(tweet_data)
                        collected_tweets_count += 1
                        if collected_tweets_count >= MAX_TWEETS:
                            break

    elif config["mode"] == "search_all":
        print("ğŸŒ mode: search_all â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ â†’ bioãƒ•ã‚£ãƒ«ã‚¿ â†’ å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿å–å¾—")
        collected_tweets_count = 0

        # filter_keywords_name_bio ã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™æƒ³å®šã ãŒã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«æ¤œç´¢ã™ã‚‹å½¢ã«å¤‰æ›´

        all_potential_users = []
        for keyword in config.get(
            "filter_keywords_name_bio", []
        ):  # name_bio ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢
            print(f"ğŸ‘¤ '{keyword}' ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ä¸­...")
            # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            # search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã‹ã€ãƒ«ãƒ¼ãƒ—å†…ã§å‘¼ã³å‡ºã™
            # ã“ã“ã§ã¯ search_accounts ãŒå˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´ã—ãŸã¨ä»®å®š
            # ã¾ãŸã¯ã€search_accounts ã‚’ filter_keywords_name_bio å…¨ä½“ã§å®Ÿè¡Œã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’å¾—ã‚‹

            # ä»®: search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒãƒƒãƒã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¿”ã™
            # users_from_keyword_search = search_accounts(driver, [keyword]) # search_accountsã®ä»•æ§˜ã«åˆã‚ã›ã‚‹
            # all_potential_users.extend(users_from_keyword_search)

            # ç¾çŠ¶ã® search_accounts ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã®ã§ã€ãã‚Œã§è‰¯ã„
            pass  # search_accounts ã¯å¾Œã§ã¾ã¨ã‚ã¦å‘¼ã³å‡ºã™ã‹ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‡¦ç†

        # search_accounts ã¯ filter_keywords_name_bio ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã¨ä»®å®š
        # (ã¾ãŸã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‘¼ã³å‡ºã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹)
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {config.get('filter_keywords_name_bio')}")
        candidate_users = search_accounts(
            driver, config.get("filter_keywords_name_bio", [])
        )

        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—ã—ã€bioã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_users = []
        for user_data in candidate_users:
            # search_accountså†…ã§bioã‚’å–å¾—ãƒ»è¨­å®šã™ã‚‹ã‹ã€ã“ã“ã§ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦bioå–å¾—
            # ã“ã“ã§ã¯ search_accounts ãŒbioã‚‚å–å¾—ã—ã¦è¿”ã™ã¨ä»®å®šï¼ˆã¾ãŸã¯åˆ¥é€”é–¢æ•°å‘¼ã³å‡ºã—ï¼‰
            # ä»®ã«ã“ã“ã§bioã‚’å–å¾—ã™ã‚‹ãªã‚‰ï¼š
            # driver.get(f"https://twitter.com/{user_data['username']}")
            # time.sleep(2)
            # try:
            #     bio_el = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, "//div[@data-testid='UserDescription']")))
            #     user_data['bio'] = bio_el.text if bio_el else ""
            # except:
            #     user_data['bio'] = ""

            # is_recruit_account ã¯ name ã¨ bio ã§åˆ¤å®š
            if is_recruit_account(
                user_data.get("name", ""), user_data.get("bio", ""), config
            ):
                filtered_users.append(user_data)

        print(f"ğŸ‘¤ bioãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(filtered_users)}")

        for user_info in filtered_users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            print(f"ğŸ¦ @{user_info['username']} ã®æŠ•ç¨¿ã‚’åé›†é–‹å§‹")
            # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            user_tweet_urls = extract_tweets(
                driver, user_info["username"], MAX_TWEETS - collected_tweets_count
            )
            # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            processed_user_tweets = extract_and_merge_tweets(
                driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
            )

            # search_all ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€æŠ•ç¨¿å†…å®¹ã®ãƒ•ã‚£ãƒ«ã‚¿ã¯é€šå¸¸ã‹ã‘ãªã„ãŒã€ã‚‚ã—å¿…è¦ãªã‚‰ã“ã“ã§ is_recruit_post ã‚’ä½¿ã†
            # ä»Šå›ã¯bioãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨æŠ•ç¨¿(MAX_TWEETSã¾ã§)ã‚’å–å¾—ã™ã‚‹ã¨è§£é‡ˆ
            tweets_for_notion_upload.extend(processed_user_tweets)
            collected_tweets_count += len(processed_user_tweets)

    elif config["mode"] == "keyword_trend":
        print("ğŸ”¥ mode: keyword_trend â†’ æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©±é¡ŒæŠ•ç¨¿ã‚’åé›†ã—ã¾ã™")
        # extract_from_search ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_from_search(
            driver,
            config["filter_keywords_tweet"],  # æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            MAX_TWEETS,
            config.get(
                "filter_keywords_name_bio"
            ),  # å–å¾—ã—ãŸæŠ•ç¨¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã•ã‚‰ã«çµã‚‹å ´åˆ
        )

    else:
        raise ValueError(f"âŒ æœªçŸ¥ã®modeæŒ‡å®šã§ã™: {config['mode']}")

    print(f"\nğŸ“Š Notionç™»éŒ²å¯¾è±¡ã®åˆè¨ˆãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(tweets_for_notion_upload)} ä»¶")

    # æŠ•ç¨¿IDæ˜‡é †ã§ä¸¦ã¹æ›¿ãˆã¦ã‹ã‚‰ç™»éŒ²ï¼ˆé †ç•ªä¿è¨¼ï¼‰
    # id ãŒæ•°å€¤ã§ãªã„å ´åˆã‚„å­˜åœ¨ã—ãªã„å ´åˆã‚’è€ƒæ…®
    tweets_for_notion_upload.sort(
        key=lambda x: (
            int(x["id"]) if x.get("id") and x["id"].isdigit() else float("inf")
        )
    )

    for i, tweet_data in enumerate(tweets_for_notion_upload, 1):
        print(f"\nğŸŒ€ {i}/{len(tweets_for_notion_upload)} ä»¶ç›® Notionç™»éŒ²å‡¦ç†ä¸­...")

        # upload_to_notion ã«æ¸¡ã™å‰ã«ã€ä¸è¦ãªã‚­ãƒ¼ã‚„WebElementãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
        tweet_data_for_upload = tweet_data.copy()
        tweet_data_for_upload.pop(
            "article_element", None
        )  # extract_thread_from_detail_page ãŒæ®‹ã™å¯èƒ½æ€§
        tweet_data_for_upload.pop("article", None)  # å¤ã„å½¢å¼ã®ã‚­ãƒ¼ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆ

        # print(json.dumps(tweet_data_for_upload, ensure_ascii=False, indent=2)) # ãƒ‡ãƒãƒƒã‚°ç”¨

        # ãƒªãƒ—ãƒ©ã‚¤ãƒãƒ¼ã‚¸ã¯ extract_and_merge_tweets ã§å®Ÿæ–½æ¸ˆã¿ã®ãŸã‚ã€ã“ã“ã§ã¯å‘¼ã³å‡ºã•ãªã„
        upload_to_notion(tweet_data_for_upload)

    driver.quit()
    print("âœ… å…¨æŠ•ç¨¿ã®å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
