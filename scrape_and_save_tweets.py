import os
import re
import time
import json
import argparse
import traceback
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from notion_client import Client
from datetime import datetime
import shutil

# âœ… åºƒå‘Šé™¤å¤–ã€RT/å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ã€æŠ•ç¨¿IDè£œå®Œä»˜ã
AD_KEYWORDS = [
    "r10.to",
    "ãµã‚‹ã•ã¨ç´ç¨",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³",
    "ãŠé‡‘å€Ÿã‚Šã‚‰ã‚Œã‚‹",
    "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¬ãƒ¬ãƒ¼ã‚¸",
    "UNEXT",
    "ã‚¨ã‚³ã‚ªã‚¯",
    "#PR",
    "æ¥½å¤©",
    "Amazon",
    "A8",
    "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    "å‰¯æ¥­",
    "bit.ly",
    "shp.ee",
    "t.co/",
]


def normalize_text(text):
    return text.strip()


def login(driver, target=None):
    if os.path.exists("twitter_cookies.json"):
        print("âœ… Cookieã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º â†’ ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—")
        print("ğŸŒ https://twitter.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­â€¦")
        driver.get("https://twitter.com/")
        driver.delete_all_cookies()
        with open("twitter_cookies.json", "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.get(f"https://twitter.com/{target or TWITTER_USERNAME}")
        return

    print("ğŸ” åˆå›ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹")
    driver.get("https://twitter.com/i/flow/login")
    email_input = WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.NAME, "text"))
    )
    email_input.send_keys(TWITTER_EMAIL)
    email_input.send_keys(Keys.ENTER)
    time.sleep(2)

    try:
        username_input = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "text"))
        )
        username_input.send_keys(TWITTER_USERNAME)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)
    except Exception:
        print("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åå…¥åŠ›ã‚¹ã‚­ãƒƒãƒ—")

    password_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.NAME, "password"))
    )
    password_input.send_keys(TWITTER_PASSWORD)
    password_input.send_keys(Keys.ENTER)
    time.sleep(6)

    cookies = driver.get_cookies()
    with open("twitter_cookies.json", "w") as f:
        json.dump(cookies, f)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ â†’ æŠ•ç¨¿è€…ãƒšãƒ¼ã‚¸ã«é·ç§»")
    driver.get(f"https://twitter.com/{EXTRACT_TARGET}")


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  â† ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=options)


def extract_tweet_id(article):
    href_els = article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
    for el in href_els:
        h = el.get_attribute("href")
        m = re.search(r"/status/(\d+)", h or "")
        if m:
            return m.group(1)
    return None


def extract_self_replies(driver, username):
    replies = []
    # cellInnerDivã”ã¨ã«ã€Œã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ã€span/h2ãŒå‡ºãŸã‚‰break
    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” extract_self_replies: ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–")
            break

        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")

        def is_quote_reply(article):
            # ã€Œå¼•ç”¨ã€ã‚„ã€ŒQuoteã€ãªã©ã®æ–‡è¨€ã‚„ã€å¼•ç”¨æ§‹é€ ã‚’æŒã¤è¦ç´ ã‚’åˆ¤å®š
            quote_els = article.find_elements(
                By.XPATH,
                ".//*[contains(text(), 'å¼•ç”¨')] | .//*[contains(text(), 'Quote')]",
            )
            # è¿½åŠ : å¼•ç”¨æ§‹é€ ã®divã‚„aria-labelã‚‚åˆ¤å®š
            quote_struct = article.find_elements(
                By.XPATH, ".//div[contains(@aria-label, 'å¼•ç”¨')]"
            )
            return bool(quote_els or quote_struct)

        for article in articles:
            try:
                handle_el = article.find_element(
                    By.XPATH,
                    ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                )
                handle = handle_el.text.strip()
                if handle.replace("@", "") != username:
                    continue

                # å¼•ç”¨RTå½¢å¼ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                if is_quote_reply(article):
                    print("âš ï¸ extract_self_replies: å¼•ç”¨RTå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                reply_text = text_el.text.strip() if text_el and text_el.text else ""

                tweet_id = extract_tweet_id(article)

                if not tweet_id:
                    print("âš ï¸ extract_self_replies: tweet_idãŒå–å¾—ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                if reply_text:
                    replies.append({"id": tweet_id, "text": reply_text})
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue
    return replies


def is_ad_post(text):
    lowered = text.lower()
    return any(k.lower() in lowered for k in AD_KEYWORDS)


def extract_thread_from_detail_page(driver, tweet_url):
    print(f"\n\U0001f575 æŠ•ç¨¿ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {tweet_url}")
    driver.get(tweet_url)
    time.sleep(3)

    if (
        "Something went wrong" in driver.page_source
        or "ã“ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ã¾ã›ã‚“" in driver.page_source
    ):
        print(f"âŒ æŠ•ç¨¿ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {tweet_url}")
        return []

    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿è¨˜äº‹ã®å–å¾—ã«å¤±æ•—: {e}")
        return []

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    tweet_blocks = []
    current_id = re.sub(r"\D", "", tweet_url.split("/")[-1])

    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")
    print(f"cellInnerDivæ•°: {len(cell_divs)}")
    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®æŠ•ç¨¿ã‚’é™¤å¤–")
            break

        # â˜… breakå‰ã®cellã®articlesã‚’ã“ã“ã§å‡¦ç†
        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")
        for article in articles:
            try:
                href_el = article.find_element(
                    By.XPATH, ".//a[contains(@href, '/status/')]"
                )
                href = href_el.get_attribute("href")
                match = re.search(r"/status/(\d{10,})", href)
                tweet_id = match.group(1) if match else None

                if not tweet_id:
                    print(f"ğŸ›‘ tweet_idæŠ½å‡ºå¤±æ•— â†’ é™¤å¤–: href={href}")
                    continue

                try:
                    tweet_div = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    parts = []
                    for elem in tweet_div.find_elements(By.XPATH, ".//*"):
                        if elem.tag_name == "img" and elem.get_attribute("alt"):
                            parts.append(elem.get_attribute("alt"))
                        elif elem.text:
                            parts.append(elem.text)
                    text = "".join(parts).strip()
                except:
                    text = ""

                images = article.find_elements(
                    By.XPATH, ".//img[contains(@src, 'twimg.com/media')]"
                )
                videos = article.find_elements(By.XPATH, ".//video")
                has_media = bool(images or videos)

                if is_reply_structure(
                    article, tweet_id=tweet_id, text=text, has_media=has_media
                ):
                    continue

                time_els = article.find_elements(By.XPATH, ".//time")
                date_str = time_els[0].get_attribute("datetime") if time_els else None
                if not date_str:
                    print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚ãªã— â†’ date=None ã«è¨­å®š: ID={tweet_id}")

                username = ""
                try:
                    username_el = article.find_element(
                        By.XPATH,
                        ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                    )
                    username = username_el.text.replace("@", "").strip()
                except:
                    pass

                tweet_blocks.append(
                    {
                        "article": article,
                        "text": text,
                        "date": date_str,
                        "id": tweet_id,
                        "username": username,
                    }
                )

            except Exception as e:
                print(f"âš ï¸ articleè§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {str(e)}")
                continue

    print(f"\nğŸ” ã‚¢ã‚¯ã‚»ã‚¹å…ƒURL: {tweet_url}")
    print(f"ğŸ”¢ ã‚¢ã‚¯ã‚»ã‚¹å…ƒID: {current_id}")

    if not tweet_blocks:
        print("âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
        return []

    tweet_blocks.sort(key=lambda x: int(x["id"]))
    for i, block in enumerate(tweet_blocks):
        print(
            f"  [{i+1}] DOMå–å¾—ID: {block['id']} | textå…ˆé ­: {block['text'].replace(chr(10), ' ')[:15]}"
        )

    valid_blocks = [
        b
        for b in tweet_blocks
        if b.get("username") == EXTRACT_TARGET and not is_ad_post(b["text"])
    ]
    if not valid_blocks:
        print("âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿è€…ä¸€è‡´+éåºƒå‘Šã®æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ â†’ é™¤å¤–")
        return []

    parent_id = sorted(valid_blocks, key=lambda x: int(x["id"]))[0]["id"]
    if current_id != parent_id:
        print(
            f"ğŸ” æŠ•ç¨¿ID {current_id} ã¯è¦ªID {parent_id} ã§ã¯ãªã„ãŸã‚é™¤å¤–ï¼ˆæŠ•ç¨¿è€…ä¸€è‡´+éåºƒå‘Šã§åˆ¤å®šï¼‰"
        )
        return []

    block = next(b for b in tweet_blocks if b["id"] == current_id)

    # â˜…ã“ã“ã§è¦ªæŠ•ç¨¿ã‹ã‚‰æ•°å€¤ã‚’å–å¾—
    impressions, retweets, likes, bookmarks, replies = extract_metrics(block["article"])

    image_urls = [
        img.get_attribute("src")
        for img in block["article"].find_elements(
            By.XPATH, ".//img[contains(@src, 'twimg.com/media')]"
        )
        if img.get_attribute("src")
    ]
    video_urls = [
        v.get_attribute("src")
        for v in block["article"].find_elements(By.XPATH, ".//video")
        if v.get_attribute("src")
    ]

    return [
        {
            "url": tweet_url,
            "id": current_id,
            "text": block["text"],
            "date": block["date"],
            "images": image_urls,
            "videos": video_urls,
            "username": block["username"],
            "impressions": impressions,
            "retweets": retweets,
            "likes": likes,
            "bookmarks": bookmarks,
            "replies": replies,
        }
    ]


def extract_and_merge_tweets(driver, tweet_urls, max_tweets):
    tweets = []
    seen_ids = set()
    registered_count = 0  # âœ… å®Ÿéš›ã«ç™»éŒ²å¯¾è±¡ã¨ã—ã¦æˆåŠŸã—ãŸä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

    for i, meta in enumerate(tweet_urls):
        if registered_count >= max_tweets:
            print("ğŸ¯ ç™»éŒ²ä»¶æ•°ãŒ MAX_TWEETS ã«é”ã—ãŸãŸã‚çµ‚äº†")
            break

        tweet_url = meta["url"] if isinstance(meta, dict) else meta
        print(f"\nğŸ§ª å‡¦ç†ä¸­: {tweet_url}")

        try:
            thread = extract_thread_from_detail_page(driver, tweet_url)
            if not thread:
                continue

            post = thread[0]  # âœ… å¸¸ã«1æŠ•ç¨¿ã®ã¿å¯¾è±¡ã¨ã™ã‚‹
            tweet_id = post.get("id")

            if not tweet_id or tweet_id in seen_ids:
                print(f"âš ï¸ é‡è¤‡ã¾ãŸã¯ç„¡åŠ¹ID â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_id}")
                continue
            if already_registered(tweet_id):
                print(f"ğŸš« ç™»éŒ²æ¸ˆã¿ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_id}")
                continue

            tweets.append(post)
            seen_ids.add(tweet_id)
            registered_count += 1
            print(
                f"âœ… ç™»éŒ²å¯¾è±¡ã¨ã—ã¦è¿½åŠ : {tweet_id}ï¼ˆç¾åœ¨ {registered_count}/{max_tweets} ä»¶ï¼‰"
            )

        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    print(f"\nğŸ“ˆ å®Œäº†: {len(tweets)} ä»¶ã®æŠ•ç¨¿ã‚’æŠ½å‡ºï¼ˆç™»éŒ²å¯¾è±¡ã¨ã—ã¦ï¼‰")
    return tweets


def extract_metrics(article):
    """
    ã„ã„ã­æ•°ãƒ»ãƒªãƒã‚¹ãƒˆæ•°ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ»ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒ»ãƒªãƒ—ãƒ©ã‚¤æ•°ã‚’æŠ½å‡º
    å–å¾—ã§ããªã„ã‚‚ã®ã¯0ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿Noneï¼‰ã§è¿”ã™
    """
    impressions = retweets = likes = bookmarks = replies = None
    try:
        divs = article.find_elements(
            By.XPATH, ".//div[contains(@aria-label, 'ä»¶ã®è¡¨ç¤º')]"
        )
        for div in divs:
            label = div.get_attribute("aria-label")
            print(f"ğŸŸ¦ aria-labelå†…å®¹: {label}")

            # 1. è¿”ä¿¡ã‚ã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³
            m_reply = re.search(
                r"(\d[\d,\.ä¸‡]*) ä»¶ã®è¿”ä¿¡ã€(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒªãƒã‚¹ãƒˆã€(\d[\d,\.ä¸‡]*) ä»¶ã®ã„ã„ã­ã€(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€(\d[\d,\.ä¸‡]*) ä»¶ã®è¡¨ç¤º",
                label or "",
            )
            if m_reply:
                replies = m_reply.group(1)
                retweets = m_reply.group(2)
                likes = m_reply.group(3)
                bookmarks = m_reply.group(4)
                impressions = m_reply.group(5)
                print(
                    f"ğŸŸ© ãƒãƒƒãƒ: è¿”ä¿¡={replies}, RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, è¡¨ç¤º={impressions}"
                )
                break

            # 2. ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã‚ã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¿”ä¿¡ãªã—ï¼‰
            m = re.search(
                r"(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒªãƒã‚¹ãƒˆã€(\d[\d,\.ä¸‡]*) ä»¶ã®ã„ã„ã­ã€(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€(\d[\d,\.ä¸‡]*) ä»¶ã®è¡¨ç¤º",
                label or "",
            )
            if m:
                retweets = m.group(1)
                likes = m.group(2)
                bookmarks = m.group(3)
                impressions = m.group(4)
                print(
                    f"ğŸŸ© ãƒãƒƒãƒ: RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, è¡¨ç¤º={impressions}"
                )
                break

            # 3. ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ãªã—ãƒ‘ã‚¿ãƒ¼ãƒ³
            m2 = re.search(
                r"(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒªãƒã‚¹ãƒˆã€(\d[\d,\.ä¸‡]*) ä»¶ã®ã„ã„ã­ã€(\d[\d,\.ä¸‡]*) ä»¶ã®è¡¨ç¤º",
                label or "",
            )
            if m2:
                retweets = m2.group(1)
                likes = m2.group(2)
                impressions = m2.group(3)
                print(f"ğŸŸ© ãƒãƒƒãƒ: RT={retweets}, ã„ã„ã­={likes}, è¡¨ç¤º={impressions}")
                break

            # 4. ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³
            m3 = re.search(r"([\d,\.ä¸‡]+) ä»¶ã®è¡¨ç¤º", label or "")
            if m3:
                impressions = m3.group(1)
                print(f"ğŸŸ¦ ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿: è¡¨ç¤º={impressions}")
                # likes/retweets/bookmarks/repliesã¯0æ‰±ã„
                retweets = 0
                likes = 0
                bookmarks = 0
                replies = 0
                break

        # 5. ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ã‚’å–å¾—ï¼ˆaria-labelä¾‹: "1 ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ã€‚ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯"ï¼‰
        if bookmarks is None:
            try:
                bm_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='bookmark']"
                )
                for btn in bm_btns:
                    bm_label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*) ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", bm_label or "")
                    if m:
                        bookmarks = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰BMå–å¾—: {bookmarks}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if replies is None or replies == 0:
            try:
                # replyãƒœã‚¿ãƒ³ã®aria-labelä¾‹: "3 ä»¶ã®è¿”ä¿¡"
                reply_btns = article.find_elements(
                    By.XPATH, ".//div[@role='group']//button"
                )
                for btn in reply_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*) ä»¶ã®è¿”ä¿¡", label or "")
                    if m:
                        replies = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤æ•°å–å¾—: {replies}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        def parse_num(s):
            if not s:
                return 0
            s = s.replace(",", "")
            if "ä¸‡" in s:
                return int(float(s.replace("ä¸‡", "")) * 10000)
            try:
                return int(s)
            except:
                return 0

        impressions = parse_num(impressions) if impressions is not None else None
        retweets = parse_num(retweets)
        likes = parse_num(likes)
        bookmarks = parse_num(bookmarks)
        replies = parse_num(replies)

    except Exception as e:
        print(f"âš ï¸ extract_metricsã‚¨ãƒ©ãƒ¼: {e}")
    return impressions, retweets, likes, bookmarks, replies


def is_reply_structure(article, tweet_id=None, text="", has_media=False):
    try:
        # IDã‚’è¡¨ç¤ºç”¨ã«è¨­å®š
        id_display = f"ï¼ˆID={tweet_id}ï¼‰" if tweet_id else ""

        # 1. æ˜ç¤ºçš„ãª reply ã‚³ãƒ³ãƒ†ãƒŠæ§‹é€ 
        reply_aria = article.find_elements(
            By.XPATH, ".//div[contains(@aria-labelledby, 'rxyo3tk')]"
        )
        if reply_aria:
            print(
                f"ğŸ›‘ is_reply_structure: aria-labelledby ã« 'rxyo3tk' æ§‹é€ ã‚ã‚Š â†’ ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š {id_display}"
            )
            return True

        # 2. ã€Œè¿”ä¿¡å…ˆã€ã®æ–‡è¨€æ¤œå‡º
        reply_text = article.find_elements(By.XPATH, ".//*[contains(text(), 'è¿”ä¿¡å…ˆ')]")
        if reply_text:
            print(
                f"ğŸ›‘ is_reply_structure: 'è¿”ä¿¡å…ˆ' ã®æ–‡è¨€ã‚’å«ã‚€ â†’ ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š {id_display}"
            )
            return True

        # 3. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã®æ•°ãŒå°‘ãªã„ â†’ ãƒªãƒ—ãƒ©ã‚¤ã‚„å¼•ç”¨
        buttons = article.find_elements(By.XPATH, ".//div[@role='group']//button")
        if len(buttons) < 4:
            print(
                f"ğŸ›‘ is_reply_structure: ãƒœã‚¿ãƒ³æ•° {len(buttons)} å€‹ â†’ ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š {id_display}"
            )
            return True

        # 4. å¼•ç”¨ã®å ´åˆï¼ˆãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ã & 50æ–‡å­—ä»¥ä¸Šãªã‚‰è¨±å¯ï¼‰
        quote_text = article.find_elements(By.XPATH, ".//*[contains(text(), 'å¼•ç”¨')]")
        if quote_text:
            text_length = len(text.strip()) if text else 0
            if has_media and text_length >= 50:
                print(
                    f"âœ… is_reply_structure: å¼•ç”¨ã‚ã‚Šï¼ˆç”»åƒ+50æ–‡å­—ä»¥ä¸Šï¼‰â†’ è¨±å¯ {id_display}"
                )
                return False
            print(
                f"ğŸ›‘ is_reply_structure: å¼•ç”¨ã‚ã‚Šï¼ˆæ¡ä»¶æœªæº€ï¼‰â†’ é™¤å¤– {id_display} | é•·ã•={text_length} | ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Š={has_media}"
            )
            return True

        # 5. ä¸Šè¨˜ã«è©²å½“ã—ãªã„ â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤æ–­
        print(f"âœ… is_reply_structure: æ§‹é€ ä¸Šå•é¡Œãªã— â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤å®š {id_display}")
        return False

    except Exception as e:
        print(f"âš ï¸ is_reply_structure: åˆ¤å®šã‚¨ãƒ©ãƒ¼ {id_display} â†’ {e}")
        return False


def has_media_in_html(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    # ç”»åƒåˆ¤å®š
    if soup.find("img", {"src": lambda x: x and "twimg.com/media" in x}):
        return True
    # å‹•ç”»åˆ¤å®š
    if soup.find("div", {"data-testid": "video-player-mini-ui-"}):
        return True
    if soup.find("button", {"aria-label": "å‹•ç”»ã‚’å†ç”Ÿ"}):
        return True
    if soup.find("video"):
        return True
    return False


def extract_tweets(driver, extract_target, max_tweets):
    print(f"\nâœ¨ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: https://twitter.com/{extract_target}")
    driver.get(f"https://twitter.com/{extract_target}")
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, "//article"))
    )

    tweet_urls = []
    seen_urls = set()
    scroll_count = 0
    scroll_position = 0
    max_scrolls = 50

    # âœ… æ–°è¦æŠ•ç¨¿ã®å¤‰åŒ–ã‚’ç›£è¦–
    pause_counter = 0
    pause_threshold = 3
    last_seen_count = 0

    while scroll_count < max_scrolls and len(tweet_urls) < max_tweets:
        print(f"\nğŸ” ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {scroll_count + 1} å›ç›®")
        articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
        print(f"ğŸ“„ ç¾åœ¨ã®articleæ•°: {len(articles)}")

        for i, article in enumerate(articles):
            try:
                print(f"ğŸ” [{i+1}/{len(articles)}] æŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ä¸­...")

                # hrefå–å¾—ã‚’å®‰å…¨ã«
                href_els = article.find_elements(
                    By.XPATH, ".//a[contains(@href, '/status/')]"
                )
                if not href_els:
                    print("âš ï¸ hrefãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue
                href = href_els[0].get_attribute("href")
                tweet_url = href if href.startswith("http") else f"https://x.com{href}"
                tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])

                if tweet_url in seen_urls:
                    print(f"ğŸŒ€ æ—¢å‡ºURL(ã‚¹ã‚­ãƒƒãƒ—): {tweet_url}")
                    continue
                seen_urls.add(tweet_url)

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                text = normalize_text(text_el.text) if text_el else ""

                images = [
                    img.get_attribute("src")
                    for img in article.find_elements(
                        By.XPATH, ".//img[contains(@src, 'twimg.com/media')]"
                    )
                ]
                videos = [
                    v.get_attribute("src")
                    for v in article.find_elements(By.XPATH, ".//video")
                    if v.get_attribute("src")
                ]
                has_media = bool(images or videos)

                # ç”»åƒã‚‚å‹•ç”»ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯HTMLã‹ã‚‰è£œåŠ©åˆ¤å®š
                if not has_media:
                    article_html = article.get_attribute("outerHTML")
                    if has_media_in_html(article_html):
                        has_media = True

                if is_reply_structure(
                    article, tweet_id=tweet_id, text=text, has_media=has_media
                ):
                    print(f"â†ªï¸ ãƒªãƒ—ãƒ©ã‚¤ã¾ãŸã¯å¼•ç”¨æ§‹é€ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                    continue

                if is_ad_post(text):
                    print(f"ğŸš« åºƒå‘Šã¨åˆ¤å®šâ†’ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                    continue

                if already_registered(tweet_id):
                    print(f"âŒ ç™»éŒ²æ¸ˆâ†’ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                    continue

                tweet_urls.append({"url": tweet_url, "id": tweet_id})

                print(f"âœ… æŠ½å‡º: {tweet_url}")
                if len(tweet_urls) >= max_tweets:
                    break

            except Exception as e:
                print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue

        for _ in range(3):
            scroll_position += 1500
            driver.execute_script(f"window.scrollTo(0, {scroll_position});")
            time.sleep(1.5)

        # âœ… æ–°è¦æŠ•ç¨¿ã®å¤‰åŒ–ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯
        if len(seen_urls) == last_seen_count:
            pause_counter += 1
            print(f"ğŸ§Š æ–°è¦æŠ•ç¨¿ãªã— â†’ pause_counter={pause_counter}")
            if pause_counter >= pause_threshold:
                print("ğŸ›‘ æ–°ã—ã„æŠ•ç¨¿ãŒæ¤œå‡ºã•ã‚Œãªã„ãŸã‚ä¸­æ–­")
                break
        else:
            pause_counter = 0
            last_seen_count = len(seen_urls)

        scroll_count += 1

    print(f"\nğŸ“ˆ å–å¾—å®Œäº† â†’ åˆè¨ˆæŠ•ç¨¿æ•°: {len(tweet_urls)} ä»¶")
    return tweet_urls


def save_media(media_urls, folder):
    os.makedirs(folder, exist_ok=True)
    saved_files = []
    for i, url in enumerate(media_urls):
        try:
            response = requests.get(url, stream=True)
            ext = ".mp4" if "video" in url else ".jpg"
            filename = f"media_{i}{ext}"
            filepath = os.path.join(folder, filename)
            with open(filepath, "wb") as f:
                shutil.copyfileobj(response.raw, f)
            print(f"ğŸ’¾ ãƒ¡ãƒ‡ã‚£ã‚¢ä¿å­˜æˆåŠŸ: {filepath}")
            saved_files.append(filepath)
        except Exception as e:
            print("âŒ ãƒ¡ãƒ‡ã‚£ã‚¢ä¿å­˜å¤±æ•—:", e)
    return saved_files


def already_registered(tweet_id):
    if not tweet_id or not tweet_id.isdigit():
        return False
    query = {"filter": {"property": "æŠ•ç¨¿ID", "rich_text": {"equals": tweet_id}}}
    try:
        result = notion.databases.query(database_id=DATABASE_ID, **query)
        return len(result.get("results", [])) > 0
    except Exception as e:
        print(f"âš ï¸ Notionã‚¯ã‚¨ãƒªå¤±æ•—: {e}")
        return False


def upload_to_notion(tweet):
    print(f"ğŸ“¤ Notionç™»éŒ²å‡¦ç†é–‹å§‹: {tweet['id']}")
    if already_registered(tweet["id"]):
        print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ—æ¸ˆ: {tweet['id']}")
        return
    props = {
        "æŠ•ç¨¿ID": {
            "rich_text": [{"type": "text", "text": {"content": str(tweet["id"])}}]
        },
        "æœ¬æ–‡": {
            "rich_text": [
                {
                    "type": "text",
                    "text": {
                        "content": tweet["text"],
                        "link": None,
                    },
                    "annotations": {
                        "bold": False,
                        "italic": False,
                        "strikethrough": False,
                        "underline": False,
                        "code": False,
                        "color": "default",
                    },
                }
            ]
        },
        "URL": {"url": tweet["url"]},
        "æŠ•ç¨¿æ—¥æ™‚": {"date": {"start": tweet["date"]} if tweet["date"] else None},
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": {"select": {"name": "æœªå›ç­”"}},
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°": {
            "number": (
                int(tweet["impressions"])
                if tweet.get("impressions") is not None
                else None
            )
        },
        "ãƒªãƒã‚¹ãƒˆæ•°": {
            "number": int(tweet["retweets"]) if tweet.get("retweets") is not None else 0
        },
        "ã„ã„ã­æ•°": {
            "number": int(tweet["likes"]) if tweet.get("likes") is not None else 0
        },
        "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°": {
            "number": (
                int(tweet["bookmarks"]) if tweet.get("bookmarks") is not None else 0
            )
        },
        "ãƒªãƒ—ãƒ©ã‚¤æ•°": {
            "number": int(tweet["replies"]) if tweet.get("replies") is not None else 0
        },
    }

    image_files = save_media(tweet["images"], "images")
    video_files = save_media(tweet["videos"], "videos")

    children_blocks = []

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã™ã¹ã¦file blockã¨ã—ã¦è¿½åŠ 
    for path in image_files:
        children_blocks.append(
            {
                "object": "block",
                "type": "file",
                "file": {
                    "type": "external",
                    "external": {"url": f"file://{os.path.abspath(path)}"},
                },
            }
        )

    # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚åŒæ§˜ã«è¿½åŠ 
    for path in video_files:
        children_blocks.append(
            {
                "object": "block",
                "type": "file",
                "file": {
                    "type": "external",
                    "external": {"url": f"file://{os.path.abspath(path)}"},
                },
            }
        )

    try:
        new_page = notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties=props,
            children=children_blocks if children_blocks else [],
        )
        print(f"ğŸ“ Notionç™»éŒ²å®Œäº†: {tweet['url']}")
    except Exception as e:
        print(f"âŒ Notionç™»éŒ²å¤±æ•—: {tweet['id']} ã‚¨ãƒ©ãƒ¼: {e}")


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def is_recruit_account(name, bio, config):
    return any(
        k in name or k in bio for k in config.get("filter_keywords_name_bio", [])
    )


def is_recruit_post(text, config):
    return any(k in text for k in config.get("filter_keywords_tweet", []))


def search_accounts(driver, keyword_list):
    results = []
    for keyword in keyword_list:
        search_url = f"https://twitter.com/search?q={keyword}&f=user"
        driver.get(search_url)
        time.sleep(3)

        # âš  æ–°UIæ§‹é€ ã«å¯¾å¿œ
        users = driver.find_elements(
            By.XPATH, "//a[contains(@href, '/')]//div[@dir='auto']/../../.."
        )
        print(f"ğŸ” å€™è£œãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¶æ•°: {len(users)}")

        for user in users:
            try:
                spans = user.find_elements(By.XPATH, ".//span")
                name = ""
                username = ""

                for span in spans:
                    text = span.text.strip()
                    if text.startswith("@"):
                        username = text.replace("@", "")
                    elif not name:
                        name = text

                if username and name:
                    results.append(
                        {
                            "name": name,
                            "username": username,
                            "bio": "",  # ã“ã®æ®µéšã§ã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»é¢ã«é£›ã‚“ã§ã„ãªã„
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºå¤±æ•—: {e}")
                continue

    return results


def merge_replies_with_driver(driver, tweet):
    try:
        driver.get(tweet["url"])
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )

        replies = extract_self_replies(driver, tweet.get("username", ""))
        if not isinstance(replies, list):
            print(
                f"âš ï¸ merge_replies_with_driver() ã§å–å¾—ã—ãŸrepliesãŒä¸æ­£ãªå‹: {type(replies)} â†’ ç©ºãƒªã‚¹ãƒˆã«ç½®æ›"
            )
            replies = []

        # âœ… Noneå¯¾ç­–
        tweet_text = tweet.get("text") or ""
        replies = sorted(
            [r for r in replies if r.get("id") and r.get("text")],
            key=lambda x: int(x["id"]),
        )

        existing_chunks = set(tweet_text.strip().split("\n\n"))
        reply_texts = []

        for r in replies:
            reply_id = r["id"]
            reply_body = r["text"].strip()
            clean_body = reply_body[:20].replace("\n", " ")
            print(f"ğŸ§µ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå€™è£œ: ID={reply_id} | textå…ˆé ­: {clean_body}")

            if not reply_body:
                continue
            if reply_body in existing_chunks:
                continue
            if r["id"] == tweet["id"]:
                continue

            reply_texts.append(reply_body)
            existing_chunks.add(reply_body)

        if reply_texts:
            tweet["text"] = tweet_text + "\n\n" + "\n\n".join(reply_texts)

    except Exception as e:
        print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå¤±æ•—ï¼ˆ{tweet.get('url', 'ä¸æ˜URL')}ï¼‰: {e}")
    return tweet


def extract_from_search(driver, keywords, max_tweets, name_bio_keywords=None):
    tweets = []
    seen_urls = set()
    seen_users = set()

    for keyword in keywords:
        print(f"ğŸ” è©±é¡Œã®ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ä¸­: {keyword}")
        search_url = f"https://twitter.com/search?q={keyword}&src=typed_query&f=top"
        driver.get(search_url)
        time.sleep(3)

        scroll_count = 0
        max_scrolls = 10
        pause_counter = 0
        pause_threshold = 3
        last_article_count = 0

        while len(tweets) < max_tweets and scroll_count < max_scrolls:
            articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            article_count = len(articles)
            print(f"ğŸ“„ è¡¨ç¤ºä¸­ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {article_count}")
            for article in articles:
                try:
                    # ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åå–å¾—
                    time_el = article.find_element(By.XPATH, ".//time")
                    tweet_href = time_el.find_element(By.XPATH, "..").get_attribute(
                        "href"
                    )
                    tweet_url = (
                        tweet_href
                        if tweet_href.startswith("http")
                        else f"https://x.com{tweet_href}"
                    )
                    if tweet_url in seen_urls:
                        continue
                    seen_urls.add(tweet_url)

                    name_block = article.find_element(
                        By.XPATH, ".//div[@data-testid='User-Name']"
                    )
                    spans = name_block.find_elements(By.XPATH, ".//span")
                    display_name = ""
                    username = ""
                    for s in spans:
                        text = s.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not display_name:
                            display_name = text

                    if not username or username in seen_users:
                        continue
                    seen_users.add(username)

                    # bioãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸å…ˆã«ã‚¢ã‚¯ã‚»ã‚¹
                    if name_bio_keywords:
                        driver.execute_script("window.open('');")
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.get(f"https://twitter.com/{username}")
                        time.sleep(2)
                        try:
                            bio_text = (
                                WebDriverWait(driver, 5)
                                .until(
                                    EC.presence_of_element_located(
                                        (
                                            By.XPATH,
                                            "//div[@data-testid='UserDescription']",
                                        )
                                    )
                                )
                                .text
                            )
                        except:
                            bio_text = ""
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])

                        if not any(
                            k in display_name for k in name_bio_keywords
                        ) and not any(k in bio_text for k in name_bio_keywords):
                            print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿éä¸€è‡´ â†’ ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                            continue

                    # âœ… æ¡ä»¶ã‚’é€šéã—ãŸå ´åˆã®ã¿æŠ•ç¨¿è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦æŠ½å‡º
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(tweet_url)
                    WebDriverWait(driver, 10).until(EC.url_contains("/status/"))

                    try:
                        full_text_el = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located(
                                (By.XPATH, "//div[@data-testid='tweetText']")
                            )
                        )
                        text = full_text_el.text.strip()
                    except Exception as e:
                        print(f"âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")
                        text = ""

                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆå®‰å®šåŒ– + ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« + ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–ï¼‰
                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸å†…ã€ã‚¨ãƒ©ãƒ¼å›é¿ãƒ»å¤šæ®µæ§‹é€ ã«å¯¾å¿œï¼‰
                    date = ""
                    for attempt in range(5):
                        try:
                            driver.execute_script("window.scrollTo(0, 0);")
                            WebDriverWait(driver, 3).until(
                                EC.presence_of_element_located((By.XPATH, "//article"))
                            )
                            time_el = driver.find_element(By.XPATH, "//article//a/time")
                            if time_el:
                                date = time_el.get_attribute("datetime")
                                break
                        except Exception as e:
                            print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—è©¦è¡Œ {attempt+1}/5 å¤±æ•—: {e}")
                            time.sleep(1)

                    if not date:
                        print("âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—ã«å¤±æ•— â†’ ç©ºæ–‡å­—ã§ç¶™ç¶š")

                    # è‡ªãƒªãƒ—ãƒ©ã‚¤å–å¾—ï¼ˆçœç•¥å¯ï¼‰
                    replies = extract_self_replies(driver, username)
                    if replies:
                        reply_texts = [
                            r["text"]
                            for r in replies
                            if "text" in r and r["text"] not in text
                        ]
                        if reply_texts:
                            text += "\n\n" + "\n\n".join(reply_texts)

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])
                    if already_registered(tweet_id):
                        print(f"ğŸš« ç™»éŒ²æ¸ˆ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                        continue

                    tweets.append(
                        {
                            "url": tweet_url,
                            "text": text,
                            "date": date,
                            "id": tweet_id,
                            "images": [],
                            "videos": [],
                            "username": username,
                            "display_name": display_name,
                        }
                    )

                    print(f"âœ… åé›†: {tweet_url} @{username}")
                    if len(tweets) >= max_tweets:
                        break

                except Exception as e:
                    print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # èª­ã¿è¾¼ã¿åˆ¤å®š
            if article_count == last_article_count:
                pause_counter += 1
                print("ğŸ§Š ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«æ–°ã—ã„æŠ•ç¨¿ãªã—")
                if pause_counter >= pause_threshold:
                    print("ğŸ›‘ æŠ•ç¨¿ãŒå¢—ãˆãªã„ãŸã‚ä¸­æ–­")
                    break
            else:
                pause_counter = 0

            last_article_count = article_count
            scroll_count += 1

    return tweets


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument(
        "--account", default="accounts.json", help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰"
    )
    args = parser.parse_args()

    config = load_config(args.config)
    account = load_config(args.account)

    global NOTION_TOKEN, DATABASE_ID, notion
    global TWITTER_EMAIL, TWITTER_USERNAME, TWITTER_PASSWORD
    global EXTRACT_TARGET, MAX_TWEETS

    NOTION_TOKEN = config["notion_token"]
    DATABASE_ID = config["database_id"]
    EXTRACT_TARGET = config["extract_target"]
    MAX_TWEETS = config["max_tweets"]

    TWITTER_EMAIL = account["email"]
    TWITTER_USERNAME = account["username"]
    TWITTER_PASSWORD = account["password"]

    notion = Client(auth=NOTION_TOKEN)
    driver = setup_driver()

    login(driver, EXTRACT_TARGET if config["mode"] == "target_only" else None)

    if config["mode"] == "target_only":
        print(
            f"ğŸ¯ mode: target_only â†’ extract_target = {EXTRACT_TARGET} ã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™"
        )

        # âœ… å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’ã‚‚ã£ã¦ URL ã‚’å¤šã‚ã«åé›†
        URL_BUFFER_FACTOR = 3
        tweet_dicts = extract_tweets(
            driver, EXTRACT_TARGET, MAX_TWEETS * URL_BUFFER_FACTOR
        )
        tweet_urls = [t["url"] for t in tweet_dicts if "url" in t]

        # âœ… å®Ÿéš›ã«ç™»éŒ²æˆåŠŸã—ãŸä»¶æ•°ãŒ MAX_TWEETS ã«é”ã™ã‚‹ã¾ã§å‡¦ç†
        tweets = extract_and_merge_tweets(driver, tweet_urls, MAX_TWEETS)

    elif config["mode"] == "search_filtered":
        print(
            "ğŸ” mode: search_filtered â†’ æ¤œç´¢ + name/bio + tweetãƒ•ã‚£ãƒ«ã‚¿ã‚’ã‹ã‘ã¦æŠ•ç¨¿ã‚’åé›†ã—ã¾ã™"
        )
        users = search_accounts(driver, config["filter_keywords_name_bio"])
        tweets = []
        for user in users:
            if is_recruit_account(user["name"], user["bio"], config):
                user_tweets = extract_tweets(driver, user["username"], MAX_TWEETS)
                tweets.extend(
                    [t for t in user_tweets if is_recruit_post(t["text"], config)]
                )

    elif config["mode"] == "search_all":
        print("ğŸŒ mode: search_all â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ â†’ bioãƒ•ã‚£ãƒ«ã‚¿ â†’ å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿å–å¾—")
        tweets = []
        remaining = MAX_TWEETS

        for keyword in config["filter_keywords_tweet"]:
            search_url = f"https://twitter.com/search?q={keyword}&f=user"
            driver.get(search_url)
            time.sleep(3)

            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located(
                        (By.XPATH, "//button[@data-testid='UserCell']")
                    )
                )
            except:
                print("âš ï¸ UserCellãŒä¸€å®šæ™‚é–“ä»¥å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

            user_elements = driver.find_elements(
                By.XPATH, "//button[@data-testid='UserCell']"
            )
            print(f"ğŸ“„ æ¤œå‡ºãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(user_elements)}")

            for user_el in user_elements:
                if remaining <= 0:
                    print("ğŸ¯ æœ€å¤§ä»¶æ•°ã«é”ã—ãŸãŸã‚çµ‚äº†")
                    break

                try:
                    spans = user_el.find_elements(By.XPATH, ".//span")
                    name, username = "", ""
                    for span in spans:
                        text = span.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not name:
                            name = text

                    if not username:
                        continue

                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(f"https://twitter.com/{username}")
                    time.sleep(2)

                    try:
                        bio = (
                            WebDriverWait(driver, 5)
                            .until(
                                EC.presence_of_element_located(
                                    (By.XPATH, "//div[@data-testid='UserDescription']")
                                )
                            )
                            .text
                        )
                    except:
                        bio = ""

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    bio_keywords = config["filter_keywords_name_bio"]
                    if not any(k in name for k in bio_keywords) and not any(
                        k in bio for k in bio_keywords
                    ):
                        continue

                    print(
                        f"âœ… æŠ½å‡ºå¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ â†’ @{username} | name: '{name}' | bio: '{bio}'"
                    )

                    tweet_dicts = extract_tweets(driver, username, remaining)
                    tweet_urls = [t["url"] for t in tweet_dicts if "url" in t]
                    tweets_for_user = extract_and_merge_tweets(
                        driver, tweet_urls, remaining
                    )

                    tweets.extend(tweets_for_user)
                    remaining -= len(tweets_for_user)

                except Exception as e:
                    print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    try:
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])
                    except:
                        pass
                    continue

    elif config["mode"] == "keyword_trend":
        print("ğŸ”¥ mode: keyword_trend â†’ æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©±é¡ŒæŠ•ç¨¿ã‚’åé›†ã—ã¾ã™")
        tweets = extract_from_search(
            driver,
            config["filter_keywords_tweet"],
            MAX_TWEETS,
            config.get("filter_keywords_name_bio"),
        )

    else:
        raise ValueError(f"âŒ æœªçŸ¥ã®modeæŒ‡å®šã§ã™: {config['mode']}")

    # æŠ•ç¨¿åé›†ã¨æ•´åˆæ€§ä¿è¨¼ä»˜ãç™»éŒ²å‡¦ç†
    print(f"\nğŸ“Š å–å¾—ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(tweets)} ä»¶")

    # âœ… æŠ•ç¨¿IDæ˜‡é †ã§ä¸¦ã¹æ›¿ãˆã¦ã‹ã‚‰ç™»éŒ²ï¼ˆé †ç•ªä¿è¨¼ï¼‰
    tweets.sort(key=lambda x: int(x["id"]))

    for i, tweet in enumerate(tweets, 1):
        print(f"\nğŸŒ€ {i}/{len(tweets)} ä»¶ç›® å‡¦ç†ä¸­...")
        print(json.dumps(tweet, ensure_ascii=False, indent=2))
        tweet = merge_replies_with_driver(driver, tweet)
        upload_to_notion(tweet)

    driver.quit()
    print("âœ… å…¨æŠ•ç¨¿ã®å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
