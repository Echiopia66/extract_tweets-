import os
import re
import cv2
import time
import json
import argparse
import traceback
import requests
import pytesseract
from PIL import Image, ImageFilter, ImageEnhance
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    StaleElementReferenceException,
    NoSuchElementException,
)
from notion_client import Client
from datetime import datetime
import shutil

# âœ… åºƒå‘Šé™¤å¤–ã€RT/å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ã€æŠ•ç¨¿IDè£œå®Œä»˜ã
AD_KEYWORDS = [
    "r10.to",
    "ãµã‚‹ã•ã¨ç´ç¨",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³",
    "ãŠé‡‘å€Ÿã‚Šã‚‰ã‚Œã‚‹",
    "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¬ãƒ¬ãƒ¼ã‚¸",
    "UNEXT",
    "ã‚¨ã‚³ã‚ªã‚¯",
    "#PR",
    "æ¥½å¤©",
    "Amazon",
    "A8",
    "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    "å‰¯æ¥­",
    "bit.ly",
    "shp.ee",
    "t.co/",
]


def normalize_text(text):
    return text.strip()


def login(driver, target=None):
    if os.path.exists("twitter_cookies.json"):
        print("âœ… Cookieã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º â†’ ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—")
        print("ğŸŒ https://twitter.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­â€¦")
        driver.get("https://twitter.com/")
        driver.delete_all_cookies()
        with open("twitter_cookies.json", "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.get(f"https://twitter.com/{target or TWITTER_USERNAME}")
        return

    print("ğŸ” åˆå›ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹")
    driver.get("https://twitter.com/i/flow/login")
    email_input = WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.NAME, "text"))
    )
    email_input.send_keys(TWITTER_EMAIL)
    email_input.send_keys(Keys.ENTER)
    time.sleep(2)

    try:
        username_input = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "text"))
        )
        username_input.send_keys(TWITTER_USERNAME)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)
    except Exception:
        print("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åå…¥åŠ›ã‚¹ã‚­ãƒƒãƒ—")

    password_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.NAME, "password"))
    )
    password_input.send_keys(TWITTER_PASSWORD)
    password_input.send_keys(Keys.ENTER)
    time.sleep(6)

    cookies = driver.get_cookies()
    with open("twitter_cookies.json", "w") as f:
        json.dump(cookies, f)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ â†’ æŠ•ç¨¿è€…ãƒšãƒ¼ã‚¸ã«é·ç§»")
    driver.get(f"https://twitter.com/{EXTRACT_TARGET}")


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  â† ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=options)


def extract_tweet_id(article):
    href_els = article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
    for el in href_els:
        h = el.get_attribute("href")
        m = re.search(r"/status/(\d+)", h or "")
        if m:
            return m.group(1)
    return None


def ocr_image(image_path):
    try:
        img = Image.open(image_path)
        img = img.convert("L")
        img = img.resize((img.width * 2, img.height * 2))
        img = ImageEnhance.Contrast(img).enhance(2.0)
        img = img.filter(ImageFilter.SHARPEN)
        import numpy as np

        img_np = np.array(img)
        img_np = cv2.medianBlur(img_np, 3)
        _, img_np = cv2.threshold(img_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        img = Image.fromarray(img_np)
        text = pytesseract.image_to_string(img, lang="jpn", config="--oem 1 --psm 6")
        print(f"ğŸ“ OCRç”»åƒ({image_path})çµæœ:\n{text.strip()}")
        if not text.strip() or sum(c.isalnum() for c in text) < 3:
            print(f"âš ï¸ OCRç”»åƒ({image_path})ã§æ–‡å­—åŒ–ã‘ã¾ãŸã¯èªè­˜å¤±æ•—ã®å¯èƒ½æ€§")
        return text.strip()
    except Exception as e:
        print(f"OCRå¤±æ•—({image_path}): {e}")
        return "[OCRã‚¨ãƒ©ãƒ¼]"


def extract_self_replies(driver, username):
    replies = []
    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” extract_self_replies: ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–")
            break

        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")

        def is_quote_reply(article):
            quote_els = article.find_elements(
                By.XPATH,
                ".//*[contains(text(), 'å¼•ç”¨')] | .//*[contains(text(), 'Quote')]",
            )
            quote_struct = article.find_elements(
                By.XPATH, ".//div[contains(@aria-label, 'å¼•ç”¨')]"
            )
            return bool(quote_els or quote_struct)

        for article in articles:
            try:
                handle_el = article.find_element(
                    By.XPATH,
                    ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                )
                handle = handle_el.text.strip()
                if handle.replace("@", "") != username:
                    continue

                if is_quote_reply(article):
                    print("âš ï¸ extract_self_replies: å¼•ç”¨RTå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                reply_text = text_el.text.strip() if text_el and text_el.text else ""

                tweet_id = extract_tweet_id(article)
                if not tweet_id:
                    print("âš ï¸ extract_self_replies: tweet_idãŒå–å¾—ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ç”»åƒãƒ»å‹•ç”»æƒ…å ±ã‚‚å–å¾—
                images = [
                    img.get_attribute("src")
                    for img in article.find_elements(
                        By.XPATH,
                        ".//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                    )
                    if img.get_attribute("src")
                ]
                video_posters = [
                    v.get_attribute("poster")
                    for v in article.find_elements(By.XPATH, ".//video")
                    if v.get_attribute("poster")
                ]

                if reply_text:
                    replies.append(
                        {
                            "id": tweet_id,
                            "text": reply_text,
                            "images": images,
                            "video_posters": video_posters,
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue
    return replies


def is_ad_post(text):
    lowered = text.lower()
    return any(k.lower() in lowered for k in AD_KEYWORDS)


def extract_thread_from_detail_page(driver, tweet_url):
    print(f"\nğŸ•µï¸ æŠ•ç¨¿ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {tweet_url}")
    driver.get(tweet_url)

    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']//time[@datetime]")
            )
        )
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿è¨˜äº‹ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å–å¾—ã«å¤±æ•—: {e}")
        return []

    if (
        "Something went wrong" in driver.page_source
        or "ã“ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ã¾ã›ã‚“" in driver.page_source
    ):
        print(f"âŒ æŠ•ç¨¿ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {tweet_url}")
        return []

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    tweet_blocks = []
    current_id_from_url = re.sub(r"\D", "", tweet_url.split("/")[-1])

    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")
    print(f"cellInnerDivæ•°: {len(cell_divs)}")
    cell_divs = sorted(cell_divs, key=get_transform_y)

    found_other_user_reply_in_thread = False
    for cell_idx, cell in enumerate(cell_divs):
        if found_other_user_reply_in_thread:
            break

        articles_in_cell = cell.find_elements(
            By.XPATH, ".//article[@data-testid='tweet']"
        )
        if not articles_in_cell:
            continue

        for article_idx, article in enumerate(articles_in_cell):
            if found_other_user_reply_in_thread:
                break

            tweet_id = None
            username = ""
            try:
                time_links = article.find_elements(
                    By.XPATH, ".//a[.//time[@datetime] and contains(@href, '/status/')]"
                )
                if time_links:
                    href = time_links[0].get_attribute("href")
                    match = re.search(r"/status/(\d{10,})", href)
                    if match:
                        tweet_id = match.group(1)

                if not tweet_id:
                    all_status_links = article.find_elements(
                        By.XPATH, ".//a[contains(@href, '/status/')]"
                    )
                    if all_status_links:
                        href = all_status_links[0].get_attribute("href")
                        match = re.search(r"/status/(\d{10,})", href)
                        if match:
                            tweet_id = match.group(1)

                if not tweet_id:
                    continue

                try:
                    username_el = article.find_element(
                        By.XPATH,
                        ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                    )
                    username = username_el.text.replace("@", "").strip()
                except:
                    pass

                if not username:
                    continue

                if username != EXTRACT_TARGET:
                    if tweet_id != current_id_from_url:
                        found_other_user_reply_in_thread = True
                        break
                    else:
                        pass

                text = ""
                try:
                    tweet_text_element = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    text_content = driver.execute_script(
                        "return arguments[0].innerText;", tweet_text_element
                    )
                    text = text_content.strip() if text_content else ""
                except NoSuchElementException:
                    text = ""
                except Exception as e_text:
                    print(
                        f"âš ï¸ æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (ID: {tweet_id}): {type(e_text).__name__} - {e_text}"
                    )
                    text = ""

                is_quote_tweet = False
                try:
                    if article.find_elements(
                        By.XPATH,
                        ".//div[@role='link' and .//article[@data-testid='tweet']]",
                    ):
                        is_quote_tweet = True
                except NoSuchElementException:
                    pass
                except Exception as e_quote_check_detail:
                    print(
                        f"âš ï¸ è©³ç´°ãƒšãƒ¼ã‚¸å¼•ç”¨RTåˆ¤å®šä¸­ã‚¨ãƒ©ãƒ¼ (ID: {tweet_id}): {type(e_quote_check_detail).__name__} - {e_quote_check_detail}"
                    )

                images = []
                all_tweet_photo_imgs = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media')]",
                )
                for img_el in all_tweet_photo_imgs:
                    try:
                        img_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if img_ancestor_article != article:
                            continue
                        img_el.find_element(By.XPATH, "ancestor::div[@role='link']")
                        continue
                    except NoSuchElementException:
                        pass
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue
                    src = img_el.get_attribute("src")
                    if src and src not in images:
                        images.append(src)

                all_card_imgs = article.find_elements(
                    By.XPATH, ".//img[contains(@src, 'twimg.com/card_img')]"
                )
                for img_el in all_card_imgs:
                    try:
                        img_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if img_ancestor_article != article:
                            continue
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue
                    src = img_el.get_attribute("src")
                    if src and src not in images:
                        images.append(src)

                video_posters = []
                video_xpath_candidates = [
                    ".//div[@data-testid='videoPlayer']//video[@poster]",
                    ".//div[@data-testid='videoComponent']//video[@poster]",
                    ".//video[@poster]",
                ]
                all_video_elements = []
                for xpath_candidate in video_xpath_candidates:
                    all_video_elements = article.find_elements(
                        By.XPATH, xpath_candidate
                    )
                    if all_video_elements:
                        break

                for v_el in all_video_elements:
                    try:
                        video_ancestor_article = v_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if video_ancestor_article != article:
                            continue
                        v_el.find_element(By.XPATH, "ancestor::div[@role='link']")
                        continue
                    except NoSuchElementException:
                        pass
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue
                    poster_url = v_el.get_attribute("poster")
                    if poster_url:
                        poster_filename = (
                            f"video_poster_{tweet_id}_{len(video_posters)}.jpg"
                        )
                        temp_poster_dir = "temp_posters"
                        if not os.path.exists(temp_poster_dir):
                            os.makedirs(temp_poster_dir)
                        poster_path = os.path.join(temp_poster_dir, poster_filename)
                        try:
                            resp = requests.get(poster_url, stream=True, timeout=10)
                            with open(poster_path, "wb") as f:
                                for chunk in resp.iter_content(1024):
                                    f.write(chunk)
                            video_posters.append(poster_path)
                        except Exception as e_poster:
                            print(f"âŒ posterç”»åƒä¿å­˜å¤±æ•— (ID: {tweet_id}): {e_poster}")

                time_els = article.find_elements(By.XPATH, ".//time")
                date_str = time_els[0].get_attribute("datetime") if time_els else None

                text_length = len(text)
                has_media = bool(images or video_posters)
                print(
                    f"DEBUG has_media check: ID {tweet_id}, images: {len(images)}, posters: {len(video_posters)}, has_media: {has_media}"
                )

                tweet_blocks.append(
                    {
                        "article_element": article,
                        "text": text,
                        "date": date_str,
                        "id": tweet_id,
                        "username": username,
                        "images": images,
                        "video_posters": video_posters,
                        "is_quote_tweet": is_quote_tweet,
                        "text_length": text_length,
                        "has_media": has_media,
                    }
                )
            except StaleElementReferenceException:
                break
            except Exception as e:
                continue
        if found_other_user_reply_in_thread:
            break

    def remove_temp_posters_from_list(blocks_to_clean):
        for block in blocks_to_clean:
            for poster_p in block.get("video_posters", []):
                if os.path.exists(poster_p):
                    try:
                        os.remove(poster_p)
                    except Exception:
                        pass

    if not tweet_blocks:
        print(f"âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ (URL: {tweet_url})")
        return []

    initial_post_data = None
    for block in tweet_blocks:
        if block["id"] == current_id_from_url:
            initial_post_data = block
            break

    if not initial_post_data:
        print(
            f"âš ï¸ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ãŒæŠ½å‡ºãƒ–ãƒ­ãƒƒã‚¯å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    if initial_post_data["username"] != EXTRACT_TARGET:
        print(
            f"ğŸ›‘ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼(@{initial_post_data['username']})ãŒå¯¾è±¡({EXTRACT_TARGET})ã¨ç•°ãªã‚Šã¾ã™ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    final_results = []
    for block_item in tweet_blocks:
        if block_item["username"] != EXTRACT_TARGET:
            remove_temp_posters_from_list([block_item])
            continue
        if is_ad_post(block_item["text"]):
            print(f"ğŸš« åºƒå‘ŠæŠ•ç¨¿ï¼ˆID: {block_item['id']}ï¼‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            remove_temp_posters_from_list([block_item])
            continue

        impressions, retweets, likes, bookmarks, replies_count = extract_metrics(
            block_item["article_element"]
        )

        final_block = block_item.copy()
        final_block.pop("article_element", None)

        final_block.update(
            {
                "url": f"https://x.com/{block_item['username']}/status/{block_item['id']}",
                "impressions": impressions,
                "retweets": retweets,
                "likes": likes,
                "bookmarks": bookmarks,
                "replies": replies_count,
            }
        )
        final_results.append(final_block)

    if not final_results:
        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµæœã€æœ‰åŠ¹ãªæŠ•ç¨¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        final_ids = {item["id"] for item in final_results}
        for block in tweet_blocks:
            if block["id"] not in final_ids:
                remove_temp_posters_from_list([block])
        return []

    final_results.sort(key=lambda x: int(x["id"]))
    return final_results


def extract_and_merge_tweets(driver, tweet_urls_data, max_tweets_to_register):
    final_tweets_for_notion = []
    # processed_ids ã¯ã€ä¸»ã«ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã‚„ã€æ¡ä»¶2,3ã§ã€Œé€”ä¸­ã§ã€ç™»éŒ²ãŒç¢ºå®šã—ãŸæŠ•ç¨¿ã®IDã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã€‚
    # ãƒ«ãƒ¼ãƒ—ã®æœ€å¾Œã«æ®‹ã‚‹ parent_post_candidate ã‚„ã€DBç™»éŒ²æ¸ˆã¿ã®ã‚‚ã®ã¯ã“ã“ã«ã¯å¿…ãšã—ã‚‚å…¥ã‚‰ãªã„ã€‚
    processed_ids = set()
    actually_registered_count = 0

    tweet_urls_data.sort(
        key=lambda x: (
            int(x["id"])
            if isinstance(x, dict) and x.get("id") and x["id"].isdigit()
            else float("inf")
        )
    )

    for i, meta in enumerate(tweet_urls_data):
        if actually_registered_count >= max_tweets_to_register:
            print(
                f"ğŸ¯ Notionã¸ã®ç™»éŒ²ä»¶æ•°ãŒ {max_tweets_to_register} ã«é”ã—ãŸãŸã‚URLå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†"
            )
            break

        tweet_url = meta["url"] if isinstance(meta, dict) else meta

        try:
            thread_posts = extract_thread_from_detail_page(driver, tweet_url)
            if not thread_posts:
                continue

            parent_post_candidate = None
            current_parent_is_db_registered = (
                False  # ç¾åœ¨ã®è¦ªå€™è£œãŒDBç™»éŒ²æ¸ˆã¿ã‹ã®ãƒ•ãƒ©ã‚°
            )

            for post_idx, post_in_thread in enumerate(thread_posts):
                current_post_id = post_in_thread.get("id")

                if not current_post_id:
                    print("âš ï¸ IDãŒãªã„æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§å‡¦ç†ã™ã‚‹æŠ•ç¨¿ãŒã€æ—¢ã«ä½•ã‚‰ã‹ã®å½¢ã§å‡¦ç†æ¸ˆã¿ã‹ç¢ºèª
                # (final_tweets_for_notion ã«å…¥ã£ã¦ã„ã‚‹ã‹ã€processed_ids ã«å«ã¾ã‚Œã‚‹ã‹)
                # ãŸã ã—ã€ãã‚ŒãŒç¾åœ¨ã® parent_post_candidate è‡ªèº«ã®å ´åˆã¯ã€ã“ã®ãƒ«ãƒ¼ãƒ—ã®æœ€å¾Œã§è©•ä¾¡ã•ã‚Œã‚‹ã®ã§é™¤å¤–
                if any(
                    ftn_item["id"] == current_post_id
                    for ftn_item in final_tweets_for_notion
                ) or (
                    current_post_id in processed_ids
                    and (
                        not parent_post_candidate
                        or current_post_id != parent_post_candidate.get("id")
                    )
                ):
                    print(
                        f"DEBUG merge_logic: Post {current_post_id} ã¯æ—¢ã«ä»Šå›ã®å®Ÿè¡Œã§å‡¦ç†æ¸ˆã¿(ç™»éŒ²ãƒªã‚¹ãƒˆ/ãƒãƒ¼ã‚¸æ¸ˆ)ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
                    )
                    continue

                is_current_post_db_registered = already_registered(current_post_id)

                is_current_post_quote = post_in_thread.get("is_quote_tweet", False)
                current_text_len = post_in_thread.get("text_length", 0)
                current_has_media = post_in_thread.get("has_media", False)

                parent_id_for_log = (
                    parent_post_candidate.get("id") if parent_post_candidate else "None"
                )
                print(
                    f"DEBUG merge_logic: Processing Post ID: {current_post_id} (DB: {is_current_post_db_registered}), ParentCand: {parent_id_for_log} (ParentDB: {current_parent_is_db_registered}), Q: {is_current_post_quote}, M: {current_has_media}, Len: {current_text_len}"
                )

                if parent_post_candidate is None:  # ã‚¹ãƒ¬ãƒƒãƒ‰ã®æœ€åˆã®æœ‰åŠ¹ãªæŠ•ç¨¿
                    if is_current_post_db_registered:
                        parent_post_candidate = post_in_thread
                        current_parent_is_db_registered = True
                        processed_ids.add(
                            current_post_id
                        )  # DBç™»éŒ²æ¸ˆã¿ãªã®ã§ã€ã“ã‚Œè‡ªä½“ã¯ç™»éŒ²ã—ãªã„ãŒã€IDã¯å‡¦ç†æ¸ˆã¿ã¨ã™ã‚‹
                        print(
                            f"DEBUG merge_logic: Set parent_post_candidate to DB_REGISTERED post: {current_post_id}"
                        )
                    elif is_current_post_quote and not (
                        current_text_len >= 50 and current_has_media
                    ):  # æ¡ä»¶4
                        print(
                            f"â„¹ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹å€™è£œ {current_post_id} ã¯æ¡ä»¶4å¼•ç”¨RTã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
                        )
                        processed_ids.add(current_post_id)
                        # parent_post_candidate ã¯ None ã®ã¾ã¾æ¬¡ã®æŠ•ç¨¿ã¸
                    else:
                        parent_post_candidate = post_in_thread
                        current_parent_is_db_registered = False
                        print(
                            f"DEBUG merge_logic: Set NEW parent_post_candidate: {current_post_id}"
                        )
                    continue  # æ¬¡ã®æŠ•ç¨¿ã®å‡¦ç†ã¸

                # parent_post_candidate ãŒå­˜åœ¨ã™ã‚‹çŠ¶æ…‹
                if current_post_id == parent_post_candidate.get(
                    "id"
                ):  # è‡ªåˆ†è‡ªèº«ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue

                is_reply_to_parent = post_in_thread.get(
                    "username"
                ) == parent_post_candidate.get("username") and int(
                    post_in_thread.get("id", 0)
                ) > int(
                    parent_post_candidate.get("id", 0)
                )

                if not is_reply_to_parent:
                    # å‰ã®è¦ªå€™è£œã‚’è©•ä¾¡ã—ã¦ç™»éŒ²ãƒªã‚¹ãƒˆã«è¿½åŠ  (DBæœªç™»éŒ²ã®å ´åˆã®ã¿)
                    if parent_post_candidate and not current_parent_is_db_registered:
                        # æ¡ä»¶4 (å¼•ç”¨RTã§çŸ­æ–‡orãƒ¡ãƒ‡ã‚£ã‚¢ãªã—) ã®ãƒã‚§ãƒƒã‚¯
                        temp_is_quote = parent_post_candidate.get(
                            "is_quote_tweet", False
                        )
                        temp_text_len = parent_post_candidate.get("text_length", 0)
                        temp_has_media = parent_post_candidate.get("has_media", False)
                        if not (
                            temp_is_quote
                            and not (temp_text_len >= 50 and temp_has_media)
                        ):
                            if actually_registered_count < max_tweets_to_register:
                                if not any(
                                    ftn_item["id"] == parent_post_candidate.get("id")
                                    for ftn_item in final_tweets_for_notion
                                ):  # é‡è¤‡è¿½åŠ é˜²æ­¢
                                    final_tweets_for_notion.append(
                                        parent_post_candidate
                                    )
                                    actually_registered_count += 1
                                    print(
                                        f"âœ… è¦ªå€™è£œ(éãƒªãƒ—ãƒ©ã‚¤åˆ†å²)ã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ : {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                                    )
                                    # processed_ids.add(parent_post_candidate["id"]) # final_tweets_for_notion ã«å…¥ã£ãŸã®ã§ä¸è¦
                            else:
                                print(
                                    f"ğŸ¯ ç™»éŒ²ä¸Šé™(éãƒªãƒ—ãƒ©ã‚¤åˆ†å²): è¦ªå€™è£œ {parent_post_candidate['id']} ã‚¹ã‚­ãƒƒãƒ—"
                                )
                                if actually_registered_count >= max_tweets_to_register:
                                    break
                        else:
                            print(
                                f"â„¹ï¸ è¦ªå€™è£œ(éãƒªãƒ—ãƒ©ã‚¤åˆ†å²) {parent_post_candidate['id']} ã¯æ¡ä»¶4å¼•ç”¨RTã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
                            )

                    # æ–°ã—ã„æŠ•ç¨¿ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã—ã¦è¨­å®š
                    if is_current_post_db_registered:
                        parent_post_candidate = post_in_thread
                        current_parent_is_db_registered = True
                        processed_ids.add(current_post_id)
                        print(
                            f"DEBUG merge_logic: Set parent_post_candidate to DB_REGISTERED post (non-reply): {current_post_id}"
                        )
                    elif is_current_post_quote and not (
                        current_text_len >= 50 and current_has_media
                    ):  # æ¡ä»¶4
                        print(
                            f"â„¹ï¸ æ–°ã—ã„è¦ªå€™è£œ(éãƒªãƒ—ãƒ©ã‚¤åˆ†å²) {current_post_id} ã¯æ¡ä»¶4å¼•ç”¨RTã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
                        )
                        parent_post_candidate = None  # è¦ªå€™è£œãƒªã‚»ãƒƒãƒˆ
                        current_parent_is_db_registered = False
                        processed_ids.add(current_post_id)
                    else:
                        parent_post_candidate = post_in_thread
                        current_parent_is_db_registered = False
                        print(
                            f"DEBUG merge_logic: Set NEW parent_post_candidate (non-reply): {current_post_id}"
                        )
                    continue

                # --- ä»¥ä¸‹ã€ãƒªãƒ—ãƒ©ã‚¤ã®å ´åˆã®å‡¦ç† ---
                if (
                    is_current_post_db_registered
                ):  # ãƒªãƒ—ãƒ©ã‚¤è‡ªä½“ãŒDBç™»éŒ²æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                    print(
                        f"DEBUG merge_logic: Reply Post {current_post_id} ã¯DBç™»éŒ²æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"
                    )
                    processed_ids.add(current_post_id)
                    continue

                if is_current_post_quote:  # æ¡ä»¶3ã¾ãŸã¯æ¡ä»¶4ã®å¼•ç”¨RTãƒªãƒ—ãƒ©ã‚¤
                    if current_text_len >= 50 and current_has_media:  # æ¡ä»¶3
                        print(
                            f"DEBUG merge_logic: Post {current_post_id} is Condition 3 Quote Reply."
                        )
                        if actually_registered_count < max_tweets_to_register:
                            if not any(
                                ftn_item["id"] == current_post_id
                                for ftn_item in final_tweets_for_notion
                            ):  # é‡è¤‡è¿½åŠ é˜²æ­¢
                                final_tweets_for_notion.append(post_in_thread)
                                actually_registered_count += 1
                                print(
                                    f"âœ… æ¡ä»¶3ãƒªãƒ—ãƒ©ã‚¤ã‚’ç™»éŒ²: {current_post_id} ({actually_registered_count}/{max_tweets_to_register})"
                                )
                            # ã“ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã™ã‚‹
                            parent_post_candidate = post_in_thread
                            current_parent_is_db_registered = False  # æ–°è¦ç™»éŒ²ã—ãŸã®ã§DBã«ã¯ã¾ã ãªã„ (final_tweets_for_notionã«å…¥ã£ãŸ)
                        else:
                            break
                    else:  # æ¡ä»¶4
                        print(
                            f"DEBUG merge_logic: Post {current_post_id} is Condition 4 Quote Reply. Skipping."
                        )
                        processed_ids.add(current_post_id)
                elif current_has_media:  # æ¡ä»¶2: ç”»åƒä»˜ããƒªãƒ—ãƒ©ã‚¤
                    print(
                        f"DEBUG merge_logic: Post {current_post_id} is Condition 2 Media Reply."
                    )
                    if actually_registered_count < max_tweets_to_register:
                        if not any(
                            ftn_item["id"] == current_post_id
                            for ftn_item in final_tweets_for_notion
                        ):  # é‡è¤‡è¿½åŠ é˜²æ­¢
                            final_tweets_for_notion.append(post_in_thread)
                            actually_registered_count += 1
                            print(
                                f"âœ… æ¡ä»¶2ãƒªãƒ—ãƒ©ã‚¤ã‚’ç™»éŒ²: {current_post_id} ({actually_registered_count}/{max_tweets_to_register})"
                            )
                        # ã“ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã™ã‚‹
                        parent_post_candidate = post_in_thread
                        current_parent_is_db_registered = False  # æ–°è¦ç™»éŒ²ã—ãŸã®ã§DBã«ã¯ã¾ã ãªã„ (final_tweets_for_notionã«å…¥ã£ãŸ)
                    else:
                        break
                else:  # æ¡ä»¶1: æ–‡å­—ã®ã¿ã®é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤
                    print(
                        f"DEBUG merge_logic: Post {current_post_id} is Condition 1 Text-only Reply."
                    )
                    if current_parent_is_db_registered:  # è¦ªãŒDBç™»éŒ²æ¸ˆã¿
                        print(
                            f"â„¹ï¸ è¦ª({parent_post_candidate.get('id')})ãŒDBç™»éŒ²æ¸ˆã¿ã®ãŸã‚ã€æ–‡å­—ã®ã¿ãƒªãƒ—ãƒ©ã‚¤ {current_post_id} ã¯ãƒãƒ¼ã‚¸ã›ãšã‚¹ã‚­ãƒƒãƒ—"
                        )
                        processed_ids.add(current_post_id)
                    elif parent_post_candidate:  # è¦ªãŒDBæœªç™»éŒ²ãªã‚‰ãƒãƒ¼ã‚¸
                        parent_text_before_merge = parent_post_candidate.get("text", "")
                        reply_text_to_merge = post_in_thread.get("text", "")
                        parent_post_candidate["text"] = (
                            parent_text_before_merge + "\n\n" + reply_text_to_merge
                        ).strip()
                        parent_post_candidate["text_length"] = len(
                            parent_post_candidate["text"]
                        )
                        print(
                            f"ğŸ§µ ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ã‚¸ -> è¦ª: {parent_post_candidate.get('id')}, ãƒªãƒ—ãƒ©ã‚¤: {current_post_id}"
                        )
                        processed_ids.add(
                            current_post_id
                        )  # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã¯å‡¦ç†æ¸ˆã¿

                if actually_registered_count >= max_tweets_to_register:
                    break

            # ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã®å…¨æŠ•ç¨¿å‡¦ç†å¾Œã€æœ€å¾Œã«æ®‹ã£ãŸè¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸ (DBæœªç™»éŒ²ã®å ´åˆã®ã¿)
            if (
                parent_post_candidate
                and not current_parent_is_db_registered
                and not any(
                    ftn_item["id"] == parent_post_candidate.get("id")
                    for ftn_item in final_tweets_for_notion
                )
                and parent_post_candidate.get("id") not in processed_ids
            ):  # processed_ids ã«ã‚‚å…¥ã£ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª

                is_final_quote = parent_post_candidate.get("is_quote_tweet", False)
                final_text_len = parent_post_candidate.get("text_length", 0)
                final_has_media = parent_post_candidate.get("has_media", False)

                if is_final_quote and not (
                    final_text_len >= 50 and final_has_media
                ):  # æ¡ä»¶4
                    print(
                        f"â„¹ï¸ æœ€çµ‚è¦ªå€™è£œ {parent_post_candidate['id']} ã¯æ¡ä»¶4ã®å¼•ç”¨RTã®ãŸã‚ç™»éŒ²ã‚¹ã‚­ãƒƒãƒ—"
                    )
                elif actually_registered_count < max_tweets_to_register:
                    final_tweets_for_notion.append(parent_post_candidate)
                    actually_registered_count += 1
                    print(
                        f"âœ… æœ€çµ‚è¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ : {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                    )
                else:
                    print(
                        f"ğŸ¯ ç™»éŒ²ä¸Šé™ã®ãŸã‚æœ€çµ‚è¦ªå€™è£œ {parent_post_candidate['id']} ã‚¹ã‚­ãƒƒãƒ—"
                    )

            if actually_registered_count >= max_tweets_to_register:
                break

        except Exception as e:
            print(
                f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ ({tweet_url}): {type(e).__name__} - {e}\n{traceback.format_exc()}"
            )
            continue

    print(f"\nğŸ“ˆ æœ€çµ‚çš„ãªNotionç™»éŒ²å¯¾è±¡æŠ•ç¨¿æ•°: {len(final_tweets_for_notion)} ä»¶")
    return final_tweets_for_notion


def extract_metrics(article):
    """
    ã„ã„ã­æ•°ãƒ»ãƒªãƒã‚¹ãƒˆæ•°ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ»ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒ»ãƒªãƒ—ãƒ©ã‚¤æ•°ã‚’æŠ½å‡º
    å–å¾—ã§ããªã„ã‚‚ã®ã¯0ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿Noneï¼‰ã§è¿”ã™
    """
    impressions_str = retweets_str = likes_str = bookmarks_str = replies_str = None
    try:
        # å„ªå…ˆçš„ã« div[role="group"] ã® aria-label ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        # ã“ã‚ŒãŒæœ€ã‚‚æƒ…å ±ãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
        group_divs = article.find_elements(
            By.XPATH, ".//div[@role='group' and @aria-label]"
        )

        primary_label_processed = False
        if group_divs:
            for group_div in group_divs:
                label = group_div.get_attribute("aria-label")
                if not label:
                    continue

                print(f"ğŸŸ¦ metrics group aria-labelå†…å®¹: {label}")
                primary_label_processed = True  # ã“ã®ãƒ©ãƒ™ãƒ«ã‚’å‡¦ç†ã—ãŸã“ã¨ã‚’ãƒãƒ¼ã‚¯

                # å„æŒ‡æ¨™ã‚’å€‹åˆ¥ã«æŠ½å‡ºã™ã‚‹ (é †ç•ªã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«)
                m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                if m_replies:
                    replies_str = m_replies.group(1)

                m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                if m_retweets:
                    retweets_str = m_retweets.group(1)

                m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                if m_likes:
                    likes_str = m_likes.group(1)

                m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                if m_bookmarks:
                    bookmarks_str = m_bookmarks.group(1)

                m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                if m_impressions:
                    impressions_str = m_impressions.group(1)

                # ä¸€ã¤ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰å…¨ã¦å–ã‚ŒãŸã‚‰æŠœã‘ã‚‹ã“ã¨ãŒå¤šã„ãŒã€ç¨€ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ã€
                # åŸºæœ¬çš„ã«ã¯æœ€åˆã® group_div ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸»ã¨ã™ã‚‹ã€‚
                # ã‚‚ã—ã€è¤‡æ•°ã® group_div ãŒç•°ãªã‚‹æƒ…å ±ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒç¢ºèªã•ã‚Œã‚Œã°ã€ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯å†è€ƒã€‚
                break

        if not primary_label_processed:
            # group_div ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€aria-label ãŒãªã„å ´åˆã€ä»¥å‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚è©¦ã™
            # ãŸã ã—ã€ã“ã®ãƒ‘ã‚¹ã¯Xã®UIãŒå¤§ããå¤‰ã‚ã£ãŸå ´åˆã¯æ©Ÿèƒ½ã—ãªã„å¯èƒ½æ€§ãŒé«˜ã„
            other_divs = article.find_elements(
                By.XPATH,
                ".//div[contains(@aria-label, 'ä»¶ã®è¡¨ç¤º') and not(@role='group')]",
            )
            for div in other_divs:
                label = div.get_attribute("aria-label")
                if not label:
                    continue
                print(f"ğŸŸ¦ other metrics div aria-labelå†…å®¹: {label}")
                # ã“ã“ã§ã‚‚åŒæ§˜ã«å€‹åˆ¥æŠ½å‡ºã‚’è©¦ã¿ã‚‹ (ä¸Šè¨˜ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
                if replies_str is None:
                    m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                    if m_replies:
                        replies_str = m_replies.group(1)
                if retweets_str is None:
                    m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                    if m_retweets:
                        retweets_str = m_retweets.group(1)
                if likes_str is None:
                    m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                    if m_likes:
                        likes_str = m_likes.group(1)
                if bookmarks_str is None:
                    m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                    if m_bookmarks:
                        bookmarks_str = m_bookmarks.group(1)
                if impressions_str is None:
                    m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                    if m_impressions:
                        impressions_str = m_impressions.group(1)
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã§å‡¦ç†

        # å€‹åˆ¥ãƒœã‚¿ãƒ³ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—
        if replies_str is None:
            try:
                reply_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='reply']"
                )
                for btn in reply_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label or "")
                    if m:
                        replies_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤æ•°å–å¾—: {replies_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if retweets_str is None:
            try:
                rt_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='retweet']"
                )
                for btn in rt_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label or "")
                    if m:
                        retweets_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒã‚¹ãƒˆæ•°å–å¾—: {retweets_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒã‚¹ãƒˆæ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if likes_str is None:
            try:
                like_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='like']"
                )
                for btn in like_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label or "")
                    if m:
                        likes_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ã„ã„ã­æ•°å–å¾—: {likes_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ã„ã„ã­æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if bookmarks_str is None:
            try:
                bm_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='bookmark']"
                )
                for btn in bm_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label or "")
                    if m:
                        bookmarks_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—: {bookmarks_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã¯ãƒœã‚¿ãƒ³ã‹ã‚‰ã¯é€šå¸¸å–ã‚Œãªã„ã®ã§ã€aria-labelé ¼ã¿
        # ã‚‚ã— impressions_str ãŒ None ã§ã€ä»–ã®æŒ‡æ¨™ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆã€
        # ã‹ã¤ã¦ã®ã€Œã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€
        # likes/retweets/bookmarks/replies ãŒå…¨ã¦0ãªã‚‰ã€impressions_str ã‚’æ¡ç”¨ã—ä»–ã‚’0ã«ã™ã‚‹ã€‚
        # ãŸã ã—ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯è¤‡é›‘ãªã®ã§ã€ä¸€æ—¦ã¯ä¸Šè¨˜ã§å–å¾—ã§ããŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
        # ã‚‚ã—ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã ã‘ãŒå–ã‚Œã¦ä»–ãŒ0ã«ãªã‚‹ã¹ãã‚±ãƒ¼ã‚¹ãŒå¤šç™ºã™ã‚‹ãªã‚‰å†æ¤œè¨ã€‚

        def parse_num(s):
            if not s:
                return 0  # None ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯0ã¨ã—ã¦æ‰±ã† (ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ä»¥å¤–)
            s_cleaned = str(s).replace(",", "")
            if "ä¸‡" in s_cleaned:
                try:
                    return int(float(s_cleaned.replace("ä¸‡", "")) * 10000)
                except ValueError:
                    return 0  # "ä¸‡" ãŒã‚ã£ã¦ã‚‚æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆ
            try:
                return int(s_cleaned)
            except ValueError:  # "K" ã‚„ "M" ãªã©ã®è‹±èªåœã®ç•¥ç§°ã¯ç¾çŠ¶éå¯¾å¿œ
                return 0  # æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆã¯0

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ None ã‚’è¨±å®¹ã—ã€ä»–ã¯0ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
        impressions = (
            parse_num(impressions_str) if impressions_str is not None else None
        )
        retweets = parse_num(retweets_str)
        likes = parse_num(likes_str)
        bookmarks = parse_num(bookmarks_str)
        replies = parse_num(replies_str)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœ€çµ‚çš„ãªå€¤ã‚’è¡¨ç¤º
        print(
            f"ğŸ”¢ æŠ½å‡ºçµæœ: è¡¨ç¤º={impressions}, RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, ãƒªãƒ—ãƒ©ã‚¤={replies}"
        )

    except Exception as e:
        print(f"âš ï¸ extract_metricså…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (impressions=None, ä»–=0)
        impressions = None
        retweets = 0
        likes = 0
        bookmarks = 0
        replies = 0

    return impressions, retweets, likes, bookmarks, replies


def is_reply_structure(
    article,
    tweet_id=None,
    text="",
    image_urls=None,  # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã®articleè¦ç´ ã‹ã‚‰ç›´æ¥æŠ½å‡ºã•ã‚ŒãŸç”»åƒURLã®ãƒªã‚¹ãƒˆ
    video_poster_urls=None,  # åŒæ§˜ã«ã€å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼URLã®ãƒªã‚¹ãƒˆ
):
    try:
        id_display = f"ï¼ˆID={tweet_id}ï¼‰" if tweet_id else ""
        # print(f"DEBUG is_reply_structure: Checking ID {tweet_id}")

        # --- 0. è¨˜äº‹ãŒéè¡¨ç¤ºã§ãªã„ã‹ç¢ºèª ---
        try:
            if not article.is_displayed():
                # print(f"DEBUG (is_reply_structure): Article for ID {tweet_id} is not displayed, skipping as reply.")
                return True
        except StaleElementReferenceException:
            # print(f"DEBUG (is_reply_structure): Stale element checking display status for ID {tweet_id}, assuming reply.")
            return True

        # --- 1. å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆã®åˆ¤å®š ---
        is_quote_tweet_structure = False
        try:
            quoted_articles_inside = article.find_elements(
                By.XPATH, ".//article[@data-testid='tweet']"
            )
            if len(quoted_articles_inside) > 0:
                is_quote_tweet_structure = True

            if not is_quote_tweet_structure:
                quote_indicators = article.find_elements(
                    By.XPATH,
                    ".//div[contains(translate(., 'ï¼±ï¼µï¼¯ï¼´ï¼¥ï¼´ï¼·ï¼¥ï¼¥ï¼´', 'quotetweet'), 'quotetweet') or contains(@aria-label, 'å¼•ç”¨') or .//span[text()='å¼•ç”¨']]",
                )
                if any(el.is_displayed() for el in quote_indicators):
                    is_quote_tweet_structure = True
        except Exception as e_quote_check:
            print(
                f"âš ï¸ is_reply_structure: å¼•ç”¨åˆ¤å®šä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_quote_check).__name__}: {e_quote_check}"
            )
            is_quote_tweet_structure = False

        if is_quote_tweet_structure:
            text_length = len(text.strip()) if text else 0
            has_direct_media = bool(image_urls or video_poster_urls)
            if text_length < 30 and not has_direct_media:
                print(
                    f"ğŸ›‘ is_reply_structure: çŸ­æ–‡ã‹ã¤ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—å¼•ç”¨RT â†’ é™¤å¤– {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={has_direct_media}"
                )
                return True
            else:
                print(
                    f"âœ… is_reply_structure: å¼•ç”¨RTï¼ˆä¸Šè¨˜é™¤å¤–æ¡ä»¶ã«è©²å½“ã›ãšï¼‰â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={has_direct_media}"
                )
                return False

        # --- 2. socialContextã«ã‚ˆã‚‹ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š (å„ªå…ˆåº¦é«˜) ---
        try:
            social_context_elements = article.find_elements(
                By.XPATH, ".//div[@data-testid='socialContext']"
            )
            if not social_context_elements:  # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã®ãƒ­ã‚°
                print(
                    f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                )

            for sc_el in social_context_elements:
                if sc_el.is_displayed():
                    sc_text_content = sc_el.text
                    sc_text_lower = sc_text_content.lower()
                    # socialContextãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                    print(
                        f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ: '{sc_text_content}'"
                    )

                    if (
                        "replying to" in sc_text_lower
                        or "è¿”ä¿¡å…ˆ:" in sc_text_content
                        or re.search(r"@\w+\s*ã«è¿”ä¿¡", sc_text_content, re.IGNORECASE)
                        or "replied to" in sc_text_lower
                    ):
                        try:
                            sc_el.find_element(
                                By.XPATH,
                                "ancestor::div[@role='link' and .//article[@data-testid='tweet']]",
                            )
                            # print(f"DEBUG is_reply_structure: ID {tweet_id}, SocialContextã¯å¼•ç”¨RTå†…ã®ã‚‚ã®ã§ã—ãŸã€‚")
                            continue
                        except NoSuchElementException:
                            print(
                                f"ğŸ’¬ is_reply_structure (socialContext): ID {tweet_id} â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š (ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´: '{sc_text_content[:30]}...')"
                            )
                            return True
        except StaleElementReferenceException:
            pass
        except NoSuchElementException:
            print(
                f"DEBUG is_reply_structure: ID {tweet_id}, socialContextè¦ç´ ã®æ¤œç´¢ã§NoSuchElement (äºˆæœŸã›ã¬ã‚±ãƒ¼ã‚¹)ã€‚"
            )
            pass
        except Exception as e_sc_check:
            print(
                f"âš ï¸ is_reply_structure: socialContextç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_sc_check).__name__}: {e_sc_check}"
            )

        # --- 3. æ§‹é€ çš„ãªãƒªãƒ—ãƒ©ã‚¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®ç¢ºèª (å·¦å´ã®ç¸¦ç·šãªã©) ---
        try:
            # body.html ã®ãƒªãƒ—ãƒ©ã‚¤æ§‹é€  (article > div > div > div > div.[r-15zivkp & r-18kxxzh]) ã‚’å‚è€ƒ
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã§ã‚‚åŒæ§˜ã®æ§‹é€ ã§ãƒªãƒ—ãƒ©ã‚¤ç·šãŒæç”»ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
            # articleè¦ç´ ã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã§ã€ç‰¹å®šéšå±¤ã«ã‚ã‚‹ãƒªãƒ—ãƒ©ã‚¤ç·šè¦ç´ ã‚’æ¢ã™
            # body.htmlã®æ§‹é€ : article > div.r-eqz5dr > div.r-16y2uox > div.css-175oi2r > div.r-18u37iz > div (ãƒªãƒ—ãƒ©ã‚¤ç·š)
            # æœ€åˆã®æ•°éšå±¤ã®ã‚¯ãƒ©ã‚¹åã¯å¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã‚ˆã‚Šæ±ç”¨çš„ã«å­è¦ç´ ã‚’è¾¿ã‚‹
            xpath_for_reply_line = "./div/div/div/div[contains(@class, 'r-15zivkp') and contains(@class, 'r-18kxxzh')]"
            reply_line_indicators = article.find_elements(
                By.XPATH, xpath_for_reply_line
            )
            if reply_line_indicators:
                print(
                    f"DEBUG is_reply_structure: ID {tweet_id}, æ§‹é€ çš„ãƒªãƒ—ãƒ©ã‚¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿å€™è£œ {len(reply_line_indicators)}ä»¶æ¤œå‡º (XPath: {xpath_for_reply_line})"
                )

            for indicator in reply_line_indicators:
                if indicator.is_displayed():
                    # ã“ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãŒå¼•ç”¨RTå†…ã®ã‚‚ã®ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    try:
                        indicator.find_element(
                            By.XPATH,
                            "ancestor::div[@role='link' and .//article[@data-testid='tweet']]",
                        )
                        # å¼•ç”¨RTå†…ã®ãƒªãƒ—ãƒ©ã‚¤ç·šãªã‚‰ã€ã“ã®åˆ¤å®šã§ã¯ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ãªã„
                        # print(f"DEBUG is_reply_structure: ID {tweet_id}, æ§‹é€ çš„ãƒªãƒ—ãƒ©ã‚¤ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã¯å¼•ç”¨RTå†…ã®ã‚‚ã®")
                    except NoSuchElementException:
                        print(
                            f"ğŸ’¬ is_reply_structure (structural_reply_line_specific): ID {tweet_id} â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š"
                        )
                        return True
        except StaleElementReferenceException:
            pass
        except Exception as e_reply_line_check:
            print(
                f"âš ï¸ is_reply_structure: æ§‹é€ çš„ãƒªãƒ—ãƒ©ã‚¤ç·šç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_reply_line_check).__name__}: {e_reply_line_check}"
            )

        # --- 4. ã€Œè¿”ä¿¡å…ˆ: @ãƒ¦ãƒ¼ã‚¶ãƒ¼åã€ã¨ã„ã†ãƒ†ã‚­ã‚¹ãƒˆãŒè¨˜äº‹ãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†ã«ç›´æ¥è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹ ---
        try:
            base_condition = "starts-with(normalize-space(.), 'Replying to @') or starts-with(normalize-space(.), 'è¿”ä¿¡å…ˆ: @') or starts-with(normalize-space(.), 'In reply to @')"
            not_in_quote_condition = (
                "not(ancestor::div[@role='link' and .//article[@data-testid='tweet']])"
            )
            not_in_text_div_condition = "not(self::div[@data-testid='tweetText']) and not(ancestor::div[@data-testid='tweetText'])"
            xpath_for_div = f".//div[{base_condition} and {not_in_quote_condition} and {not_in_text_div_condition}]"
            not_in_text_span_condition = "not(ancestor::div[@data-testid='tweetText'])"
            xpath_for_span = f".//span[{base_condition} and {not_in_quote_condition} and {not_in_text_span_condition}]"

            reply_to_user_text_elements = []
            elements_div = article.find_elements(By.XPATH, xpath_for_div)
            if elements_div:
                reply_to_user_text_elements.extend(elements_div)
            elements_span = article.find_elements(By.XPATH, xpath_for_span)
            if elements_span:
                reply_to_user_text_elements.extend(elements_span)

            if not reply_to_user_text_elements:  # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã®ãƒ­ã‚°
                print(
                    f"DEBUG is_reply_structure: ID {tweet_id}, è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                )

            for el in reply_to_user_text_elements:
                if el.is_displayed():
                    el_text_content = ""
                    try:
                        el_text_content = el.text
                    except StaleElementReferenceException:
                        continue

                    # è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆå€™è£œãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
                    print(
                        f"DEBUG is_reply_structure: ID {tweet_id}, è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆå€™è£œ: '{el_text_content}'"
                    )

                    if "@" in el_text_content:
                        print(
                            f"ğŸ’¬ is_reply_structure (reply_to_user_text): ID {tweet_id} â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š (ãƒ†ã‚­ã‚¹ãƒˆä¸€è‡´: '{el_text_content[:30]}...')"
                        )
                        return True
        except StaleElementReferenceException:
            pass
        except Exception as e_indicator:
            print(
                f"âš ï¸ is_reply_structure: è¿”ä¿¡å…ˆãƒ†ã‚­ã‚¹ãƒˆç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_indicator).__name__}: {e_indicator}"
            )

        # --- 5. ãƒœã‚¿ãƒ³ã®æ•°ã«ã‚ˆã‚‹åˆ¤å®š (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€å„ªå…ˆåº¦ä½) ---
        try:
            action_buttons_group = article.find_elements(
                By.XPATH, ".//div[@role='group' and count(.//button[@data-testid]) > 0]"
            )
            if action_buttons_group:
                buttons_in_group = action_buttons_group[0].find_elements(
                    By.XPATH, ".//button[@data-testid]"
                )
                # print(f"DEBUG is_reply_structure: ID {tweet_id}, ãƒœã‚¿ãƒ³æ•°: {len(buttons_in_group)}")
                if 0 < len(buttons_in_group) <= 3:
                    try:
                        action_buttons_group[0].find_element(
                            By.XPATH,
                            "ancestor::div[@role='link' and .//article[@data-testid='tweet']]",
                        )
                    except NoSuchElementException:
                        print(
                            f"ğŸ’¬ is_reply_structure (button_count): ID {tweet_id}, Count: {len(buttons_in_group)} (<=3) â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®šã®å¯èƒ½æ€§"
                        )
                        return True
            # else:
            # print(f"DEBUG is_reply_structure: ID {tweet_id}, ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except StaleElementReferenceException:
            pass
        except Exception as e_button_count:
            print(
                f"âš ï¸ is_reply_structure: ãƒœã‚¿ãƒ³æ•°ç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_button_count).__name__}: {e_button_count}"
            )

        # --- ä¸Šè¨˜ã®ã„ãšã‚Œã®æ¡ä»¶ã«ã‚‚è©²å½“ã—ãªã„å ´åˆã¯è¦ªæŠ•ç¨¿ã¨ã¿ãªã™ ---
        print(
            f"âœ… is_reply_structure: æ§‹é€ ä¸Šå•é¡Œãªã—ï¼ˆéå¼•ç”¨RTã€éãƒªãƒ—ãƒ©ã‚¤ï¼‰â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤å®š {id_display}"
        )
        return False

    except StaleElementReferenceException:
        print(
            f"âš ï¸ is_reply_structure: StaleElementReferenceExceptionç™ºç”Ÿ {id_display} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False
    except Exception as e:
        print(
            f"âš ï¸ is_reply_structure: åˆ¤å®šã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e).__name__}: {e}\n{traceback.format_exc()} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False


def has_media_in_html(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    # ç”»åƒåˆ¤å®š
    if soup.find("img", {"src": lambda x: x and "twimg.com/media" in x}):
        return True
    # å‹•ç”»åˆ¤å®š
    if soup.find("div", {"data-testid": "video-player-mini-ui-"}):
        return True
    if soup.find("button", {"aria-label": "å‹•ç”»ã‚’å†ç”Ÿ"}):
        return True
    if soup.find("video"):
        return True
    return False


def extract_tweets(driver, extract_target, max_tweets):
    print(f"\nâœ¨ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: https://twitter.com/{extract_target}")
    driver.get(f"https://twitter.com/{extract_target}")
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, "//article"))
    )

    tweet_urls = []
    seen_urls = set()
    scroll_count = 0
    max_scrolls = 20  # ä¾‹ãˆã°20å›ã‚’æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã¨ã—ã¦è¨­å®š
    # ... (rest of the variables) ...
    pause_counter = 0
    pause_threshold = 3
    last_seen_count = 0

    while scroll_count < max_scrolls and len(tweet_urls) < max_tweets:
        print(f"\nğŸ” ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {scroll_count + 1}/{max_scrolls} å›ç›®")
        current_articles_in_dom = driver.find_elements(
            By.XPATH, "//article[@data-testid='tweet']"
        )
        print(f"ğŸ“„ ç¾åœ¨ã®articleæ•°: {len(current_articles_in_dom)}")

        new_tweets_found_in_scroll = 0
        for i, article in enumerate(current_articles_in_dom):
            try:
                # ... (URL and tweet_id extraction logic remains the same) ...
                href_els = article.find_elements(
                    By.XPATH,
                    ".//a[contains(@href, '/status/') and not(ancestor::div[contains(@style, 'display: none')])]",
                )
                if not href_els:
                    continue

                tweet_url = ""
                for href_el in href_els:
                    href_attr = href_el.get_attribute("href")
                    if href_attr and "/status/" in href_attr:
                        if f"/{extract_target}/status/" in href_attr:
                            tweet_url = (
                                href_attr
                                if href_attr.startswith("http")
                                else f"https://x.com{href_attr}"
                            )
                            break
                if not tweet_url:
                    first_href = href_els[0].get_attribute("href")
                    tweet_url = (
                        first_href
                        if first_href.startswith("http")
                        else f"https://x.com{first_href}"
                    )

                tweet_id_match = re.search(r"/status/(\d+)", tweet_url)
                if not tweet_id_match:
                    print(f"âš ï¸ URLã‹ã‚‰tweet_idæŠ½å‡ºå¤±æ•—: {tweet_url}")
                    continue
                tweet_id = tweet_id_match.group(1)

                if tweet_url in seen_urls:
                    continue

                username_in_url_match = re.search(r"x\.com/([^/]+)/status", tweet_url)
                if (
                    not username_in_url_match
                    or username_in_url_match.group(1).lower() != extract_target.lower()
                ):
                    continue

                text_el = None
                try:
                    text_el = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                except:
                    pass
                text = normalize_text(text_el.text) if text_el and text_el.text else ""

                # ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã®æŠ½å‡ºï¼ˆis_reply_structureã«æ¸¡ã™ãŸã‚ï¼‰
                # ç¾åœ¨ã® 'article' ã«ç›´æ¥å±ã™ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
                images_for_check = []
                image_elements = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                )
                for img_el in image_elements:
                    # ã“ã®img_elãŒç¾åœ¨ã®articleç›´ä¸‹ï¼ˆã¾ãŸã¯ãã®tweetPhotoå†…ï¼‰ã«ã‚ã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸå¼•ç”¨RTå†…ã®ã‚‚ã®ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    # æœ€ã‚‚è¿‘ã„ç¥–å…ˆã®articleãŒç¾åœ¨ã®articleè‡ªèº«ã§ã‚ã‚‹ã‹ã§åˆ¤å®š
                    try:
                        closest_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            src = img_el.get_attribute("src")
                            if src:
                                images_for_check.append(src)
                    except StaleElementReferenceException:  # è¦ç´ ãŒæ¶ˆãˆãŸå ´åˆ
                        print(f"âš ï¸ ç”»åƒè¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                        pass

                videos_for_check = []
                video_elements = article.find_elements(
                    By.XPATH, ".//div[@data-testid='videoPlayer']//video[@poster]"
                )
                for video_el in video_elements:
                    try:
                        closest_ancestor_article = video_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            poster = video_el.get_attribute("poster")
                            if poster:
                                videos_for_check.append(poster)
                    except StaleElementReferenceException:
                        print(f"âš ï¸ å‹•ç”»è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:
                        pass

                # has_media ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ is_reply_structure ã‹ã‚‰å‰Šé™¤ã—ãŸã®ã§ã€ã“ã“ã§ã‚‚æ¸¡ã•ãªã„
                if is_reply_structure(
                    article,
                    tweet_id=tweet_id,
                    text=text,
                    # has_media=has_media_for_check, # å‰Šé™¤
                    image_urls=images_for_check,
                    video_poster_urls=videos_for_check,
                ):
                    continue

                if is_ad_post(text):
                    continue

                tweet_urls.append({"url": tweet_url, "id": tweet_id})
                seen_urls.add(tweet_url)
                new_tweets_found_in_scroll += 1

                print(f"âœ… åé›†å€™è£œã«è¿½åŠ : {tweet_url} ({len(tweet_urls)}ä»¶ç›®)")
                if len(tweet_urls) >= max_tweets:
                    break

            except StaleElementReferenceException:
                print(
                    "âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿã€‚DOMãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚"
                )
                break
            except Exception as e:
                print(
                    f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e} (URL: {tweet_url if 'tweet_url' in locals() else 'ä¸æ˜'})"
                )
                continue

        # ... (rest of the scroll and loop logic) ...
        if len(tweet_urls) >= max_tweets:
            print(
                f"ğŸ¯ åé›†å€™è£œæ•°ãŒMAX_TWEETS ({max_tweets}) ã«é”ã—ãŸãŸã‚ã€URLåé›†ã‚’çµ‚äº†ã€‚"
            )
            break

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2.5)
        scroll_count += 1

        if new_tweets_found_in_scroll == 0:
            pause_counter += 1
            print(
                f"ğŸ§Š ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§æ–°è¦æŠ•ç¨¿ãªã— â†’ pause_counter={pause_counter}/{pause_threshold}"
            )
            if pause_counter >= pause_threshold:
                print("ğŸ›‘ æ–°ã—ã„æŠ•ç¨¿ãŒé€£ç¶šã—ã¦æ¤œå‡ºã•ã‚Œãªã„ãŸã‚URLåé›†ã‚’ä¸­æ–­")
                break
        else:
            pause_counter = 0

    print(f"\nğŸ“ˆ åé›†å€™è£œã®URLå–å¾—å®Œäº† â†’ åˆè¨ˆ: {len(tweet_urls)} ä»¶")
    return tweet_urls


def already_registered(tweet_id):
    if not tweet_id or not tweet_id.isdigit():
        return False
    query = {"filter": {"property": "æŠ•ç¨¿ID", "rich_text": {"equals": tweet_id}}}
    try:
        result = notion.databases.query(database_id=DATABASE_ID, **query)
        return len(result.get("results", [])) > 0
    except Exception as e:
        print(f"âš ï¸ Notionã‚¯ã‚¨ãƒªå¤±æ•—: {e}")
        return False


def ocr_and_remove_image(image_path, label=None):
    """
    ç”»åƒãƒ‘ã‚¹ã‚’å—ã‘å–ã‚ŠOCRã—ã€ä½¿ç”¨å¾Œã«å‰Šé™¤ã™ã‚‹ã€‚
    labelãŒã‚ã‚Œã°çµæœã®å…ˆé ­ã«ä»˜ä¸ã€‚
    """
    result = ""
    try:
        ocr_result = ocr_image(image_path)
        if ocr_result:
            cleaned = clean_ocr_text(ocr_result)
            result = f"[{label}]\n{cleaned}" if label else cleaned
    except Exception as e:
        print(f"âš ï¸ OCRå¤±æ•—: {e}")
    finally:
        try:
            os.remove(image_path)
            print(f"ğŸ—‘ï¸ ç”»åƒå‰Šé™¤: {image_path}")
        except Exception as e:
            print(f"âš ï¸ ç”»åƒå‰Šé™¤å¤±æ•—: {e}")
    return result


def clean_ocr_text(text):
    # é™¤å¤–ã—ãŸã„æ–‡è¨€ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã“ã“ã«è¿½åŠ 
    EXCLUDE_PATTERNS = [
        "æœè³ªå•ã‚’ã€Œã„ã„ã­!ã€ ã™ã‚‹",
        "ã“ã®æŠ•ç¨¿ã‚’ã„ã„ã­ï¼",
        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    ]
    lines = text.splitlines()
    cleaned = [
        line for line in lines if not any(pat in line for pat in EXCLUDE_PATTERNS)
    ]
    return "\n".join(cleaned)


def upload_to_notion(tweet):
    print(f"ğŸ“¤ Notionç™»éŒ²å‡¦ç†é–‹å§‹: {tweet['id']}")
    print(f"ğŸ–¼ï¸ images: {tweet.get('images')}")

    if already_registered(tweet["id"]):
        print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ—æ¸ˆ: {tweet['id']}")
        return

    props = {
        "æŠ•ç¨¿ID": {
            "rich_text": [{"type": "text", "text": {"content": str(tweet["id"])}}]
        },
        "æœ¬æ–‡": {
            "rich_text": [
                {
                    "type": "text",
                    "text": {
                        "content": tweet["text"],
                        "link": None,
                    },
                    "annotations": {
                        "bold": False,
                        "italic": False,
                        "strikethrough": False,
                        "underline": False,
                        "code": False,
                        "color": "default",
                    },
                }
            ]
        },
        "URL": {"url": tweet["url"]},
        "æŠ•ç¨¿æ—¥æ™‚": {"date": {"start": tweet["date"]} if tweet["date"] else None},
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": {"select": {"name": "æœªå›ç­”"}},
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°": {
            "number": (
                int(tweet["impressions"])
                if tweet.get("impressions") is not None
                else None
            )
        },
        "ãƒªãƒã‚¹ãƒˆæ•°": {
            "number": int(tweet["retweets"]) if tweet.get("retweets") is not None else 0
        },
        "ã„ã„ã­æ•°": {
            "number": int(tweet["likes"]) if tweet.get("likes") is not None else 0
        },
        "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°": {
            "number": (
                int(tweet["bookmarks"]) if tweet.get("bookmarks") is not None else 0
            )
        },
        "ãƒªãƒ—ãƒ©ã‚¤æ•°": {
            "number": int(tweet["replies"]) if tweet.get("replies") is not None else 0
        },
        "æ–‡å­—èµ·ã“ã—": {"rich_text": []},
    }

    ocr_texts = []

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®OCRï¼ˆtweet["images"]ï¼‰
    for idx, img_url in enumerate(tweet.get("images", [])):
        img_path = f"ocr_image_{tweet['id']}_{idx}.jpg"
        try:
            resp = requests.get(img_url, stream=True)
            with open(img_path, "wb") as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            ocr_text = ocr_and_remove_image(img_path, label=f"ç”»åƒ{idx+1}")
            if ocr_text:
                ocr_texts.append(ocr_text)
        except Exception as e:
            print(f"âš ï¸ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

    # posterç”»åƒã®OCR
    poster_paths = tweet.get("video_posters") or []
    if isinstance(poster_paths, str):
        poster_paths = [poster_paths]
    for idx, poster_path in enumerate(poster_paths):
        ocr_text = ocr_and_remove_image(poster_path, label=f"å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«{idx+1}")
        if ocr_text:
            ocr_texts.append(ocr_text)

    if ocr_texts:
        props["æ–‡å­—èµ·ã“ã—"]["rich_text"] = [
            {"type": "text", "text": {"content": "\n\n".join(ocr_texts)}}
        ]

    children_blocks = []

    try:
        new_page = notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties=props,
            children=children_blocks,
        )
        print(f"ğŸ“ Notionç™»éŒ²å®Œäº†: {tweet['url']}")
    except Exception as e:
        print(f"âŒ Notionç™»éŒ²å¤±æ•—: {tweet['id']} ã‚¨ãƒ©ãƒ¼: {e}")


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def is_recruit_account(name, bio, config):
    return any(
        k in name or k in bio for k in config.get("filter_keywords_name_bio", [])
    )


def is_recruit_post(text, config):
    return any(k in text for k in config.get("filter_keywords_tweet", []))


def search_accounts(driver, keyword_list):
    results = []
    for keyword in keyword_list:
        search_url = f"https://twitter.com/search?q={keyword}&f=user"
        driver.get(search_url)
        time.sleep(3)

        # âš  æ–°UIæ§‹é€ ã«å¯¾å¿œ
        users = driver.find_elements(
            By.XPATH, "//a[contains(@href, '/')]//div[@dir='auto']/../../.."
        )
        print(f"ğŸ” å€™è£œãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¶æ•°: {len(users)}")

        for user in users:
            try:
                spans = user.find_elements(By.XPATH, ".//span")
                name = ""
                username = ""

                for span in spans:
                    text = span.text.strip()
                    if text.startswith("@"):
                        username = text.replace("@", "")
                    elif not name:
                        name = text

                if username and name:
                    results.append(
                        {
                            "name": name,
                            "username": username,
                            "bio": "",  # ã“ã®æ®µéšã§ã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»é¢ã«é£›ã‚“ã§ã„ãªã„
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºå¤±æ•—: {e}")
                continue

    return results


def merge_replies_with_driver(driver, tweet):
    try:
        driver.get(tweet["url"])
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )

        replies = extract_self_replies(driver, tweet.get("username", ""))
        if not isinstance(replies, list):
            print(
                f"âš ï¸ merge_replies_with_driver() ã§å–å¾—ã—ãŸrepliesãŒä¸æ­£ãªå‹: {type(replies)} â†’ ç©ºãƒªã‚¹ãƒˆã«ç½®æ›"
            )
            replies = []

        tweet_text = tweet.get("text") or ""
        replies = sorted(
            [r for r in replies if r.get("id") and r.get("text")],
            key=lambda x: int(x["id"]),
        )

        existing_chunks = set(tweet_text.strip().split("\n\n"))
        reply_texts = []

        for r in replies:
            # ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ãªã„
            if r.get("images") or r.get("video_posters"):
                print(
                    f"ğŸ›‘ ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ã¾ã›ã‚“: {r['id']}"
                )
                continue

            reply_id = r["id"]
            reply_body = r["text"].strip()
            clean_body = reply_body[:20].replace("\n", " ")
            print(f"ğŸ§µ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå€™è£œ: ID={reply_id} | textå…ˆé ­: {clean_body}")

            if not reply_body:
                continue
            if reply_body in existing_chunks:
                continue
            if r["id"] == tweet["id"]:
                continue

            reply_texts.append(reply_body)
            existing_chunks.add(reply_body)

        if reply_texts:
            tweet["text"] = tweet_text + "\n\n" + "\n\n".join(reply_texts)

    except Exception as e:
        print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå¤±æ•—ï¼ˆ{tweet.get('url', 'ä¸æ˜URL')}ï¼‰: {e}")
    return tweet


def extract_from_search(driver, keywords, max_tweets, name_bio_keywords=None):
    tweets = []
    seen_urls = set()
    seen_users = set()

    for keyword in keywords:
        print(f"ğŸ” è©±é¡Œã®ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ä¸­: {keyword}")
        search_url = f"https://twitter.com/search?q={keyword}&src=typed_query&f=top"
        driver.get(search_url)
        time.sleep(3)

        scroll_count = 0
        max_scrolls = 10
        pause_counter = 0
        pause_threshold = 3
        last_article_count = 0

        while len(tweets) < max_tweets and scroll_count < max_scrolls:
            articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            article_count = len(articles)
            print(f"ğŸ“„ è¡¨ç¤ºä¸­ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {article_count}")
            for article in articles:
                try:
                    # ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åå–å¾—
                    time_el = article.find_element(By.XPATH, ".//time")
                    tweet_href = time_el.find_element(By.XPATH, "..").get_attribute(
                        "href"
                    )
                    tweet_url = (
                        tweet_href
                        if tweet_href.startswith("http")
                        else f"https://x.com{tweet_href}"
                    )
                    if tweet_url in seen_urls:
                        continue
                    seen_urls.add(tweet_url)

                    name_block = article.find_element(
                        By.XPATH, ".//div[@data-testid='User-Name']"
                    )
                    spans = name_block.find_elements(By.XPATH, ".//span")
                    display_name = ""
                    username = ""
                    for s in spans:
                        text = s.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not display_name:
                            display_name = text

                    if not username or username in seen_users:
                        continue
                    seen_users.add(username)

                    # bioãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸å…ˆã«ã‚¢ã‚¯ã‚»ã‚¹
                    if name_bio_keywords:
                        driver.execute_script("window.open('');")
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.get(f"https://twitter.com/{username}")
                        time.sleep(2)
                        try:
                            bio_text = (
                                WebDriverWait(driver, 5)
                                .until(
                                    EC.presence_of_element_located(
                                        (
                                            By.XPATH,
                                            "//div[@data-testid='UserDescription']",
                                        )
                                    )
                                )
                                .text
                            )
                        except:
                            bio_text = ""
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])

                        if not any(
                            k in display_name for k in name_bio_keywords
                        ) and not any(k in bio_text for k in name_bio_keywords):
                            print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿éä¸€è‡´ â†’ ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                            continue

                    # âœ… æ¡ä»¶ã‚’é€šéã—ãŸå ´åˆã®ã¿æŠ•ç¨¿è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦æŠ½å‡º
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(tweet_url)
                    WebDriverWait(driver, 10).until(EC.url_contains("/status/"))

                    try:
                        full_text_el = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located(
                                (By.XPATH, "//div[@data-testid='tweetText']")
                            )
                        )
                        text = full_text_el.text.strip()
                    except Exception as e:
                        print(f"âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")
                        text = ""

                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆå®‰å®šåŒ– + ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« + ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–ï¼‰
                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸å†…ã€ã‚¨ãƒ©ãƒ¼å›é¿ãƒ»å¤šæ®µæ§‹é€ ã«å¯¾å¿œï¼‰
                    date = ""
                    for attempt in range(5):
                        try:
                            driver.execute_script("window.scrollTo(0, 0);")
                            WebDriverWait(driver, 3).until(
                                EC.presence_of_element_located((By.XPATH, "//article"))
                            )
                            time_el = driver.find_element(By.XPATH, "//article//a/time")
                            if time_el:
                                date = time_el.get_attribute("datetime")
                                break
                        except Exception as e:
                            print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—è©¦è¡Œ {attempt+1}/5 å¤±æ•—: {e}")
                            time.sleep(1)

                    if not date:
                        print("âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—ã«å¤±æ•— â†’ ç©ºæ–‡å­—ã§ç¶™ç¶š")

                    # è‡ªãƒªãƒ—ãƒ©ã‚¤å–å¾—ï¼ˆçœç•¥å¯ï¼‰
                    replies = extract_self_replies(driver, username)
                    if replies:
                        reply_texts = [
                            r["text"]
                            for r in replies
                            if "text" in r and r["text"] not in text
                        ]
                        if reply_texts:
                            text += "\n\n" + "\n\n".join(reply_texts)

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])
                    if already_registered(tweet_id):
                        print(f"ğŸš« ç™»éŒ²æ¸ˆ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                        continue

                    tweets.append(
                        {
                            "url": tweet_url,
                            "text": text,
                            "date": date,
                            "id": tweet_id,
                            "username": username,
                            "display_name": display_name,
                        }
                    )

                    print(f"âœ… åé›†: {tweet_url} @{username}")
                    if len(tweets) >= max_tweets:
                        break

                except Exception as e:
                    print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # èª­ã¿è¾¼ã¿åˆ¤å®š
            if article_count == last_article_count:
                pause_counter += 1
                print("ğŸ§Š ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«æ–°ã—ã„æŠ•ç¨¿ãªã—")
                if pause_counter >= pause_threshold:
                    print("ğŸ›‘ æŠ•ç¨¿ãŒå¢—ãˆãªã„ãŸã‚ä¸­æ–­")
                    break
            else:
                pause_counter = 0

            last_article_count = article_count
            scroll_count += 1

    return tweets


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument(
        "--account", default="accounts.json", help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰"
    )
    args = parser.parse_args()

    config = load_config(args.config)
    account = load_config(args.account)

    global NOTION_TOKEN, DATABASE_ID, notion
    global TWITTER_EMAIL, TWITTER_USERNAME, TWITTER_PASSWORD
    global EXTRACT_TARGET, MAX_TWEETS

    NOTION_TOKEN = config["notion_token"]
    DATABASE_ID = config["database_id"]
    EXTRACT_TARGET = config["extract_target"]
    MAX_TWEETS = config["max_tweets"]

    TWITTER_EMAIL = account["email"]
    TWITTER_USERNAME = account["username"]
    TWITTER_PASSWORD = account["password"]

    notion = Client(auth=NOTION_TOKEN)
    driver = setup_driver()

    login(driver, EXTRACT_TARGET if config["mode"] == "target_only" else None)

    tweets_for_notion_upload = []

    if config["mode"] == "target_only":
        print(
            f"ğŸ¯ mode: target_only â†’ extract_target = {EXTRACT_TARGET} ã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™"
        )
        URL_BUFFER_FACTOR = 3
        tweet_url_dicts = extract_tweets(
            driver, EXTRACT_TARGET, MAX_TWEETS * URL_BUFFER_FACTOR
        )
        # extract_tweets ã¯ {"url": url, "id": id} ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_and_merge_tweets(
            driver, tweet_url_dicts, MAX_TWEETS
        )

    elif config["mode"] == "search_filtered":
        print(
            "ğŸ” mode: search_filtered â†’ æ¤œç´¢ + name/bio + tweetãƒ•ã‚£ãƒ«ã‚¿ã‚’ã‹ã‘ã¦æŠ•ç¨¿ã‚’åé›†ã—ã¾ã™"
        )
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        users = search_accounts(driver, config["filter_keywords_name_bio"])
        collected_tweets_count = 0
        for user_info in users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            if is_recruit_account(
                user_info["name"], user_info["bio"], config
            ):  # bioã¯search_accountså†…ã§å–å¾—ãƒ»è¨­å®šãŒå¿…è¦
                # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                user_tweet_urls = extract_tweets(
                    driver, user_info["username"], MAX_TWEETS - collected_tweets_count
                )
                # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                processed_user_tweets = extract_and_merge_tweets(
                    driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
                )

                # ã•ã‚‰ã«æŠ•ç¨¿å†…å®¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                for tweet_data in processed_user_tweets:
                    if is_recruit_post(tweet_data["text"], config):
                        tweets_for_notion_upload.append(tweet_data)
                        collected_tweets_count += 1
                        if collected_tweets_count >= MAX_TWEETS:
                            break

    elif config["mode"] == "search_all":
        print("ğŸŒ mode: search_all â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ â†’ bioãƒ•ã‚£ãƒ«ã‚¿ â†’ å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿å–å¾—")
        collected_tweets_count = 0

        # filter_keywords_name_bio ã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™æƒ³å®šã ãŒã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«æ¤œç´¢ã™ã‚‹å½¢ã«å¤‰æ›´

        all_potential_users = []
        for keyword in config.get(
            "filter_keywords_name_bio", []
        ):  # name_bio ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢
            print(f"ğŸ‘¤ '{keyword}' ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ä¸­...")
            # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            # search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã‹ã€ãƒ«ãƒ¼ãƒ—å†…ã§å‘¼ã³å‡ºã™
            # ã“ã“ã§ã¯ search_accounts ãŒå˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´ã—ãŸã¨ä»®å®š
            # ã¾ãŸã¯ã€search_accounts ã‚’ filter_keywords_name_bio å…¨ä½“ã§å®Ÿè¡Œã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’å¾—ã‚‹

            # ä»®: search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒãƒƒãƒã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¿”ã™
            # users_from_keyword_search = search_accounts(driver, [keyword]) # search_accountsã®ä»•æ§˜ã«åˆã‚ã›ã‚‹
            # all_potential_users.extend(users_from_keyword_search)

            # ç¾çŠ¶ã® search_accounts ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã®ã§ã€ãã‚Œã§è‰¯ã„
            pass  # search_accounts ã¯å¾Œã§ã¾ã¨ã‚ã¦å‘¼ã³å‡ºã™ã‹ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‡¦ç†

        # search_accounts ã¯ filter_keywords_name_bio ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã¨ä»®å®š
        # (ã¾ãŸã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‘¼ã³å‡ºã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹)
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {config.get('filter_keywords_name_bio')}")
        candidate_users = search_accounts(
            driver, config.get("filter_keywords_name_bio", [])
        )

        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—ã—ã€bioã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_users = []
        for user_data in candidate_users:
            # search_accountså†…ã§bioã‚’å–å¾—ãƒ»è¨­å®šã™ã‚‹ã‹ã€ã“ã“ã§ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦bioå–å¾—
            # ã“ã“ã§ã¯ search_accounts ãŒbioã‚‚å–å¾—ã—ã¦è¿”ã™ã¨ä»®å®šï¼ˆã¾ãŸã¯åˆ¥é€”é–¢æ•°å‘¼ã³å‡ºã—ï¼‰
            # ä»®ã«ã“ã“ã§bioã‚’å–å¾—ã™ã‚‹ãªã‚‰ï¼š
            # driver.get(f"https://twitter.com/{user_data['username']}")
            # time.sleep(2)
            # try:
            #     bio_el = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, "//div[@data-testid='UserDescription']")))
            #     user_data['bio'] = bio_el.text if bio_el else ""
            # except:
            #     user_data['bio'] = ""

            # is_recruit_account ã¯ name ã¨ bio ã§åˆ¤å®š
            if is_recruit_account(
                user_data.get("name", ""), user_data.get("bio", ""), config
            ):
                filtered_users.append(user_data)

        print(f"ğŸ‘¤ bioãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(filtered_users)}")

        for user_info in filtered_users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            print(f"ğŸ¦ @{user_info['username']} ã®æŠ•ç¨¿ã‚’åé›†é–‹å§‹")
            # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            user_tweet_urls = extract_tweets(
                driver, user_info["username"], MAX_TWEETS - collected_tweets_count
            )
            # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            processed_user_tweets = extract_and_merge_tweets(
                driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
            )

            # search_all ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€æŠ•ç¨¿å†…å®¹ã®ãƒ•ã‚£ãƒ«ã‚¿ã¯é€šå¸¸ã‹ã‘ãªã„ãŒã€ã‚‚ã—å¿…è¦ãªã‚‰ã“ã“ã§ is_recruit_post ã‚’ä½¿ã†
            # ä»Šå›ã¯bioãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨æŠ•ç¨¿(MAX_TWEETSã¾ã§)ã‚’å–å¾—ã™ã‚‹ã¨è§£é‡ˆ
            tweets_for_notion_upload.extend(processed_user_tweets)
            collected_tweets_count += len(processed_user_tweets)

    elif config["mode"] == "keyword_trend":
        print("ğŸ”¥ mode: keyword_trend â†’ æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©±é¡ŒæŠ•ç¨¿ã‚’åé›†ã—ã¾ã™")
        # extract_from_search ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_from_search(
            driver,
            config["filter_keywords_tweet"],  # æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            MAX_TWEETS,
            config.get(
                "filter_keywords_name_bio"
            ),  # å–å¾—ã—ãŸæŠ•ç¨¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã•ã‚‰ã«çµã‚‹å ´åˆ
        )

    else:
        raise ValueError(f"âŒ æœªçŸ¥ã®modeæŒ‡å®šã§ã™: {config['mode']}")

    print(f"\nğŸ“Š Notionç™»éŒ²å¯¾è±¡ã®åˆè¨ˆãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(tweets_for_notion_upload)} ä»¶")

    # æŠ•ç¨¿IDæ˜‡é †ã§ä¸¦ã¹æ›¿ãˆã¦ã‹ã‚‰ç™»éŒ²ï¼ˆé †ç•ªä¿è¨¼ï¼‰
    # id ãŒæ•°å€¤ã§ãªã„å ´åˆã‚„å­˜åœ¨ã—ãªã„å ´åˆã‚’è€ƒæ…®
    tweets_for_notion_upload.sort(
        key=lambda x: (
            int(x["id"]) if x.get("id") and x["id"].isdigit() else float("inf")
        )
    )

    for i, tweet_data in enumerate(tweets_for_notion_upload, 1):
        print(f"\nğŸŒ€ {i}/{len(tweets_for_notion_upload)} ä»¶ç›® Notionç™»éŒ²å‡¦ç†ä¸­...")

        # upload_to_notion ã«æ¸¡ã™å‰ã«ã€ä¸è¦ãªã‚­ãƒ¼ã‚„WebElementãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
        tweet_data_for_upload = tweet_data.copy()
        tweet_data_for_upload.pop(
            "article_element", None
        )  # extract_thread_from_detail_page ãŒæ®‹ã™å¯èƒ½æ€§
        tweet_data_for_upload.pop("article", None)  # å¤ã„å½¢å¼ã®ã‚­ãƒ¼ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆ

        # print(json.dumps(tweet_data_for_upload, ensure_ascii=False, indent=2)) # ãƒ‡ãƒãƒƒã‚°ç”¨

        # ãƒªãƒ—ãƒ©ã‚¤ãƒãƒ¼ã‚¸ã¯ extract_and_merge_tweets ã§å®Ÿæ–½æ¸ˆã¿ã®ãŸã‚ã€ã“ã“ã§ã¯å‘¼ã³å‡ºã•ãªã„
        upload_to_notion(tweet_data_for_upload)

    driver.quit()
    print("âœ… å…¨æŠ•ç¨¿ã®å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
