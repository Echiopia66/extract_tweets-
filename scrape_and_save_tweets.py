import os
import re
import cv2
import time
import json
import argparse
import traceback
import requests
import pytesseract
from PIL import Image, ImageFilter, ImageEnhance
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import (
    StaleElementReferenceException,
    NoSuchElementException,
)
from notion_client import Client
from datetime import datetime
import shutil

# âœ… åºƒå‘Šé™¤å¤–ã€RT/å¼•ç”¨RTãƒ«ãƒ¼ãƒ«ã€æŠ•ç¨¿IDè£œå®Œä»˜ã
AD_KEYWORDS = [
    "r10.to",
    "ãµã‚‹ã•ã¨ç´ç¨",
    "ã‚«ãƒ¼ãƒ‰ãƒ­ãƒ¼ãƒ³",
    "ãŠé‡‘å€Ÿã‚Šã‚‰ã‚Œã‚‹",
    "ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ã‚¬ãƒ¬ãƒ¼ã‚¸",
    "UNEXT",
    "ã‚¨ã‚³ã‚ªã‚¯",
    "#PR",
    "æ¥½å¤©",
    "Amazon",
    "A8",
    "ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    "å‰¯æ¥­",
    "bit.ly",
    "shp.ee",
    "t.co/",
]


def normalize_text(text):
    return text.strip()


def login(driver, target=None):
    if os.path.exists("twitter_cookies.json"):
        print("âœ… Cookieã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º â†’ ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—")
        print("ğŸŒ https://twitter.com ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã‚¯ãƒƒã‚­ãƒ¼èª­ã¿è¾¼ã¿ä¸­â€¦")
        driver.get("https://twitter.com/")
        driver.delete_all_cookies()
        with open("twitter_cookies.json", "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                driver.add_cookie(cookie)
        driver.get(f"https://twitter.com/{target or TWITTER_USERNAME}")
        return

    print("ğŸ” åˆå›ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ã‚’é–‹å§‹")
    driver.get("https://twitter.com/i/flow/login")
    email_input = WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.NAME, "text"))
    )
    email_input.send_keys(TWITTER_EMAIL)
    email_input.send_keys(Keys.ENTER)
    time.sleep(2)

    try:
        username_input = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.NAME, "text"))
        )
        username_input.send_keys(TWITTER_USERNAME)
        username_input.send_keys(Keys.ENTER)
        time.sleep(2)
    except Exception:
        print("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼åå…¥åŠ›ã‚¹ã‚­ãƒƒãƒ—")

    password_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.NAME, "password"))
    )
    password_input.send_keys(TWITTER_PASSWORD)
    password_input.send_keys(Keys.ENTER)
    time.sleep(6)

    cookies = driver.get_cookies()
    with open("twitter_cookies.json", "w") as f:
        json.dump(cookies, f)
    print("âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ â†’ æŠ•ç¨¿è€…ãƒšãƒ¼ã‚¸ã«é·ç§»")
    driver.get(f"https://twitter.com/{EXTRACT_TARGET}")


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  â† ã“ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ja-JP")
    options.add_argument("--disable-blink-features=AutomationControlled")
    return webdriver.Chrome(options=options)


def extract_tweet_id(article):
    href_els = article.find_elements(By.XPATH, ".//a[contains(@href, '/status/')]")
    for el in href_els:
        h = el.get_attribute("href")
        m = re.search(r"/status/(\d+)", h or "")
        if m:
            return m.group(1)
    return None


def ocr_image(image_path):
    try:
        img = Image.open(image_path)
        img = img.convert("L")
        img = img.resize((img.width * 2, img.height * 2))
        img = ImageEnhance.Contrast(img).enhance(2.0)
        img = img.filter(ImageFilter.SHARPEN)
        import numpy as np

        img_np = np.array(img)
        img_np = cv2.medianBlur(img_np, 3)
        _, img_np = cv2.threshold(img_np, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        img = Image.fromarray(img_np)
        text = pytesseract.image_to_string(img, lang="jpn", config="--oem 1 --psm 6")
        print(f"ğŸ“ OCRç”»åƒ({image_path})çµæœ:\n{text.strip()}")
        if not text.strip() or sum(c.isalnum() for c in text) < 3:
            print(f"âš ï¸ OCRç”»åƒ({image_path})ã§æ–‡å­—åŒ–ã‘ã¾ãŸã¯èªè­˜å¤±æ•—ã®å¯èƒ½æ€§")
        return text.strip()
    except Exception as e:
        print(f"OCRå¤±æ•—({image_path}): {e}")
        return "[OCRã‚¨ãƒ©ãƒ¼]"


def extract_self_replies(driver, username):
    replies = []
    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    cell_divs = sorted(cell_divs, key=get_transform_y)

    for cell in cell_divs:
        texts = []
        for tag in ["span", "h2"]:
            for el in cell.find_elements(By.XPATH, f".//{tag}"):
                t = (
                    el.text.strip()
                    .replace("\u200b", "")
                    .replace("\n", "")
                    .replace(" ", "")
                )
                if t:
                    texts.append(t)
        if any("ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹" in t for t in texts):
            print("ğŸ” extract_self_replies: ã‚‚ã£ã¨è¦‹ã¤ã‘ã‚‹ä»¥é™ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’é™¤å¤–")
            break

        articles = cell.find_elements(By.XPATH, ".//article[@data-testid='tweet']")

        def is_quote_reply(article):
            quote_els = article.find_elements(
                By.XPATH,
                ".//*[contains(text(), 'å¼•ç”¨')] | .//*[contains(text(), 'Quote')]",
            )
            quote_struct = article.find_elements(
                By.XPATH, ".//div[contains(@aria-label, 'å¼•ç”¨')]"
            )
            return bool(quote_els or quote_struct)

        for article in articles:
            try:
                handle_el = article.find_element(
                    By.XPATH,
                    ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                )
                handle = handle_el.text.strip()
                if handle.replace("@", "") != username:
                    continue

                if is_quote_reply(article):
                    print("âš ï¸ extract_self_replies: å¼•ç”¨RTå½¢å¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                text_el = article.find_element(
                    By.XPATH, ".//div[@data-testid='tweetText']"
                )
                reply_text = text_el.text.strip() if text_el and text_el.text else ""

                tweet_id = extract_tweet_id(article)
                if not tweet_id:
                    print("âš ï¸ extract_self_replies: tweet_idãŒå–å¾—ã§ããªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                # ç”»åƒãƒ»å‹•ç”»æƒ…å ±ã‚‚å–å¾—
                images = [
                    img.get_attribute("src")
                    for img in article.find_elements(
                        By.XPATH,
                        ".//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                    )
                    if img.get_attribute("src")
                ]
                video_posters = [
                    v.get_attribute("poster")
                    for v in article.find_elements(By.XPATH, ".//video")
                    if v.get_attribute("poster")
                ]

                if reply_text:
                    replies.append(
                        {
                            "id": tweet_id,
                            "text": reply_text,
                            "images": images,
                            "video_posters": video_posters,
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue
    return replies


def is_ad_post(text):
    lowered = text.lower()
    return any(k.lower() in lowered for k in AD_KEYWORDS)


def extract_thread_from_detail_page(driver, tweet_url):
    print(f"\nğŸ•µï¸ æŠ•ç¨¿ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {tweet_url}")
    driver.get(tweet_url)

    # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å¾…ã¡æ™‚é–“ã‚’èª¿æ•´ã—ã€ä¸»è¦ãªè¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…ã¤
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_all_elements_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']//time[@datetime]")
            )
        )
        # print("âœ… è¨˜äº‹ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å­˜åœ¨ã‚’ç¢ºèª")
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿è¨˜äº‹ã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å–å¾—ã«å¤±æ•—: {e}")
        return []

    if (
        "Something went wrong" in driver.page_source
        or "ã“ã®ãƒšãƒ¼ã‚¸ã¯å­˜åœ¨ã—ã¾ã›ã‚“" in driver.page_source
    ):
        print(f"âŒ æŠ•ç¨¿ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {tweet_url}")
        return []

    def get_transform_y(cell):
        style = cell.get_attribute("style") or ""
        m = re.search(r"translateY\(([\d\.]+)px\)", style)
        return float(m.group(1)) if m else 0

    tweet_blocks = []
    current_id_from_url = re.sub(r"\D", "", tweet_url.split("/")[-1])
    # print(f"ğŸ†” current_id_from_url: {current_id_from_url}")

    cell_divs = driver.find_elements(By.XPATH, "//div[@data-testid='cellInnerDiv']")
    print(f"cellInnerDivæ•°: {len(cell_divs)}")
    cell_divs = sorted(cell_divs, key=get_transform_y)

    found_other_user_reply_in_thread = False
    for cell_idx, cell in enumerate(cell_divs):
        if found_other_user_reply_in_thread:
            # print(f"ğŸ›‘ ã‚¹ãƒ¬ãƒƒãƒ‰å†…ã§ä»–äººãƒªãƒ—ãƒ©ã‚¤æ¤œå‡ºæ¸ˆã¿ã®ãŸã‚ã€cell {cell_idx + 1} ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            break

        articles_in_cell = cell.find_elements(
            By.XPATH, ".//article[@data-testid='tweet']"
        )
        if not articles_in_cell:
            continue

        for article_idx, article in enumerate(articles_in_cell):
            if found_other_user_reply_in_thread:
                break

            tweet_id = None
            username = ""
            try:
                # --- Robust Tweet ID Extraction ---
                time_links = article.find_elements(
                    By.XPATH, ".//a[.//time[@datetime] and contains(@href, '/status/')]"
                )
                if time_links:
                    href = time_links[0].get_attribute("href")
                    match = re.search(r"/status/(\d{10,})", href)
                    if match:
                        tweet_id = match.group(1)

                if not tweet_id:  # Fallback if time_link method didn't yield ID
                    all_status_links = article.find_elements(
                        By.XPATH, ".//a[contains(@href, '/status/')]"
                    )
                    if all_status_links:
                        # Iterate to find the first link that is likely the main article's permalink
                        # This is a heuristic: often the first one, or one not deep inside a quote structure
                        # For simplicity, we'll take the first one found if the time_link fails.
                        href = all_status_links[0].get_attribute("href")
                        match = re.search(r"/status/(\d{10,})", href)
                        if match:
                            tweet_id = match.group(1)
                # --- End of Tweet ID Extraction ---

                if not tweet_id:
                    # print(f"DEBUG: Failed to extract tweet_id for article {article_idx} in cell {cell_idx}")
                    continue

                # print(f"DEBUG: Article {article_idx}, Extracted tweet_id: {tweet_id}")

                try:
                    username_el = article.find_element(
                        By.XPATH,
                        ".//div[@data-testid='User-Name']//span[contains(text(), '@')]",
                    )
                    username = username_el.text.replace("@", "").strip()
                except:
                    pass
                # print(f"DEBUG: Article {article_idx}, ID: {tweet_id}, Username: {username}")

                if not username:
                    # print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒå–å¾—ã§ããªã‹ã£ãŸæŠ•ç¨¿ï¼ˆID: {tweet_id}ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                    continue

                if username != EXTRACT_TARGET:
                    if tweet_id != current_id_from_url:
                        # print(f"ğŸ›‘ ä»–äººã®æŠ•ç¨¿ï¼ˆ@{username}ã€ID: {tweet_id}ï¼‰ã‚’æ¤œå‡ºã€‚ä»¥é™ã®å–å¾—ã‚’åœæ­¢ã€‚")
                        found_other_user_reply_in_thread = True
                        break
                        # else:
                        # print(f"ğŸ”¶ èµ·ç‚¹æŠ•ç¨¿({current_id_from_url})ãŒä»–äºº(@{username})ã§ã™ãŒã€ä¸€åº¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚")
                        pass  # Will be checked later by initial_post_data["username"]

                text = ""
                try:
                    # æœ¬æ–‡ã®æŠ½å‡º (ä»¥å‰ã®ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒ­ã‚¸ãƒƒã‚¯ã¯è©³ç´°ãƒšãƒ¼ã‚¸ã«ã¯ä¸è¦ãªãŸã‚å‰Šé™¤)
                    tweet_text_element = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                    # JavaScriptã®innerTextã‚’ä½¿ç”¨ã—ã¦ã€è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¨æ”¹è¡Œã‚’ã‚ˆã‚Šæ­£ç¢ºã«å–å¾—
                    text_content = driver.execute_script(
                        "return arguments[0].innerText;", tweet_text_element
                    )
                    text = text_content.strip() if text_content else ""

                except NoSuchElementException:
                    # print(f"DEBUG: tweetText element not found for ID: {tweet_id}")
                    text = ""  # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—
                except Exception as e_text:
                    print(
                        f"âš ï¸ æœ¬æ–‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼ (ID: {tweet_id}): {type(e_text).__name__} - {e_text}"
                    )
                    text = ""

                images = []
                all_tweet_photo_imgs = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media')]",
                )
                for img_el in all_tweet_photo_imgs:
                    try:
                        # Check if the image belongs to the current article and not a quoted tweet within
                        img_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if (
                            img_ancestor_article != article
                        ):  # If the image's ancestor article is not the current one, skip
                            continue
                        # Further check: if the image is inside a div with role="link", it's likely part of a quote/card
                        img_el.find_element(By.XPATH, "ancestor::div[@role='link']")
                        continue  # Skip if it's inside a linkable quote/card structure
                    except NoSuchElementException:
                        # This means it's NOT inside a role="link" div, so it's a direct image of the current article
                        pass
                    except StaleElementReferenceException:
                        # print(f"âš ï¸ Stale element while checking image ancestor for ID: {tweet_id}")
                        continue
                    except Exception:  # Other exceptions during checks
                        # print(f"âš ï¸ Error checking image ancestor for ID: {tweet_id}")
                        continue

                    src = img_el.get_attribute("src")
                    if src and src not in images:
                        images.append(src)

                # Extract card images (like summary cards)
                all_card_imgs = article.find_elements(
                    By.XPATH, ".//img[contains(@src, 'twimg.com/card_img')]"
                )
                for img_el in all_card_imgs:
                    try:
                        img_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if img_ancestor_article != article:
                            continue
                        # Card images are often within a linkable container, but they are direct media for *this* tweet
                        # So, unlike tweetPhoto, we don't necessarily skip if inside role="link" if it's the main article's card
                        # However, if the card_img is part of a *quoted tweet's card*, we should skip.
                        # This distinction can be hard. For now, if it's a card_img and its ancestor is the current article, take it.
                        # A more robust check might involve ensuring it's not inside a *nested* article's card.
                    except StaleElementReferenceException:
                        # print(f"âš ï¸ Stale element while checking card_img ancestor for ID: {tweet_id}")
                        continue
                    except Exception:
                        # print(f"âš ï¸ Error checking card_img ancestor for ID: {tweet_id}")
                        continue

                    src = img_el.get_attribute("src")
                    if src and src not in images:
                        images.append(src)

                video_posters = []
                video_xpath_candidates = [
                    ".//div[@data-testid='videoPlayer']//video[@poster]",
                    ".//div[@data-testid='videoComponent']//video[@poster]",
                    ".//video[@poster]",
                ]
                all_video_elements = []
                for xpath_candidate in video_xpath_candidates:
                    all_video_elements = article.find_elements(
                        By.XPATH, xpath_candidate
                    )
                    if all_video_elements:
                        break

                for v_el in all_video_elements:
                    try:
                        video_ancestor_article = v_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if video_ancestor_article != article:
                            continue
                        v_el.find_element(By.XPATH, "ancestor::div[@role='link']")
                        continue
                    except NoSuchElementException:
                        pass
                    except StaleElementReferenceException:
                        continue
                    except Exception:
                        continue

                    poster_url = v_el.get_attribute("poster")
                    if poster_url:
                        poster_filename = (
                            f"video_poster_{tweet_id}_{len(video_posters)}.jpg"
                        )
                        temp_poster_dir = "temp_posters"
                        if not os.path.exists(temp_poster_dir):
                            os.makedirs(temp_poster_dir)
                        poster_path = os.path.join(temp_poster_dir, poster_filename)
                        try:
                            resp = requests.get(poster_url, stream=True, timeout=10)
                            with open(poster_path, "wb") as f:
                                for chunk in resp.iter_content(1024):
                                    f.write(chunk)
                            video_posters.append(poster_path)
                        except Exception as e_poster:
                            print(f"âŒ posterç”»åƒä¿å­˜å¤±æ•— (ID: {tweet_id}): {e_poster}")

                time_els = article.find_elements(By.XPATH, ".//time")
                date_str = time_els[0].get_attribute("datetime") if time_els else None

                tweet_blocks.append(
                    {
                        "article_element": article,
                        "text": text,
                        "date": date_str,
                        "id": tweet_id,
                        "username": username,
                        "images": images,
                        "video_posters": video_posters,
                    }
                )
                # print(f"DEBUG: Added to tweet_blocks: id={tweet_id}, user={username}, images: {len(images)}, posters: {len(video_posters)}")

            except StaleElementReferenceException:
                # print(f"âš ï¸ StaleElementReferenceException in article loop. ID: {tweet_id if tweet_id else 'ä¸æ˜'}")
                break  # Break from inner articles_in_cell loop for this cell
            except Exception as e:
                # print(f"âš ï¸ articleè§£æã‚¨ãƒ©ãƒ¼ (outer): {type(e).__name__} - {str(e)} (ID: {tweet_id if tweet_id else 'ä¸æ˜'})")
                continue  # Continue to next article in cell

        if found_other_user_reply_in_thread:
            break  # Break from outer cell_divs loop

    def remove_temp_posters_from_list(blocks_to_clean):
        for block in blocks_to_clean:
            for poster_p in block.get("video_posters", []):
                if os.path.exists(poster_p):
                    try:
                        os.remove(poster_p)
                    except Exception as e_del:
                        print(
                            f"âš ï¸ ä¸€æ™‚ãƒã‚¹ã‚¿ãƒ¼å‰Šé™¤å¤±æ•— (ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—): {poster_p}, {e_del}"
                        )

    if not tweet_blocks:
        print(f"âš ï¸ æœ‰åŠ¹ãªæŠ•ç¨¿ãƒ–ãƒ­ãƒƒã‚¯ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ (URL: {tweet_url})")
        return []

    initial_post_data = None
    # print(f"DEBUG: Searching for initial_post_data with id={current_id_from_url} in {len(tweet_blocks)} blocks.")
    for block in tweet_blocks:
        # print(f"DEBUG: Checking block for initial_post_data: id={block['id']}, username={block['username']}")
        if block["id"] == current_id_from_url:
            initial_post_data = block
            # print(f"DEBUG: Found initial_post_data: id={initial_post_data['id']}")
            break

    if not initial_post_data:
        print(
            f"âš ï¸ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ãŒæŠ½å‡ºã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªãƒ–ãƒ­ãƒƒã‚¯ID: {[b['id'] for b in tweet_blocks if 'id' in b]}"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    if initial_post_data["username"] != EXTRACT_TARGET:
        print(
            f"ğŸ›‘ URLæŒ‡å®šã®æŠ•ç¨¿({current_id_from_url})ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼(@{initial_post_data['username']})ãŒå¯¾è±¡({EXTRACT_TARGET})ã¨ç•°ãªã‚Šã¾ã™ã€‚ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¯ç„¡åŠ¹ã§ã™ã€‚"
        )
        remove_temp_posters_from_list(tweet_blocks)
        return []

    final_results = []
    for block_item in tweet_blocks:
        if "article_element" not in block_item:  # Ensure essential key exists
            remove_temp_posters_from_list([block_item])
            continue

        if block_item["username"] != EXTRACT_TARGET:
            remove_temp_posters_from_list([block_item])  # Clean up posters if skipping
            continue

        if is_ad_post(block_item["text"]):
            print(f"ğŸš« åºƒå‘ŠæŠ•ç¨¿ï¼ˆID: {block_item['id']}ï¼‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            remove_temp_posters_from_list([block_item])
            continue

        impressions, retweets, likes, bookmarks, replies_count = extract_metrics(
            block_item["article_element"]
        )

        final_results.append(
            {
                "url": f"https://x.com/{block_item['username']}/status/{block_item['id']}",
                "id": block_item["id"],
                "text": block_item["text"],
                "date": block_item["date"],
                "images": block_item["images"],  # Should be direct images of this tweet
                "username": block_item["username"],
                "impressions": impressions,
                "retweets": retweets,
                "likes": likes,
                "bookmarks": bookmarks,
                "replies": replies_count,
                "video_posters": block_item[
                    "video_posters"
                ],  # Should be direct video posters
            }
        )

    if not final_results:
        print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®çµæœã€æœ‰åŠ¹ãªæŠ•ç¨¿ãŒæ®‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # Ensure all posters are cleaned up if no final results
        final_ids = {item["id"] for item in final_results}  # Will be empty
        for block in tweet_blocks:  # Iterate original blocks
            if block["id"] not in final_ids:  # All will not be in final_ids
                remove_temp_posters_from_list([block])
        return []

    final_results.sort(key=lambda x: int(x["id"]))
    return final_results


def extract_and_merge_tweets(driver, tweet_urls_data, max_tweets_to_register):
    final_tweets_for_notion = []
    processed_ids = set()
    actually_registered_count = 0

    tweet_urls_data.sort(
        key=lambda x: (
            int(x["id"])
            if isinstance(x, dict) and x.get("id") and x["id"].isdigit()
            else 0
        ),
        reverse=True,
    )

    def is_media_present_in_post(post_data):
        # images ã« card_img ã‚‚å«ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã®ã§ã€ã“ã‚Œã§OK
        has_images = bool(post_data.get("images"))
        has_video_posters = bool(post_data.get("video_posters"))
        return has_images or has_video_posters

    for i, meta in enumerate(tweet_urls_data):
        if actually_registered_count >= max_tweets_to_register:
            print(f"ğŸ¯ Notionã¸ã®ç™»éŒ²ä»¶æ•°ãŒ {max_tweets_to_register} ã«é”ã—ãŸãŸã‚çµ‚äº†")
            break

        tweet_url = meta["url"] if isinstance(meta, dict) else meta

        try:
            # extract_thread_from_detail_page ã¯ã€å…ƒã®æŠ•ç¨¿ã¨å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªãƒ—ãƒ©ã‚¤ã‚’
            # ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿(ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±å«ã‚€)ã®ãƒªã‚¹ãƒˆã¨ã—ã¦è¿”ã™
            thread_posts = extract_thread_from_detail_page(driver, tweet_url)
            if not thread_posts:
                continue

            parent_post_candidate = None

            # thread_posts ã¯æ—¢ã«IDæ˜‡é †ã«ãªã£ã¦ã„ã‚‹ã¯ãš
            for post_in_thread in thread_posts:
                current_post_id = post_in_thread.get("id")

                if not current_post_id:
                    print("âš ï¸ IDãŒãªã„æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                if current_post_id in processed_ids:
                    continue

                if already_registered(current_post_id):
                    processed_ids.add(current_post_id)
                    continue

                # is_reply_structure ã®ã‚ˆã†ãªåˆ¤å®šã¯ extract_tweets ã§æ¸ˆã‚“ã§ã„ã‚‹æƒ³å®š
                # ã“ã“ã§ã¯ extract_target ã®æŠ•ç¨¿ã®ã¿ã‚’æ‰±ã†

                if parent_post_candidate is None:
                    # æœ€åˆã®æŠ•ç¨¿ã‚’è¦ªå€™è£œã¨ã™ã‚‹
                    parent_post_candidate = post_in_thread
                else:
                    # 2ã¤ç›®ä»¥é™ã®æŠ•ç¨¿ã¯ãƒªãƒ—ãƒ©ã‚¤ã¨ã¿ãªã™
                    # ã“ã®ãƒªãƒ—ãƒ©ã‚¤ãŒãƒ¡ãƒ‡ã‚£ã‚¢(card_imgå«ã‚€)ã‚’æŒã¤ã‹åˆ¤å®š
                    reply_has_media = is_media_present_in_post(post_in_thread)

                    if reply_has_media:
                        # ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ããƒªãƒ—ãƒ©ã‚¤ã®å ´åˆ:
                        # 1. ãã‚Œã¾ã§ã®è¦ªå€™è£œã‚’ç™»éŒ²
                        if parent_post_candidate["id"] not in processed_ids:
                            if actually_registered_count < max_tweets_to_register:
                                final_tweets_for_notion.append(parent_post_candidate)
                                actually_registered_count += 1
                                print(
                                    f"âœ… è¦ªæŠ•ç¨¿ã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ (ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ—ãƒ©ã‚¤åˆ†é›¢): {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                                )
                                processed_ids.add(parent_post_candidate["id"])
                            else:
                                print(
                                    f"ğŸ¯ ç™»éŒ²ä¸Šé™({max_tweets_to_register})ã®ãŸã‚ã€è¦ªå€™è£œ {parent_post_candidate['id']} ã¯ç™»éŒ²ã‚¹ã‚­ãƒƒãƒ—(ãƒ¡ãƒ‡ã‚£ã‚¢ãƒªãƒ—ãƒ©ã‚¤åˆ†é›¢æ™‚)"
                                )
                                # ä¸Šé™ã«é”ã—ãŸã‚‰ä»¥é™ã®å‡¦ç†ã¯ä¸è¦
                                break

                        # 2. ã“ã®ãƒ¡ãƒ‡ã‚£ã‚¢ä»˜ããƒªãƒ—ãƒ©ã‚¤ã‚’æ–°ã—ã„è¦ªå€™è£œã¨ã™ã‚‹
                        parent_post_candidate = post_in_thread
                    else:
                        # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒªãƒ—ãƒ©ã‚¤ã®å ´åˆ â†’ è¦ªå€™è£œã«ãƒãƒ¼ã‚¸
                        parent_post_candidate["text"] = (
                            parent_post_candidate.get("text", "")
                            + "\n\n"
                            + post_in_thread.get("text", "")
                        ).strip()
                        processed_ids.add(
                            current_post_id
                        )  # ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒªãƒ—ãƒ©ã‚¤ã‚‚å‡¦ç†æ¸ˆã¿

            # ãƒ«ãƒ¼ãƒ—å¾Œã€æœ€å¾Œã«æ®‹ã£ãŸè¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸
            if (
                parent_post_candidate
                and parent_post_candidate["id"] not in processed_ids
            ):
                if actually_registered_count < max_tweets_to_register:
                    final_tweets_for_notion.append(parent_post_candidate)
                    actually_registered_count += 1
                    print(
                        f"âœ… æœ€çµ‚è¦ªå€™è£œã‚’ç™»éŒ²ãƒªã‚¹ãƒˆã¸è¿½åŠ : {parent_post_candidate['id']} ({actually_registered_count}/{max_tweets_to_register})"
                    )
                    processed_ids.add(parent_post_candidate["id"])
                else:
                    print(
                        f"ğŸ¯ ç™»éŒ²ä¸Šé™({max_tweets_to_register})ã®ãŸã‚ã€æœ€çµ‚è¦ªå€™è£œ {parent_post_candidate['id']} ã¯ç™»éŒ²ã‚¹ã‚­ãƒƒãƒ—"
                    )

            # ç™»éŒ²ä¸Šé™ã«é”ã—ã¦ã„ãŸã‚‰å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã‚‚æŠœã‘ã‚‹
            if actually_registered_count >= max_tweets_to_register:
                print(
                    f"ğŸ¯ Notionã¸ã®ç™»éŒ²ä»¶æ•°ãŒ {max_tweets_to_register} ã«é”ã—ãŸãŸã‚ã€URLå‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†"
                )
                break

        except Exception as e:
            print(
                f"âš ï¸ ã‚¹ãƒ¬ãƒƒãƒ‰å‡¦ç†å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼ ({tweet_url}): {type(e).__name__} - {e}\n{traceback.format_exc()}"
            )
            continue

    print(f"\nğŸ“ˆ æœ€çµ‚çš„ãªNotionç™»éŒ²å¯¾è±¡æŠ•ç¨¿æ•°: {len(final_tweets_for_notion)} ä»¶")
    return final_tweets_for_notion


def extract_metrics(article):
    """
    ã„ã„ã­æ•°ãƒ»ãƒªãƒã‚¹ãƒˆæ•°ãƒ»ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ»ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒ»ãƒªãƒ—ãƒ©ã‚¤æ•°ã‚’æŠ½å‡º
    å–å¾—ã§ããªã„ã‚‚ã®ã¯0ï¼ˆã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿Noneï¼‰ã§è¿”ã™
    """
    impressions_str = retweets_str = likes_str = bookmarks_str = replies_str = None
    try:
        # å„ªå…ˆçš„ã« div[role="group"] ã® aria-label ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã‚‹
        # ã“ã‚ŒãŒæœ€ã‚‚æƒ…å ±ãŒã¾ã¨ã¾ã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„
        group_divs = article.find_elements(
            By.XPATH, ".//div[@role='group' and @aria-label]"
        )

        primary_label_processed = False
        if group_divs:
            for group_div in group_divs:
                label = group_div.get_attribute("aria-label")
                if not label:
                    continue

                print(f"ğŸŸ¦ metrics group aria-labelå†…å®¹: {label}")
                primary_label_processed = True  # ã“ã®ãƒ©ãƒ™ãƒ«ã‚’å‡¦ç†ã—ãŸã“ã¨ã‚’ãƒãƒ¼ã‚¯

                # å„æŒ‡æ¨™ã‚’å€‹åˆ¥ã«æŠ½å‡ºã™ã‚‹ (é †ç•ªã«ä¾å­˜ã—ãªã„ã‚ˆã†ã«)
                m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                if m_replies:
                    replies_str = m_replies.group(1)

                m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                if m_retweets:
                    retweets_str = m_retweets.group(1)

                m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                if m_likes:
                    likes_str = m_likes.group(1)

                m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                if m_bookmarks:
                    bookmarks_str = m_bookmarks.group(1)

                m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                if m_impressions:
                    impressions_str = m_impressions.group(1)

                # ä¸€ã¤ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰å…¨ã¦å–ã‚ŒãŸã‚‰æŠœã‘ã‚‹ã“ã¨ãŒå¤šã„ãŒã€ç¨€ã«åˆ†å‰²ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã—ã€
                # åŸºæœ¬çš„ã«ã¯æœ€åˆã® group_div ã®ãƒ©ãƒ™ãƒ«ã‚’ä¸»ã¨ã™ã‚‹ã€‚
                # ã‚‚ã—ã€è¤‡æ•°ã® group_div ãŒç•°ãªã‚‹æƒ…å ±ã‚’æŒã¤ã‚±ãƒ¼ã‚¹ãŒç¢ºèªã•ã‚Œã‚Œã°ã€ã“ã“ã®ãƒ­ã‚¸ãƒƒã‚¯å†è€ƒã€‚
                break

        if not primary_label_processed:
            # group_div ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€aria-label ãŒãªã„å ´åˆã€ä»¥å‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚è©¦ã™
            # ãŸã ã—ã€ã“ã®ãƒ‘ã‚¹ã¯Xã®UIãŒå¤§ããå¤‰ã‚ã£ãŸå ´åˆã¯æ©Ÿèƒ½ã—ãªã„å¯èƒ½æ€§ãŒé«˜ã„
            other_divs = article.find_elements(
                By.XPATH,
                ".//div[contains(@aria-label, 'ä»¶ã®è¡¨ç¤º') and not(@role='group')]",
            )
            for div in other_divs:
                label = div.get_attribute("aria-label")
                if not label:
                    continue
                print(f"ğŸŸ¦ other metrics div aria-labelå†…å®¹: {label}")
                # ã“ã“ã§ã‚‚åŒæ§˜ã«å€‹åˆ¥æŠ½å‡ºã‚’è©¦ã¿ã‚‹ (ä¸Šè¨˜ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)
                if replies_str is None:
                    m_replies = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label)
                    if m_replies:
                        replies_str = m_replies.group(1)
                if retweets_str is None:
                    m_retweets = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label)
                    if m_retweets:
                        retweets_str = m_retweets.group(1)
                if likes_str is None:
                    m_likes = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label)
                    if m_likes:
                        likes_str = m_likes.group(1)
                if bookmarks_str is None:
                    m_bookmarks = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label)
                    if m_bookmarks:
                        bookmarks_str = m_bookmarks.group(1)
                if impressions_str is None:
                    m_impressions = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¡¨ç¤º", label)
                    if m_impressions:
                        impressions_str = m_impressions.group(1)
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã§å‡¦ç†

        # å€‹åˆ¥ãƒœã‚¿ãƒ³ã‹ã‚‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—
        if replies_str is None:
            try:
                reply_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='reply']"
                )
                for btn in reply_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®è¿”ä¿¡", label or "")
                    if m:
                        replies_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒ—ãƒ©ã‚¤æ•°å–å¾—: {replies_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if retweets_str is None:
            try:
                rt_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='retweet']"
                )
                for btn in rt_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒªãƒã‚¹ãƒˆ", label or "")
                    if m:
                        retweets_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒªãƒã‚¹ãƒˆæ•°å–å¾—: {retweets_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒªãƒã‚¹ãƒˆæ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if likes_str is None:
            try:
                like_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='like']"
                )
                for btn in like_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ã„ã„ã­", label or "")
                    if m:
                        likes_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ã„ã„ã­æ•°å–å¾—: {likes_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ã„ã„ã­æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        if bookmarks_str is None:
            try:
                bm_btns = article.find_elements(
                    By.XPATH, ".//button[@data-testid='bookmark']"
                )
                for btn in bm_btns:
                    label = btn.get_attribute("aria-label")
                    m = re.search(r"(\d[\d,\.ä¸‡]*)\s*ä»¶ã®ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯", label or "")
                    if m:
                        bookmarks_str = m.group(1)
                        print(f"ğŸŸ¦ ãƒœã‚¿ãƒ³ã‹ã‚‰ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°å–å¾—: {bookmarks_str}")
                        break
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°ãƒœã‚¿ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã¯ãƒœã‚¿ãƒ³ã‹ã‚‰ã¯é€šå¸¸å–ã‚Œãªã„ã®ã§ã€aria-labelé ¼ã¿
        # ã‚‚ã— impressions_str ãŒ None ã§ã€ä»–ã®æŒ‡æ¨™ãŒå–ã‚Œã¦ã„ã‚‹å ´åˆã€
        # ã‹ã¤ã¦ã®ã€Œã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ã€ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å–ã‚Œã¦ã„ãŸå¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€
        # likes/retweets/bookmarks/replies ãŒå…¨ã¦0ãªã‚‰ã€impressions_str ã‚’æ¡ç”¨ã—ä»–ã‚’0ã«ã™ã‚‹ã€‚
        # ãŸã ã—ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯è¤‡é›‘ãªã®ã§ã€ä¸€æ—¦ã¯ä¸Šè¨˜ã§å–å¾—ã§ããŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ã†ã€‚
        # ã‚‚ã—ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã ã‘ãŒå–ã‚Œã¦ä»–ãŒ0ã«ãªã‚‹ã¹ãã‚±ãƒ¼ã‚¹ãŒå¤šç™ºã™ã‚‹ãªã‚‰å†æ¤œè¨ã€‚

        def parse_num(s):
            if not s:
                return 0  # None ã‚„ç©ºæ–‡å­—ã®å ´åˆã¯0ã¨ã—ã¦æ‰±ã† (ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ä»¥å¤–)
            s_cleaned = str(s).replace(",", "")
            if "ä¸‡" in s_cleaned:
                try:
                    return int(float(s_cleaned.replace("ä¸‡", "")) * 10000)
                except ValueError:
                    return 0  # "ä¸‡" ãŒã‚ã£ã¦ã‚‚æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆ
            try:
                return int(s_cleaned)
            except ValueError:  # "K" ã‚„ "M" ãªã©ã®è‹±èªåœã®ç•¥ç§°ã¯ç¾çŠ¶éå¯¾å¿œ
                return 0  # æ•°å€¤å¤‰æ›ã§ããªã„å ´åˆã¯0

        # ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ None ã‚’è¨±å®¹ã—ã€ä»–ã¯0ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
        impressions = (
            parse_num(impressions_str) if impressions_str is not None else None
        )
        retweets = parse_num(retweets_str)
        likes = parse_num(likes_str)
        bookmarks = parse_num(bookmarks_str)
        replies = parse_num(replies_str)

        # ãƒ‡ãƒãƒƒã‚°ç”¨ã«æœ€çµ‚çš„ãªå€¤ã‚’è¡¨ç¤º
        print(
            f"ğŸ”¢ æŠ½å‡ºçµæœ: è¡¨ç¤º={impressions}, RT={retweets}, ã„ã„ã­={likes}, BM={bookmarks}, ãƒªãƒ—ãƒ©ã‚¤={replies}"
        )

    except Exception as e:
        print(f"âš ï¸ extract_metricså…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ (impressions=None, ä»–=0)
        impressions = None
        retweets = 0
        likes = 0
        bookmarks = 0
        replies = 0

    return impressions, retweets, likes, bookmarks, replies


def is_reply_structure(
    article,
    tweet_id=None,
    text="",
    image_urls=None,  # ã“ã®image_urlsã¯å‘¼ã³å‡ºã—å…ƒ(extract_tweets)ã§åé›†ã•ã‚ŒãŸã€ç¾åœ¨ã®articleã«ç›´æ¥å±ã™ã‚‹ç”»åƒURL
    video_poster_urls=None,  # åŒæ§˜ã«ã€ç¾åœ¨ã®articleã«ç›´æ¥å±ã™ã‚‹å‹•ç”»ãƒã‚¹ã‚¿ãƒ¼URL
):
    try:
        id_display = f"ï¼ˆID={tweet_id}ï¼‰" if tweet_id else ""

        # 1. åºƒå‘ŠæŠ•ç¨¿ã®å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯ (is_ad_post ã¯åˆ¥é€”å®šç¾©ã•ã‚Œã¦ã„ã‚‹æƒ³å®š)
        # is_reply_structure ã®è²¬å‹™ã§ã¯ãªã„ãŸã‚ã€å‘¼ã³å‡ºã—å…ƒ (extract_tweets) ã§è¡Œã†ã¹ã
        # if is_ad_post(text):
        #     print(f"ğŸš« is_reply_structure: åºƒå‘Šåˆ¤å®š â†’ é™¤å¤– {id_display}")
        #     return True

        # 2. å¼•ç”¨ãƒ„ã‚¤ãƒ¼ãƒˆã®åˆ¤å®š
        is_quote_tweet_structure = False
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: articleè¦ç´ ã®å­å­«ã«ã€ç›´æ¥ã®å­ã¨ã—ã¦ã€Œå¼•ç”¨ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã¤spanã¨
            #           ã€Œrole="link"ã€ã‚’æŒã¤divã®ä¸¡æ–¹ã‚’æŒã¤divãŒå­˜åœ¨ã™ã‚‹ã‹ã€‚
            #           (ä¾‹: å¼•ç”¨è©³ç´°.html ã®ã‚ˆã†ãªæ§‹é€ )
            #           normalize-space()ã§å‰å¾Œã®ç©ºç™½ã‚’ç„¡è¦–ã—ã¦ã€Œå¼•ç”¨ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒã•ã›ã‚‹ã€‚
            xpath_for_specific_quote_container = ".//div[count(./span[normalize-space(text())='å¼•ç”¨']) > 0 and count(./div[@role='link']) > 0]"
            if (
                len(article.find_elements(By.XPATH, xpath_for_specific_quote_container))
                > 0
            ):
                is_quote_tweet_structure = True
                # print(f"DEBUG: is_reply_structure - å¼•ç”¨åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³1ã«ä¸€è‡´ {id_display}")

            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å¾“æ¥ã®åˆ¤å®šï¼ˆãƒã‚¹ãƒˆã•ã‚ŒãŸarticleã‚’æŒã¤å¼•ç”¨RTï¼‰ã‚‚ãƒã‚§ãƒƒã‚¯
            # is_quote_tweet_structure ãŒã¾ã Falseã®å ´åˆã®ã¿å®Ÿè¡Œ
            if not is_quote_tweet_structure:
                # å¼•ç”¨RTã¯ã€è‡ªèº«ã® <article> å†…ã«ã€å¼•ç”¨å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®
                # <div role="link"> (ã¾ãŸã¯é¡ä¼¼ã®ã‚³ãƒ³ãƒ†ãƒŠ) ãŒã‚ã‚Šã€ãã®ä¸­ã«ã•ã‚‰ã« <article data-testid="tweet"> ãŒãƒã‚¹ãƒˆã•ã‚Œã‚‹æ§‹é€ ãŒå¤šã„ã€‚
                quoted_tweet_articles_in_link_role = article.find_elements(
                    By.XPATH,
                    ".//div[@role='link' and .//article[@data-testid='tweet']]",
                )
                if len(quoted_tweet_articles_in_link_role) > 0:
                    is_quote_tweet_structure = True
                    # print(f"DEBUG: is_reply_structure - å¼•ç”¨åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³2ã«ä¸€è‡´ {id_display}")

        except Exception as e_quote_check:
            print(
                f"âš ï¸ is_reply_structure: å¼•ç”¨åˆ¤å®šä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_quote_check).__name__}: {e_quote_check}"
            )
            is_quote_tweet_structure = (
                False  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨ç­–ã¨ã—ã¦å¼•ç”¨RTã§ã¯ãªã„ã¨ã¿ãªã™
            )

        if is_quote_tweet_structure:
            text_length = len(text.strip()) if text else 0

            # å¼•ç”¨RTæœ¬ä½“ãŒæŒã¤ãƒ¡ãƒ‡ã‚£ã‚¢ã®åˆ¤å®š
            # image_urls ã‚„ video_poster_urls ã¯ã€ã“ã® is_reply_structure ã‚’å‘¼ã³å‡ºã™
            # extract_tweets é–¢æ•°å†…ã§ã€ç¾åœ¨ã® article ã«ç›´æ¥å±ã™ã‚‹ã‚‚ã®ã¨ã—ã¦æŠ½å‡ºãƒ»æ¸¡ã•ã‚Œã‚‹æƒ³å®š
            has_own_images = bool(
                image_urls and any("twimg.com/media" in url for url in image_urls)
            )
            has_own_video_posters = bool(video_poster_urls)
            has_own_card_img = bool(
                image_urls and any("twimg.com/card_img" in url for url in image_urls)
            )

            quote_rt_has_own_media = (
                has_own_images or has_own_video_posters or has_own_card_img
            )

            # ãƒ«ãƒ¼ãƒ«: ã€Œ50æ–‡å­—ä»¥ä¸Šã€ã‹ã¤ã€Œãƒ¡ãƒ‡ã‚£ã‚¢ãŒãªã„ã€å¼•ç”¨RTã¯å–å¾—ã—ãªã„ (ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹)
            if text_length >= 50 and not quote_rt_has_own_media:
                print(
                    f"ğŸ›‘ is_reply_structure: å¼•ç”¨RTï¼ˆ50æ–‡å­—ä»¥ä¸Š ã‹ã¤ æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—ï¼‰â†’ é™¤å¤– {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={quote_rt_has_own_media}"
                )
                return True  # ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ (å–å¾—ã—ãªã„)
            else:
                # ä¸Šè¨˜ã®ã‚¹ã‚­ãƒƒãƒ—æ¡ä»¶ã«è©²å½“ã—ãªã„å¼•ç”¨RTã¯ã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã§ã¯å–å¾—å¯¾è±¡ã¨ã™ã‚‹
                # (ä¾‹: 50æ–‡å­—æœªæº€ã®å¼•ç”¨RTã€ã¾ãŸã¯ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’æŒã¤å¼•ç”¨RT)
                print(
                    f"âœ… is_reply_structure: å¼•ç”¨RTï¼ˆä¸Šè¨˜é™¤å¤–æ¡ä»¶ã«è©²å½“ã›ãšï¼‰â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦è¨±å¯ {id_display} | é•·ã•={text_length}, æœ¬ä½“ãƒ¡ãƒ‡ã‚£ã‚¢={quote_rt_has_own_media}"
                )
                return False  # ã‚¹ã‚­ãƒƒãƒ—ã—ãªã„ (å–å¾—ã™ã‚‹)

        # 3. é€šå¸¸ã®ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ã®åˆ¤å®š
        #   - ã€Œè¿”ä¿¡å…ˆ: @usernameã€ã®ã‚ˆã†ãªãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹
        #   - æŠ•ç¨¿ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³ã®æ•°ãŒå°‘ãªã„ã‹ï¼ˆé€šå¸¸æŠ•ç¨¿ã¯4ã¤ä»¥ä¸Šã€ãƒªãƒ—ãƒ©ã‚¤ã¯å°‘ãªã„ã“ã¨ãŒã‚ã‚‹ï¼‰

        # è¿”ä¿¡å…ˆè¡¨ç¤ºã®ç¢ºèª (ã‚ˆã‚Šç¢ºå®Ÿãªãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š)
        # XPathã‚’èª¿æ•´ã—ã¦ã€articleç›´ä¸‹ã®è¦ç´ ã«é™å®šã™ã‚‹ã‹ã€ã‚ˆã‚Šå…·ä½“çš„ãªæ§‹é€ ã‚’æŒ‡å®šã™ã‚‹
        # "Replying to" ã‚„ "è¿”ä¿¡å…ˆ:" ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã¤è¦ç´ ã‚’æ¢ã™
        # ã‚ˆã‚Šå…·ä½“çš„ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åè¡¨ç¤º(@...)ã®å…„å¼Ÿè¦ç´ ã‚„è¦ªè¦ç´ ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„
        reply_to_indicator_xpaths = [
            ".//div[starts-with(normalize-space(.), 'Replying to') or starts-with(normalize-space(.), 'è¿”ä¿¡å…ˆ:')]",
            ".//span[starts-with(normalize-space(.), 'Replying to') or starts-with(normalize-space(.), 'è¿”ä¿¡å…ˆ:')]",
            ".//div[@data-testid='User-Name']/../../../div[1]//span[starts-with(normalize-space(.), 'Replying to') or starts-with(normalize-space(.), 'è¿”ä¿¡å…ˆ:')]",  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åè¡¨ç¤ºã®ä¸Šéƒ¨ã«ã‚ã‚‹å ´åˆ
        ]

        is_indicator_visible = False
        for xpath in reply_to_indicator_xpaths:
            try:
                indicators = article.find_elements(By.XPATH, xpath)
                for indicator_el in indicators:
                    if indicator_el.is_displayed():  # è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹è¦ç´ ã®ã¿ã‚’å¯¾è±¡
                        is_indicator_visible = True
                        break
                if is_indicator_visible:
                    break
            except StaleElementReferenceException:
                pass  # è¦ç´ ãŒæ¶ˆãˆãŸå ´åˆã¯ç„¡è¦–
            except NoSuchElementException:
                pass  # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æ¬¡ã®XPathã¸
            except Exception as e_reply_indicator:
                print(
                    f"âš ï¸ is_reply_structure: è¿”ä¿¡å…ˆè¡¨ç¤ºç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} ({xpath}) â†’ {type(e_reply_indicator).__name__}: {e_reply_indicator}"
                )
                pass

        if is_indicator_visible:
            print(
                f"ğŸ’¬ is_reply_structure: è¿”ä¿¡å…ˆè¡¨ç¤ºã‚ã‚Š â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®š {id_display}"
            )
            return True  # é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦ã‚¹ã‚­ãƒƒãƒ—

        # ãƒœã‚¿ãƒ³ã®æ•°ã«ã‚ˆã‚‹åˆ¤å®š (è£œåŠ©çš„ã€ã¾ãŸã¯ä¸Šè¨˜ã§åˆ¤å®šã§ããªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã®ãƒ„ã‚¤ãƒ¼ãƒˆã¨è©³ç´°ãƒšãƒ¼ã‚¸ã®ãƒ„ã‚¤ãƒ¼ãƒˆã§ãƒœã‚¿ãƒ³æ§‹é€ ãŒç•°ãªã‚‹å ´åˆãŒã‚ã‚‹ã®ã§æ³¨æ„
        # data-testid ã‚’æŒã¤ button è¦ç´ ã‚’æ•°ãˆã‚‹
        # å¼•ç”¨RTã§ãªã„ã“ã¨ãŒæ—¢ã«ç¢ºèªã•ã‚Œã¦ã„ã‚‹å‰æã§ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã«å…¥ã‚‹
        try:
            buttons = article.find_elements(
                By.XPATH, ".//div[@role='group']//button[@data-testid]"
            )
            # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ä¸Šã§ã¯é€šå¸¸4ã¤ (reply, retweet, like, view/bookmark)
            # ãƒªãƒ—ãƒ©ã‚¤ã®å ´åˆã€viewãŒãªã„ã“ã¨ãŒã‚ã‚‹ (3ã¤ã«ãªã‚‹)
            # éå¸¸ã«å¤ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚„ç‰¹æ®Šãªã‚±ãƒ¼ã‚¹ã§ã¯ã•ã‚‰ã«å°‘ãªã„ã“ã¨ã‚‚
            # é–¾å€¤ã¯çŠ¶æ³ã«å¿œã˜ã¦èª¿æ•´ã€‚ã“ã“ã§ã¯3ã¤ä»¥ä¸‹ãªã‚‰ãƒªãƒ—ãƒ©ã‚¤ã®å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã€‚
            # å¼•ç”¨RTã§ãªã„ã“ã¨ã¯ä¸Šã§åˆ¤å®šæ¸ˆã¿ãªã®ã§ã€ãƒœã‚¿ãƒ³ãŒå°‘ãªã„å ´åˆã¯é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤ã®å¯èƒ½æ€§ã€‚
            if len(buttons) <= 3:  # é–¾å€¤ã‚’ <= 3 ã«å¤‰æ›´ (4ã¤æœªæº€ã¯ãƒªãƒ—ãƒ©ã‚¤ã®å¯èƒ½æ€§)
                print(
                    f"ğŸ’¬ is_reply_structure: ãƒœã‚¿ãƒ³æ•° {len(buttons)} å€‹ï¼ˆ3å€‹ä»¥ä¸‹ï¼‰â†’ é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤åˆ¤å®šã®å¯èƒ½æ€§ {id_display}"
                )
                return True  # é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤ã¨ã—ã¦ã‚¹ã‚­ãƒƒãƒ—
        except Exception as e_button_count:
            print(
                f"âš ï¸ is_reply_structure: ãƒœã‚¿ãƒ³æ•°ç¢ºèªä¸­ã®ã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e_button_count).__name__}: {e_button_count}"
            )
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åˆ¤å®šä¸èƒ½ãªã®ã§ã€å®‰å…¨ç­–ã¨ã—ã¦è¦ªæŠ•ç¨¿æ‰±ã„ï¼ˆFalseã‚’è¿”ã™ï¼‰

        # ä¸Šè¨˜ã®ã„ãšã‚Œã®æ¡ä»¶ï¼ˆå¼•ç”¨RTã®é™¤å¤–ã€é€šå¸¸ãƒªãƒ—ãƒ©ã‚¤æ§‹é€ ï¼‰ã«ã‚‚è©²å½“ã—ãªã„å ´åˆã¯è¦ªæŠ•ç¨¿ã¨ã¿ãªã™
        print(
            f"âœ… is_reply_structure: æ§‹é€ ä¸Šå•é¡Œãªã—ï¼ˆéå¼•ç”¨RTã€éãƒªãƒ—ãƒ©ã‚¤ï¼‰â†’ è¦ªæŠ•ç¨¿ã¨åˆ¤å®š {id_display}"
        )
        return False  # è¦ªæŠ•ç¨¿ã¨ã—ã¦å–å¾—

    except StaleElementReferenceException:
        print(
            f"âš ï¸ is_reply_structure: StaleElementReferenceExceptionç™ºç”Ÿ {id_display} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False  # è¦ç´ ãŒç„¡åŠ¹ã«ãªã£ãŸå ´åˆã¯ã€èª¤ã£ã¦é™¤å¤–ã—ãªã„ã‚ˆã†ã«Falseã‚’è¿”ã™ï¼ˆè¦ä»¶ã«ã‚ˆã‚‹ï¼‰
    except Exception as e:
        print(
            f"âš ï¸ is_reply_structure: åˆ¤å®šã‚¨ãƒ©ãƒ¼ {id_display} â†’ {type(e).__name__}: {e}\n{traceback.format_exc()} â†’ è¦ªæŠ•ç¨¿ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ç­–ï¼‰"
        )
        return False  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å®‰å…¨å´ã«å€’ã™


def has_media_in_html(article_html):
    soup = BeautifulSoup(article_html, "html.parser")
    # ç”»åƒåˆ¤å®š
    if soup.find("img", {"src": lambda x: x and "twimg.com/media" in x}):
        return True
    # å‹•ç”»åˆ¤å®š
    if soup.find("div", {"data-testid": "video-player-mini-ui-"}):
        return True
    if soup.find("button", {"aria-label": "å‹•ç”»ã‚’å†ç”Ÿ"}):
        return True
    if soup.find("video"):
        return True
    return False


def extract_tweets(driver, extract_target, max_tweets):
    print(f"\nâœ¨ ã‚¢ã‚¯ã‚»ã‚¹ä¸­: https://twitter.com/{extract_target}")
    driver.get(f"https://twitter.com/{extract_target}")
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, "//article"))
    )

    tweet_urls = []
    seen_urls = set()
    scroll_count = 0
    max_scrolls = 20  # ä¾‹ãˆã°20å›ã‚’æœ€å¤§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å›æ•°ã¨ã—ã¦è¨­å®š
    # ... (rest of the variables) ...
    pause_counter = 0
    pause_threshold = 3
    last_seen_count = 0

    while scroll_count < max_scrolls and len(tweet_urls) < max_tweets:
        print(f"\nğŸ” ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« {scroll_count + 1}/{max_scrolls} å›ç›®")
        current_articles_in_dom = driver.find_elements(
            By.XPATH, "//article[@data-testid='tweet']"
        )
        print(f"ğŸ“„ ç¾åœ¨ã®articleæ•°: {len(current_articles_in_dom)}")

        new_tweets_found_in_scroll = 0
        for i, article in enumerate(current_articles_in_dom):
            try:
                # ... (URL and tweet_id extraction logic remains the same) ...
                href_els = article.find_elements(
                    By.XPATH,
                    ".//a[contains(@href, '/status/') and not(ancestor::div[contains(@style, 'display: none')])]",
                )
                if not href_els:
                    continue

                tweet_url = ""
                for href_el in href_els:
                    href_attr = href_el.get_attribute("href")
                    if href_attr and "/status/" in href_attr:
                        if f"/{extract_target}/status/" in href_attr:
                            tweet_url = (
                                href_attr
                                if href_attr.startswith("http")
                                else f"https://x.com{href_attr}"
                            )
                            break
                if not tweet_url:
                    first_href = href_els[0].get_attribute("href")
                    tweet_url = (
                        first_href
                        if first_href.startswith("http")
                        else f"https://x.com{first_href}"
                    )

                tweet_id_match = re.search(r"/status/(\d+)", tweet_url)
                if not tweet_id_match:
                    print(f"âš ï¸ URLã‹ã‚‰tweet_idæŠ½å‡ºå¤±æ•—: {tweet_url}")
                    continue
                tweet_id = tweet_id_match.group(1)

                if tweet_url in seen_urls:
                    continue

                username_in_url_match = re.search(r"x\.com/([^/]+)/status", tweet_url)
                if (
                    not username_in_url_match
                    or username_in_url_match.group(1).lower() != extract_target.lower()
                ):
                    continue

                text_el = None
                try:
                    text_el = article.find_element(
                        By.XPATH, ".//div[@data-testid='tweetText']"
                    )
                except:
                    pass
                text = normalize_text(text_el.text) if text_el and text_el.text else ""

                # ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±ã®æŠ½å‡ºï¼ˆis_reply_structureã«æ¸¡ã™ãŸã‚ï¼‰
                # ç¾åœ¨ã® 'article' ã«ç›´æ¥å±ã™ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
                images_for_check = []
                image_elements = article.find_elements(
                    By.XPATH,
                    ".//div[@data-testid='tweetPhoto']//img[contains(@src, 'twimg.com/media') or contains(@src, 'twimg.com/card_img')]",
                )
                for img_el in image_elements:
                    # ã“ã®img_elãŒç¾åœ¨ã®articleç›´ä¸‹ï¼ˆã¾ãŸã¯ãã®tweetPhotoå†…ï¼‰ã«ã‚ã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸå¼•ç”¨RTå†…ã®ã‚‚ã®ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    # æœ€ã‚‚è¿‘ã„ç¥–å…ˆã®articleãŒç¾åœ¨ã®articleè‡ªèº«ã§ã‚ã‚‹ã‹ã§åˆ¤å®š
                    try:
                        closest_ancestor_article = img_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            src = img_el.get_attribute("src")
                            if src:
                                images_for_check.append(src)
                    except StaleElementReferenceException:  # è¦ç´ ãŒæ¶ˆãˆãŸå ´åˆ
                        print(f"âš ï¸ ç”»åƒè¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:  # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                        pass

                videos_for_check = []
                video_elements = article.find_elements(
                    By.XPATH, ".//div[@data-testid='videoPlayer']//video[@poster]"
                )
                for video_el in video_elements:
                    try:
                        closest_ancestor_article = video_el.find_element(
                            By.XPATH, "ancestor::article[@data-testid='tweet'][1]"
                        )
                        if closest_ancestor_article == article:
                            poster = video_el.get_attribute("poster")
                            if poster:
                                videos_for_check.append(poster)
                    except StaleElementReferenceException:
                        print(f"âš ï¸ å‹•ç”»è¦ç´ ãƒã‚§ãƒƒã‚¯ä¸­ã«StaleElement (ID: {tweet_id})")
                        continue
                    except Exception:
                        pass

                # has_media ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ is_reply_structure ã‹ã‚‰å‰Šé™¤ã—ãŸã®ã§ã€ã“ã“ã§ã‚‚æ¸¡ã•ãªã„
                if is_reply_structure(
                    article,
                    tweet_id=tweet_id,
                    text=text,
                    # has_media=has_media_for_check, # å‰Šé™¤
                    image_urls=images_for_check,
                    video_poster_urls=videos_for_check,
                ):
                    continue

                if is_ad_post(text):
                    continue

                tweet_urls.append({"url": tweet_url, "id": tweet_id})
                seen_urls.add(tweet_url)
                new_tweets_found_in_scroll += 1

                print(f"âœ… åé›†å€™è£œã«è¿½åŠ : {tweet_url} ({len(tweet_urls)}ä»¶ç›®)")
                if len(tweet_urls) >= max_tweets:
                    break

            except StaleElementReferenceException:
                print(
                    "âš ï¸ StaleElementReferenceExceptionç™ºç”Ÿã€‚DOMãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’å†è©¦è¡Œã—ã¾ã™ã€‚"
                )
                break
            except Exception as e:
                print(
                    f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {type(e).__name__} - {e} (URL: {tweet_url if 'tweet_url' in locals() else 'ä¸æ˜'})"
                )
                continue

        # ... (rest of the scroll and loop logic) ...
        if len(tweet_urls) >= max_tweets:
            print(
                f"ğŸ¯ åé›†å€™è£œæ•°ãŒMAX_TWEETS ({max_tweets}) ã«é”ã—ãŸãŸã‚ã€URLåé›†ã‚’çµ‚äº†ã€‚"
            )
            break

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2.5)
        scroll_count += 1

        if new_tweets_found_in_scroll == 0:
            pause_counter += 1
            print(
                f"ğŸ§Š ã“ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§æ–°è¦æŠ•ç¨¿ãªã— â†’ pause_counter={pause_counter}/{pause_threshold}"
            )
            if pause_counter >= pause_threshold:
                print("ğŸ›‘ æ–°ã—ã„æŠ•ç¨¿ãŒé€£ç¶šã—ã¦æ¤œå‡ºã•ã‚Œãªã„ãŸã‚URLåé›†ã‚’ä¸­æ–­")
                break
        else:
            pause_counter = 0

    print(f"\nğŸ“ˆ åé›†å€™è£œã®URLå–å¾—å®Œäº† â†’ åˆè¨ˆ: {len(tweet_urls)} ä»¶")
    return tweet_urls


def already_registered(tweet_id):
    if not tweet_id or not tweet_id.isdigit():
        return False
    query = {"filter": {"property": "æŠ•ç¨¿ID", "rich_text": {"equals": tweet_id}}}
    try:
        result = notion.databases.query(database_id=DATABASE_ID, **query)
        return len(result.get("results", [])) > 0
    except Exception as e:
        print(f"âš ï¸ Notionã‚¯ã‚¨ãƒªå¤±æ•—: {e}")
        return False


def ocr_and_remove_image(image_path, label=None):
    """
    ç”»åƒãƒ‘ã‚¹ã‚’å—ã‘å–ã‚ŠOCRã—ã€ä½¿ç”¨å¾Œã«å‰Šé™¤ã™ã‚‹ã€‚
    labelãŒã‚ã‚Œã°çµæœã®å…ˆé ­ã«ä»˜ä¸ã€‚
    """
    result = ""
    try:
        ocr_result = ocr_image(image_path)
        if ocr_result:
            cleaned = clean_ocr_text(ocr_result)
            result = f"[{label}]\n{cleaned}" if label else cleaned
    except Exception as e:
        print(f"âš ï¸ OCRå¤±æ•—: {e}")
    finally:
        try:
            os.remove(image_path)
            print(f"ğŸ—‘ï¸ ç”»åƒå‰Šé™¤: {image_path}")
        except Exception as e:
            print(f"âš ï¸ ç”»åƒå‰Šé™¤å¤±æ•—: {e}")
    return result


def clean_ocr_text(text):
    # é™¤å¤–ã—ãŸã„æ–‡è¨€ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã“ã“ã«è¿½åŠ 
    EXCLUDE_PATTERNS = [
        "æœè³ªå•ã‚’ã€Œã„ã„ã­!ã€ ã™ã‚‹",
        "ã“ã®æŠ•ç¨¿ã‚’ã„ã„ã­ï¼",
        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    ]
    lines = text.splitlines()
    cleaned = [
        line for line in lines if not any(pat in line for pat in EXCLUDE_PATTERNS)
    ]
    return "\n".join(cleaned)


def upload_to_notion(tweet):
    print(f"ğŸ“¤ Notionç™»éŒ²å‡¦ç†é–‹å§‹: {tweet['id']}")
    print(f"ğŸ–¼ï¸ images: {tweet.get('images')}")

    if already_registered(tweet["id"]):
        print(f"ğŸš« ã‚¹ã‚­ãƒƒãƒ—æ¸ˆ: {tweet['id']}")
        return

    props = {
        "æŠ•ç¨¿ID": {
            "rich_text": [{"type": "text", "text": {"content": str(tweet["id"])}}]
        },
        "æœ¬æ–‡": {
            "rich_text": [
                {
                    "type": "text",
                    "text": {
                        "content": tweet["text"],
                        "link": None,
                    },
                    "annotations": {
                        "bold": False,
                        "italic": False,
                        "strikethrough": False,
                        "underline": False,
                        "code": False,
                        "color": "default",
                    },
                }
            ]
        },
        "URL": {"url": tweet["url"]},
        "æŠ•ç¨¿æ—¥æ™‚": {"date": {"start": tweet["date"]} if tweet["date"] else None},
        "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": {"select": {"name": "æœªå›ç­”"}},
        "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ•°": {
            "number": (
                int(tweet["impressions"])
                if tweet.get("impressions") is not None
                else None
            )
        },
        "ãƒªãƒã‚¹ãƒˆæ•°": {
            "number": int(tweet["retweets"]) if tweet.get("retweets") is not None else 0
        },
        "ã„ã„ã­æ•°": {
            "number": int(tweet["likes"]) if tweet.get("likes") is not None else 0
        },
        "ãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯æ•°": {
            "number": (
                int(tweet["bookmarks"]) if tweet.get("bookmarks") is not None else 0
            )
        },
        "ãƒªãƒ—ãƒ©ã‚¤æ•°": {
            "number": int(tweet["replies"]) if tweet.get("replies") is not None else 0
        },
        "æ–‡å­—èµ·ã“ã—": {"rich_text": []},
    }

    ocr_texts = []

    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®OCRï¼ˆtweet["images"]ï¼‰
    for idx, img_url in enumerate(tweet.get("images", [])):
        img_path = f"ocr_image_{tweet['id']}_{idx}.jpg"
        try:
            resp = requests.get(img_url, stream=True)
            with open(img_path, "wb") as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            ocr_text = ocr_and_remove_image(img_path, label=f"ç”»åƒ{idx+1}")
            if ocr_text:
                ocr_texts.append(ocr_text)
        except Exception as e:
            print(f"âš ï¸ ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")

    # posterç”»åƒã®OCR
    poster_paths = tweet.get("video_posters") or []
    if isinstance(poster_paths, str):
        poster_paths = [poster_paths]
    for idx, poster_path in enumerate(poster_paths):
        ocr_text = ocr_and_remove_image(poster_path, label=f"å‹•ç”»ã‚µãƒ ãƒã‚¤ãƒ«{idx+1}")
        if ocr_text:
            ocr_texts.append(ocr_text)

    if ocr_texts:
        props["æ–‡å­—èµ·ã“ã—"]["rich_text"] = [
            {"type": "text", "text": {"content": "\n\n".join(ocr_texts)}}
        ]

    children_blocks = []

    try:
        new_page = notion.pages.create(
            parent={"database_id": DATABASE_ID},
            properties=props,
            children=children_blocks,
        )
        print(f"ğŸ“ Notionç™»éŒ²å®Œäº†: {tweet['url']}")
    except Exception as e:
        print(f"âŒ Notionç™»éŒ²å¤±æ•—: {tweet['id']} ã‚¨ãƒ©ãƒ¼: {e}")


def load_config(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def is_recruit_account(name, bio, config):
    return any(
        k in name or k in bio for k in config.get("filter_keywords_name_bio", [])
    )


def is_recruit_post(text, config):
    return any(k in text for k in config.get("filter_keywords_tweet", []))


def search_accounts(driver, keyword_list):
    results = []
    for keyword in keyword_list:
        search_url = f"https://twitter.com/search?q={keyword}&f=user"
        driver.get(search_url)
        time.sleep(3)

        # âš  æ–°UIæ§‹é€ ã«å¯¾å¿œ
        users = driver.find_elements(
            By.XPATH, "//a[contains(@href, '/')]//div[@dir='auto']/../../.."
        )
        print(f"ğŸ” å€™è£œãƒ¦ãƒ¼ã‚¶ãƒ¼ä»¶æ•°: {len(users)}")

        for user in users:
            try:
                spans = user.find_elements(By.XPATH, ".//span")
                name = ""
                username = ""

                for span in spans:
                    text = span.text.strip()
                    if text.startswith("@"):
                        username = text.replace("@", "")
                    elif not name:
                        name = text

                if username and name:
                    results.append(
                        {
                            "name": name,
                            "username": username,
                            "bio": "",  # ã“ã®æ®µéšã§ã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç”»é¢ã«é£›ã‚“ã§ã„ãªã„
                        }
                    )
            except Exception as e:
                print(f"âš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºå¤±æ•—: {e}")
                continue

    return results


def merge_replies_with_driver(driver, tweet):
    try:
        driver.get(tweet["url"])
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located(
                (By.XPATH, "//article[@data-testid='tweet']")
            )
        )

        replies = extract_self_replies(driver, tweet.get("username", ""))
        if not isinstance(replies, list):
            print(
                f"âš ï¸ merge_replies_with_driver() ã§å–å¾—ã—ãŸrepliesãŒä¸æ­£ãªå‹: {type(replies)} â†’ ç©ºãƒªã‚¹ãƒˆã«ç½®æ›"
            )
            replies = []

        tweet_text = tweet.get("text") or ""
        replies = sorted(
            [r for r in replies if r.get("id") and r.get("text")],
            key=lambda x: int(x["id"]),
        )

        existing_chunks = set(tweet_text.strip().split("\n\n"))
        reply_texts = []

        for r in replies:
            # ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ãªã„
            if r.get("images") or r.get("video_posters"):
                print(
                    f"ğŸ›‘ ç”»åƒãƒ»å‹•ç”»ãƒ»card_imgä»˜ããƒªãƒ—ãƒ©ã‚¤ã¯è¦ªã«ãƒãƒ¼ã‚¸ã—ã¾ã›ã‚“: {r['id']}"
                )
                continue

            reply_id = r["id"]
            reply_body = r["text"].strip()
            clean_body = reply_body[:20].replace("\n", " ")
            print(f"ğŸ§µ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå€™è£œ: ID={reply_id} | textå…ˆé ­: {clean_body}")

            if not reply_body:
                continue
            if reply_body in existing_chunks:
                continue
            if r["id"] == tweet["id"]:
                continue

            reply_texts.append(reply_body)
            existing_chunks.add(reply_body)

        if reply_texts:
            tweet["text"] = tweet_text + "\n\n" + "\n\n".join(reply_texts)

    except Exception as e:
        print(f"âš ï¸ ãƒªãƒ—ãƒ©ã‚¤çµ±åˆå¤±æ•—ï¼ˆ{tweet.get('url', 'ä¸æ˜URL')}ï¼‰: {e}")
    return tweet


def extract_from_search(driver, keywords, max_tweets, name_bio_keywords=None):
    tweets = []
    seen_urls = set()
    seen_users = set()

    for keyword in keywords:
        print(f"ğŸ” è©±é¡Œã®ãƒ„ã‚¤ãƒ¼ãƒˆæ¤œç´¢ä¸­: {keyword}")
        search_url = f"https://twitter.com/search?q={keyword}&src=typed_query&f=top"
        driver.get(search_url)
        time.sleep(3)

        scroll_count = 0
        max_scrolls = 10
        pause_counter = 0
        pause_threshold = 3
        last_article_count = 0

        while len(tweets) < max_tweets and scroll_count < max_scrolls:
            articles = driver.find_elements(By.XPATH, "//article[@data-testid='tweet']")
            article_count = len(articles)
            print(f"ğŸ“„ è¡¨ç¤ºä¸­ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {article_count}")
            for article in articles:
                try:
                    # ãƒ„ã‚¤ãƒ¼ãƒˆURLã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼åå–å¾—
                    time_el = article.find_element(By.XPATH, ".//time")
                    tweet_href = time_el.find_element(By.XPATH, "..").get_attribute(
                        "href"
                    )
                    tweet_url = (
                        tweet_href
                        if tweet_href.startswith("http")
                        else f"https://x.com{tweet_href}"
                    )
                    if tweet_url in seen_urls:
                        continue
                    seen_urls.add(tweet_url)

                    name_block = article.find_element(
                        By.XPATH, ".//div[@data-testid='User-Name']"
                    )
                    spans = name_block.find_elements(By.XPATH, ".//span")
                    display_name = ""
                    username = ""
                    for s in spans:
                        text = s.text.strip()
                        if text.startswith("@"):
                            username = text.replace("@", "")
                        elif not display_name:
                            display_name = text

                    if not username or username in seen_users:
                        continue
                    seen_users.add(username)

                    # bioãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ã¸å…ˆã«ã‚¢ã‚¯ã‚»ã‚¹
                    if name_bio_keywords:
                        driver.execute_script("window.open('');")
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.get(f"https://twitter.com/{username}")
                        time.sleep(2)
                        try:
                            bio_text = (
                                WebDriverWait(driver, 5)
                                .until(
                                    EC.presence_of_element_located(
                                        (
                                            By.XPATH,
                                            "//div[@data-testid='UserDescription']",
                                        )
                                    )
                                )
                                .text
                            )
                        except:
                            bio_text = ""
                        driver.close()
                        driver.switch_to.window(driver.window_handles[0])

                        if not any(
                            k in display_name for k in name_bio_keywords
                        ) and not any(k in bio_text for k in name_bio_keywords):
                            print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿éä¸€è‡´ â†’ ã‚¹ã‚­ãƒƒãƒ—: @{username}")
                            continue

                    # âœ… æ¡ä»¶ã‚’é€šéã—ãŸå ´åˆã®ã¿æŠ•ç¨¿è©³ç´°ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦æŠ½å‡º
                    driver.execute_script("window.open('');")
                    driver.switch_to.window(driver.window_handles[-1])
                    driver.get(tweet_url)
                    WebDriverWait(driver, 10).until(EC.url_contains("/status/"))

                    try:
                        full_text_el = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located(
                                (By.XPATH, "//div[@data-testid='tweetText']")
                            )
                        )
                        text = full_text_el.text.strip()
                    except Exception as e:
                        print(f"âš ï¸ æœ¬æ–‡å–å¾—å¤±æ•—: {e}")
                        text = ""

                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆå®‰å®šåŒ– + ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ« + ã‚»ãƒ¬ã‚¯ã‚¿å¼·åŒ–ï¼‰
                    # æŠ•ç¨¿æ—¥æ™‚å–å¾—ï¼ˆè©³ç´°ãƒšãƒ¼ã‚¸å†…ã€ã‚¨ãƒ©ãƒ¼å›é¿ãƒ»å¤šæ®µæ§‹é€ ã«å¯¾å¿œï¼‰
                    date = ""
                    for attempt in range(5):
                        try:
                            driver.execute_script("window.scrollTo(0, 0);")
                            WebDriverWait(driver, 3).until(
                                EC.presence_of_element_located((By.XPATH, "//article"))
                            )
                            time_el = driver.find_element(By.XPATH, "//article//a/time")
                            if time_el:
                                date = time_el.get_attribute("datetime")
                                break
                        except Exception as e:
                            print(f"âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—è©¦è¡Œ {attempt+1}/5 å¤±æ•—: {e}")
                            time.sleep(1)

                    if not date:
                        print("âš ï¸ æŠ•ç¨¿æ—¥æ™‚å–å¾—ã«å¤±æ•— â†’ ç©ºæ–‡å­—ã§ç¶™ç¶š")

                    # è‡ªãƒªãƒ—ãƒ©ã‚¤å–å¾—ï¼ˆçœç•¥å¯ï¼‰
                    replies = extract_self_replies(driver, username)
                    if replies:
                        reply_texts = [
                            r["text"]
                            for r in replies
                            if "text" in r and r["text"] not in text
                        ]
                        if reply_texts:
                            text += "\n\n" + "\n\n".join(reply_texts)

                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])

                    tweet_id = re.sub(r"\D", "", tweet_url.split("/")[-1])
                    if already_registered(tweet_id):
                        print(f"ğŸš« ç™»éŒ²æ¸ˆ â†’ ã‚¹ã‚­ãƒƒãƒ—: {tweet_url}")
                        continue

                    tweets.append(
                        {
                            "url": tweet_url,
                            "text": text,
                            "date": date,
                            "id": tweet_id,
                            "username": username,
                            "display_name": display_name,
                        }
                    )

                    print(f"âœ… åé›†: {tweet_url} @{username}")
                    if len(tweets) >= max_tweets:
                        break

                except Exception as e:
                    print(f"âš ï¸ æŠ•ç¨¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Ÿè¡Œ
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            # èª­ã¿è¾¼ã¿åˆ¤å®š
            if article_count == last_article_count:
                pause_counter += 1
                print("ğŸ§Š ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«æ–°ã—ã„æŠ•ç¨¿ãªã—")
                if pause_counter >= pause_threshold:
                    print("ğŸ›‘ æŠ•ç¨¿ãŒå¢—ãˆãªã„ãŸã‚ä¸­æ–­")
                    break
            else:
                pause_counter = 0

            last_article_count = article_count
            scroll_count += 1

    return tweets


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰")
    parser.add_argument(
        "--account", default="accounts.json", help="ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONï¼‰"
    )
    args = parser.parse_args()

    config = load_config(args.config)
    account = load_config(args.account)

    global NOTION_TOKEN, DATABASE_ID, notion
    global TWITTER_EMAIL, TWITTER_USERNAME, TWITTER_PASSWORD
    global EXTRACT_TARGET, MAX_TWEETS

    NOTION_TOKEN = config["notion_token"]
    DATABASE_ID = config["database_id"]
    EXTRACT_TARGET = config["extract_target"]
    MAX_TWEETS = config["max_tweets"]

    TWITTER_EMAIL = account["email"]
    TWITTER_USERNAME = account["username"]
    TWITTER_PASSWORD = account["password"]

    notion = Client(auth=NOTION_TOKEN)
    driver = setup_driver()

    login(driver, EXTRACT_TARGET if config["mode"] == "target_only" else None)

    tweets_for_notion_upload = []

    if config["mode"] == "target_only":
        print(
            f"ğŸ¯ mode: target_only â†’ extract_target = {EXTRACT_TARGET} ã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™"
        )
        URL_BUFFER_FACTOR = 3
        tweet_url_dicts = extract_tweets(
            driver, EXTRACT_TARGET, MAX_TWEETS * URL_BUFFER_FACTOR
        )
        # extract_tweets ã¯ {"url": url, "id": id} ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_and_merge_tweets(
            driver, tweet_url_dicts, MAX_TWEETS
        )

    elif config["mode"] == "search_filtered":
        print(
            "ğŸ” mode: search_filtered â†’ æ¤œç´¢ + name/bio + tweetãƒ•ã‚£ãƒ«ã‚¿ã‚’ã‹ã‘ã¦æŠ•ç¨¿ã‚’åé›†ã—ã¾ã™"
        )
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        users = search_accounts(driver, config["filter_keywords_name_bio"])
        collected_tweets_count = 0
        for user_info in users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            if is_recruit_account(
                user_info["name"], user_info["bio"], config
            ):  # bioã¯search_accountså†…ã§å–å¾—ãƒ»è¨­å®šãŒå¿…è¦
                # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                user_tweet_urls = extract_tweets(
                    driver, user_info["username"], MAX_TWEETS - collected_tweets_count
                )
                # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                processed_user_tweets = extract_and_merge_tweets(
                    driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
                )

                # ã•ã‚‰ã«æŠ•ç¨¿å†…å®¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                for tweet_data in processed_user_tweets:
                    if is_recruit_post(tweet_data["text"], config):
                        tweets_for_notion_upload.append(tweet_data)
                        collected_tweets_count += 1
                        if collected_tweets_count >= MAX_TWEETS:
                            break

    elif config["mode"] == "search_all":
        print("ğŸŒ mode: search_all â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ â†’ bioãƒ•ã‚£ãƒ«ã‚¿ â†’ å„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿å–å¾—")
        collected_tweets_count = 0

        # filter_keywords_name_bio ã‚’ä½¿ã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æ¤œç´¢
        # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™æƒ³å®šã ãŒã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«æ¤œç´¢ã™ã‚‹å½¢ã«å¤‰æ›´

        all_potential_users = []
        for keyword in config.get(
            "filter_keywords_name_bio", []
        ):  # name_bio ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢
            print(f"ğŸ‘¤ '{keyword}' ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ä¸­...")
            # search_accounts ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            # search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã‹ã€ãƒ«ãƒ¼ãƒ—å†…ã§å‘¼ã³å‡ºã™
            # ã“ã“ã§ã¯ search_accounts ãŒå˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´ã—ãŸã¨ä»®å®š
            # ã¾ãŸã¯ã€search_accounts ã‚’ filter_keywords_name_bio å…¨ä½“ã§å®Ÿè¡Œã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¹ãƒˆã‚’å¾—ã‚‹

            # ä»®: search_accounts ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€ãƒãƒƒãƒã—ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¿”ã™
            # users_from_keyword_search = search_accounts(driver, [keyword]) # search_accountsã®ä»•æ§˜ã«åˆã‚ã›ã‚‹
            # all_potential_users.extend(users_from_keyword_search)

            # ç¾çŠ¶ã® search_accounts ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ã®ã§ã€ãã‚Œã§è‰¯ã„
            pass  # search_accounts ã¯å¾Œã§ã¾ã¨ã‚ã¦å‘¼ã³å‡ºã™ã‹ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‡¦ç†

        # search_accounts ã¯ filter_keywords_name_bio ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã¨ä»®å®š
        # (ã¾ãŸã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¯ã«å‘¼ã³å‡ºã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹)
        print(f"ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {config.get('filter_keywords_name_bio')}")
        candidate_users = search_accounts(
            driver, config.get("filter_keywords_name_bio", [])
        )

        # ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—ã—ã€bioã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_users = []
        for user_data in candidate_users:
            # search_accountså†…ã§bioã‚’å–å¾—ãƒ»è¨­å®šã™ã‚‹ã‹ã€ã“ã“ã§ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦bioå–å¾—
            # ã“ã“ã§ã¯ search_accounts ãŒbioã‚‚å–å¾—ã—ã¦è¿”ã™ã¨ä»®å®šï¼ˆã¾ãŸã¯åˆ¥é€”é–¢æ•°å‘¼ã³å‡ºã—ï¼‰
            # ä»®ã«ã“ã“ã§bioã‚’å–å¾—ã™ã‚‹ãªã‚‰ï¼š
            # driver.get(f"https://twitter.com/{user_data['username']}")
            # time.sleep(2)
            # try:
            #     bio_el = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, "//div[@data-testid='UserDescription']")))
            #     user_data['bio'] = bio_el.text if bio_el else ""
            # except:
            #     user_data['bio'] = ""

            # is_recruit_account ã¯ name ã¨ bio ã§åˆ¤å®š
            if is_recruit_account(
                user_data.get("name", ""), user_data.get("bio", ""), config
            ):
                filtered_users.append(user_data)

        print(f"ğŸ‘¤ bioãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {len(filtered_users)}")

        for user_info in filtered_users:
            if collected_tweets_count >= MAX_TWEETS:
                break
            print(f"ğŸ¦ @{user_info['username']} ã®æŠ•ç¨¿ã‚’åé›†é–‹å§‹")
            # extract_tweets ã¯URLè¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            user_tweet_urls = extract_tweets(
                driver, user_info["username"], MAX_TWEETS - collected_tweets_count
            )
            # extract_and_merge_tweets ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
            processed_user_tweets = extract_and_merge_tweets(
                driver, user_tweet_urls, MAX_TWEETS - collected_tweets_count
            )

            # search_all ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€æŠ•ç¨¿å†…å®¹ã®ãƒ•ã‚£ãƒ«ã‚¿ã¯é€šå¸¸ã‹ã‘ãªã„ãŒã€ã‚‚ã—å¿…è¦ãªã‚‰ã“ã“ã§ is_recruit_post ã‚’ä½¿ã†
            # ä»Šå›ã¯bioãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨æŠ•ç¨¿(MAX_TWEETSã¾ã§)ã‚’å–å¾—ã™ã‚‹ã¨è§£é‡ˆ
            tweets_for_notion_upload.extend(processed_user_tweets)
            collected_tweets_count += len(processed_user_tweets)

    elif config["mode"] == "keyword_trend":
        print("ğŸ”¥ mode: keyword_trend â†’ æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©±é¡ŒæŠ•ç¨¿ã‚’åé›†ã—ã¾ã™")
        # extract_from_search ã¯å‡¦ç†æ¸ˆã¿ã®æŠ•ç¨¿è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        tweets_for_notion_upload = extract_from_search(
            driver,
            config["filter_keywords_tweet"],  # æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            MAX_TWEETS,
            config.get(
                "filter_keywords_name_bio"
            ),  # å–å¾—ã—ãŸæŠ•ç¨¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ã•ã‚‰ã«çµã‚‹å ´åˆ
        )

    else:
        raise ValueError(f"âŒ æœªçŸ¥ã®modeæŒ‡å®šã§ã™: {config['mode']}")

    print(f"\nğŸ“Š Notionç™»éŒ²å¯¾è±¡ã®åˆè¨ˆãƒ„ã‚¤ãƒ¼ãƒˆæ•°: {len(tweets_for_notion_upload)} ä»¶")

    # æŠ•ç¨¿IDæ˜‡é †ã§ä¸¦ã¹æ›¿ãˆã¦ã‹ã‚‰ç™»éŒ²ï¼ˆé †ç•ªä¿è¨¼ï¼‰
    # id ãŒæ•°å€¤ã§ãªã„å ´åˆã‚„å­˜åœ¨ã—ãªã„å ´åˆã‚’è€ƒæ…®
    tweets_for_notion_upload.sort(
        key=lambda x: (
            int(x["id"]) if x.get("id") and x["id"].isdigit() else float("inf")
        )
    )

    for i, tweet_data in enumerate(tweets_for_notion_upload, 1):
        print(f"\nğŸŒ€ {i}/{len(tweets_for_notion_upload)} ä»¶ç›® Notionç™»éŒ²å‡¦ç†ä¸­...")

        # upload_to_notion ã«æ¸¡ã™å‰ã«ã€ä¸è¦ãªã‚­ãƒ¼ã‚„WebElementãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
        tweet_data_for_upload = tweet_data.copy()
        tweet_data_for_upload.pop(
            "article_element", None
        )  # extract_thread_from_detail_page ãŒæ®‹ã™å¯èƒ½æ€§
        tweet_data_for_upload.pop("article", None)  # å¤ã„å½¢å¼ã®ã‚­ãƒ¼ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆ

        # print(json.dumps(tweet_data_for_upload, ensure_ascii=False, indent=2)) # ãƒ‡ãƒãƒƒã‚°ç”¨

        # ãƒªãƒ—ãƒ©ã‚¤ãƒãƒ¼ã‚¸ã¯ extract_and_merge_tweets ã§å®Ÿæ–½æ¸ˆã¿ã®ãŸã‚ã€ã“ã“ã§ã¯å‘¼ã³å‡ºã•ãªã„
        upload_to_notion(tweet_data_for_upload)

    driver.quit()
    print("âœ… å…¨æŠ•ç¨¿ã®å‡¦ç†å®Œäº†")


if __name__ == "__main__":
    main()
